好的，让我们开始吧。作业二应该进行得顺利。如果您有任何疑问，请随时联系我们。关于提案项目，如果您有任何疑问，请在我们的办公时间内或通过 Piazza 联系我们。 现在有人有其他问题需要我回答吗？好的，今天我们要开始讨论策略梯度方法。在当前强化学习领域，策略梯度方法可能是应用最广泛的方法。因此，熟悉这些方法非常有用。每当我们谈论强化学习时，我们都会不断提到这些主要属性，我们希望智能体能够学会做出决策，优化这些决策，处理延迟的后果，进行探索，并通过统计方法在真实的高维空间中有效地完成这一切。上次我们讨论的模仿学习是...

通过不同的方法，我们可以为我们的 AI 智能体提供额外的结构或支持，使其能够尝试更快地学习。模仿学习是一种利用人类示范提供结构支持的方式。在讨论函数逼近时，我们已经看到了其他排序结构或人类先验知识的方法。因此，当我们考虑定义 q 时，例如将 q 定义为 s、a 和 w，其中 w 是一组参数时，我们实际上在根据如何表示价值函数来强加某种结构。这种选择可能非常强大，就像假设价值函数是线性的那样，也可能非常微弱，比如使用深度神经网络。因此，当我们规定这些函数的近似值和表示形式时，我们隐含地做出了选择，决定我们希望为模型注入多少结构和领域知识。

为了我们的 AI 智能体的学习，我们今天要开始讨论的话题是策略搜索，这是一个可以非常自然地输入领域知识的方法。在我们今天的一些机器人示例中，我们将看到策略搜索的作用，这也是一种非常有效的学习方式。在我们深入了解无模型强化学习，并尝试扩展到非常大的状态空间之前，策略搜索起着重要作用。有一些人向我提出了关于非常大的动作空间的问题，这是一个非常重要的话题。尽管本季度我们不会深入讨论这个问题，但我们会讨论动作空间何时是连续但低维的情况。我们已经开始讨论状态空间何时是高维且非常大的情况。因此，我们讨论了通过某种参数化方式来近似问题，比如使用参数 Theta 或者我们经常使用的 w，来表示函数的参数化。我们会利用我们的价值函数来进行讨论。

在强化学习中，我们通常定义一个值函数，用于计算特定状态或状态行动的预期奖励贴现总和，然后我们可以从这个值函数中提取策略。然而，今天我们要讨论的是直接参数化策略。在这种情况下，我们不再使用值函数，而是直接对策略进行参数化。在表格设置中，策略只是一个简单的状态到行动的映射，可以看作是一个查找表，为每个状态记录要采取的行动。现在，我们不再尝试编写这样的策略表，而是要将策略参数化，即使用一组权重（或Theta参数）来表示。尽管今天我们主要使用Theta参数，但可以将其视为权重。这种参数化策略的方法是我们今天要讨论的重点。通过参数化策略，我们将有更多灵活的方法来表达策略。和状态行动价值函数类似，这种方法会对学习空间产生重大影响，因为它有效地定义了我们可以探索的策略空间。

在你可能学习的政策类别中，我们再次着重介绍无模型强化学习，这意味着我们并不假设能够获得关于世界动态或奖励的先验模型。在本季度初，我们列出了一些图表，现在我想回顾一下。我们之前一直在讨论价值相关的内容，但并没有太多涉及模型，虽然模型也很重要。现在我们已经讨论了很多关于价值函数、基于方法的问题，接下来我们要谈论策略，具体说是直接策略搜索方法。正如你所料，有很多工作尝试将这两者结合起来，通常被称为演员-评论家方法。你会尝试明确维护参数化策略，以及明确维护参数化评论家或价值函数。所以，政策是我们接下来要讨论的一个问题。好的，今天我们将开始讨论基于政策的方法。那么，为什么要这样做呢？ 嗯，【噪音】嗯。

这实际上可以追溯到上周我们讨论的模仿学习。在模仿学习中, 我们谈到了一个事实：有时人类很难编写奖励函数，因此对他们来说，模仿演示策略可能更容易。同样，在某些情况下，也许编写策略空间的参数化比编写状态-动作价值函数空间的参数化更容易。另外，它们通常在高维或连续动作空间中更有效，让我们能够学习随机策略。虽然到目前为止我们还没有讨论过，但我会给你一些例子，说明我们确实在哪些情况下需要随机策略。另外，它们有时具有更好的收敛性能，尽管这可能会有些争议，这取决于我们将其与无模型或基于模型的方法进行比较以及我们如何进行大量计算。因此，计算可能是一个重要因素。真正最大的缺点之一是它们通常只会收敛到局部最优值。

因此，你会最终收敛到一个好的策略，这是一个很好的策略，但通常不能保证收敛到全局最优值。有一些技术可以确保收敛到局部最优解和全局最优解。今天我将重点介绍其中一些技术，但一般来说，在深度强化学习中使用的几乎所有方法都是基于策略梯度，仅能收敛到局部最优解。另一个挑战是，通常我们通过尝试评估策略然后估计其梯度来实现这一点，这可能会导致样本利用率低下。因此，当采用基于梯度的方法时，可能需要大量数据来估计梯度。

那么，为什么我们需要某种随机策略呢？在之前提到的表格设置中，是因为我们想要这样吗？如果回想一下第一堂课，当我们有一个表格马尔可夫决策过程（MDP）时。

在确定性且最优的策略π下，MDP设置中无需随机策略，因为总是存在与最优策略具有相同值的确定性策略。在表格MDP案例中，这是不必要的。然而，并非总是在表格MDP案例中采取行动。举个例子，谁熟悉石头剪刀布？大多数人都熟悉。如果不熟悉，可能用其他名称玩过。在剪刀石头布游戏中，两人可以选择石头、剪刀或布，他们之间存在胜负关系。在这种情况下，如果有确定性策略，

在这样的情况下，我会改变策略，而大语言模型非常容易受到损失。但是，采用统一的随机策略基本上是最佳选择。这里所说的最优性指的是什么？在这种情况下，我所指的是，如果你胜利了，你可以得到加一的回报，如果你失败了，你可以得到零或减一的回报。在这门课程中，我们不会涉及太多关于多智能体案例的讨论，但这是一个非常有趣的研究领域。在这种情况下，环境并非未知。环境会对我们采取的策略做出反应，而且可能是具有对抗性的，因此我们需要制定可以应对对手的稳健策略。因此，第二种情况是Aliased Gridworld。那么，在这种情况下，为什么随机性如此重要呢？因为我们并不是真正处于随机环境中，而是处于对抗环境中，我们还有其他代理与我们互动，他们的行为是不固定的，可以根据我们的策略进行调整。因此，环境不是像石头、剪刀、布那样，不管我们做什么行为，环境都不会做出预定的选择。

在过去，AI智能体能够对这些情况做出反应。但是，由于具有非平稳性和对抗性，它表现得有些复杂。另外，它不是马尔可夫过程，因此只能部分可观察且存在混叠现象，这意味着无法准确根据传感器来区分多种状态。举个例子，机器人配备了激光传感器，可以感知走廊上的位置。然而，在很多不同的走廊中，第一个障碍物的距离在所有180度方向上看起来都相同。这就是一个简单的例子。

在网格世界中，假设智能体的传感器无法区分灰色状态，并具有特定的特征，例如可以探测到它的周围是否有墙壁。基于这些特征，智能体可以判断自身朝向的不同情况，比如北、东、南、西方向是否有墙壁。这样，智能体可以基于具体情况作出相应的应对方案。

在另一种情况下，可能会有相同的灰色状态。因此，如果我们使用某种近似值函数来进行基于值的强化学习，它将考虑到这些特征，比如我应该采取什么行动以及周围是否有墙壁。或者我们可以采用基于策略的方法，也会考虑这些特征，但是直接尝试根据这些特征做出决策，这些决策可能是随机的。因此，在这种情况下，AI智能体正在努力学习如何在这个世界中导航，它的目标是到达奖励点。所以它会避开骷髅和交叉骨头，因为那些会带来负奖励。由于状态相同，智能体无法区分自己是在这里还是在那里，因此它必须在两个状态下做出相同的行动。所以它要么向左走，要么向右走，无论选择哪个方向都不是最佳的，因为如果它真的在这个位置，那么最好的选择是向那个方向移动。

不是在这里和下面。 嗯，在这种情况下，AI 智能体可以区分它是在这里还是在那里，但它可能会来回移动，或者做出糟糕的决定。因此，它可能会陷入困境，并且永远无法确定何时可以安全地下降并获取资金。因此，它学习了一种接近确定性的决策策略，因为我们通常学习的策略，无论是贪婪的还是随机的，通常都会表现不佳。但是，如果在某个状态下有随机策略，你可以随机选择。你可能会说，“我不确定我是处于这个状态还是那个状态，所以我可能向东走，也可能向西走，有50%的概率。”然后通常会很快达到目标状态。因为请注意，当达到目标状态时，AI 智能体可以确定下一步应该采取什么行动，因为这看起来与两个状态的情况不同。因此，一旦处于中间状态，AI 智能体就能找到正确的行动。这只是一个例子，说明在某些情况下，随机策略比确定性策略更有价值，因为这个环境不符合马尔可夫性质，是部分可观察的。

好的，让我们重新整理一下这段内容。所以，有些原因可能会导致我们偏向直接基于政策进行建模，除了其他原因。具体来说，当我们拥有参数化策略时，我们希望找到的目标会变得更加清晰。在一个世界不是马尔可夫的情况下，即部分可观察且随机的情况下，随机策略是否会更好呢？这取决于我们所要建模的对象。相较于完全随机化，使用部分可观察马尔可夫决策过程（POMDP）策略可能更好，因为它会在灰色区域中做出一些决策，而不是完全随机化。通过这种策略，您可以跟踪您在环境中的位置的估计，以便确定您所处状态的信念状态，并最终能够准确地识别。

当你处于这种状态时，你需要做出确定的右转或左转决定。这取决于你愿意如何对情景进行建模。当我们开始进行参数化策略搜索时，我们的目标是找到能够产生最佳价值的参数。找到最佳价值的策略类别与之前看到的非常相似，我们可以考虑各种情景和无限的连续设置。在情景环境中，智能体通常会执行多个时间步长，比如说H个步长，但这可能是可变的，有可能一直到达最终状态为止。我们需要考虑预期值是什么，价值在哪里，以及价值是多少？我们从起始状态或起始状态的分布中获得的预期折扣奖励是多少？然后我们的目标是找到能够产生最高价值的参数化策略。在持续环境下，即在线环境中，另一种选择是……

我们将不会执行H步骤，而是永远执行下去。没有终止状态可用，我们可以取状态分布的平均值。因此，就像我们之前讨论的那样，在特定策略下引起的马尔可夫链上出现的稳态分布。因为我们前面提到过一个事实：一旦调整了策略，基本上你就进入了马尔可夫回报过程。您也可以把得到的状态分布看作是马尔可夫链。所以，如果我们永远执行动作，我们可以说从平均角度来看，我们在固定分布下达到的状态的价值是多少？另一种方法是考虑每个时间步的平均奖励。现在，为了简单起见，今天我们主要关注情景设置，但我们也可以探讨类似技术针对其他形式设置的应用。因此，和以前一样，这是一个优化问题，类似于我们在值函数近似情况下看到的情况，对于线性值函数的情况，我们使用深度神经网络。

为了优化我们的模型，我们需要使用一种优化工具来搜索最佳参数。一种选择是进行无梯度优化。虽然我们不太倾向于在策略搜索中采用这种方法，但有许多无梯度优化的方法可供选择。我们的目标是找到能最大化 V Pi Theta 的参数。类似于 Q 函数中的情况，现在我们有了 Theta 来定义一个策略，希望找到能使其取得最大值的参数。因此，我们致力于尽可能有效地寻找函数的最大值，有许多方法可以实现这一目标，而不需要函数是可微分的。在某些情况下，这些方法实际上可能非常有效。我的同事在这方面做了一些出色的工作，我们开发了一种自动识别外骨骼辅助模式的方法，可以最大程度地减少。

人们行走时的代谢能量成本是一个噪音问题。在优化过程中，用户首先体验一个控制律，通过将一阶动态模型拟合到两分钟的瞬态数据来估计稳态能源成本。然后改变控制律并再次估计代谢率。重复这个过程来形成一代内规定数量的控制律。然后使用协方差矩阵适应进化策略来创建下一代。每代的平均值代表最优控制参数值的最佳估计。经过约一个小时的优化后，能源成本平均降低了24%，与没有任何帮助相比。这项工作是由我的同事史蒂夫·柯林斯 (Steve Collins)完成的。

我们是否可以帮助人们表现更出色呢？这个例子想要说明的是，在许多情况下，外骨骼可以发挥作用。有很多中风患者，行动不便的人，以及失去肢体的退伍军人。在这些情况下，一个挑战是如何确定外骨骼的参数，以便为人们行走提供支持，因为外骨骼的参数通常需要根据个体的生理状态进行调整。不同的人需要不同类型的参数，但你希望快速地实现这一点。因此，你希望能够快速了解每个人的情况，以帮助他们在行走时得到最大的支持。正确的控制参数是什么？所以史蒂夫的实验室把这看作是某种政策搜索问题。你让人们佩戴设备，然后尝试不同的控制方法，看看哪种方法可以使外骨骼提供最佳支持。你在测量他们的代谢效率，也就是他们需要多大的努力来行走，特别是呼吸方面。这可以帮助确定哪种控制方法最有效。

接着，根据这些信息，您可以确定下一组控制法则，并以闭环方式尽快完成所有任务。我之所以提出这一点，一是因为它非常有效，同时也是一篇很好的科学论文，说明了它相较于先前技术的更高效性。其次，它使用了CMA-ES，这是一种无梯度方法。尽管我们今天讨论的大多数内容都是基于梯度的方法，但有一些很好的例子表明，在许多其他类型的应用中，不使用基于梯度的方法进行策略搜索是可行的。因此，我认为在您的工具箱中了解这些方法是很有必要的，不必局限于基于梯度的方法。而像CMA-ES这样的方法之一的确好处之一是其保证性能，这对于达到全局最优是非常有用的。因此，在某些情况下，您可能真的希望确保这一点，尤其是在高风险情况下。总的来说，最近人们反复注意到，有时这些方法的效果确实好到令人惊讶，它们在某些方面通常是一种强力方法。

有一种智能而强有力的方法，通常会非常有效。因此，在您研究的应用程序中，这些方法是值得考虑的。但是，尽管如此，有时它们虽然在某些方面非常出色，但在并行化方面帮助有限。在效率方面常常存在问题。因此，根据您所涉及的领域和数据结构，基于梯度的方法通常是有用的，特别是如果您对获得局部最优解感到满意的话。因此，我们今天要讨论的主题是基于价值的方法，例如梯度下降和其他试图利用决策问题结构的方法。因此，对于某些方法，如CMA-ES，可能并不了解这个世界是MDP或任何形式的顺序随机过程。我们将重点关注如何利用马尔科夫决策过程的结构在决策过程中。接下来我们谈一谈策略梯度方法。

嗯，再次，使用 θ 定义事物，这样我们可以明确参数，并且我们将重点关注偶发性马尔可夫决策过程（MDP），这意味着我们将在某个特定的时间内运行我们的策略，时间步数，直到我们到达最终状态，或者您可能知道，可能是 h 步。在这段时间内，我们会得到一些奖励，然后我们就会重置。因此，我们将寻找局部最大值，并且我们将针对定义策略的参数获取梯度，然后使用一些小的学习率[噪声]。所以这就是——这应该看起来非常相似，很类似，类似于基于 Q 和 V 的搜索。这里的主要区别是，我们不是对定义 Q 函数的参数求导，而是对定义策略的参数求导。所以，这里要做的最简单的事情就是，嗯，进行有限差分。

对于每个策略参数，只需微调它，如果对每个参数的每个维度进行微调，就可以获得梯度的估计。这里只是使用有限差分来估计梯度。在每种情况下，您可以使用一定数量的样本来执行此操作。假设您有一个k维的一组定义策略的参数，尝试微调其中一个，然后重复此过程，您将获得一组新策略的样本。通过对所有不同的维度都这样做，现在您就有了梯度的近似值。这种方法虽然有些复杂，效率可能不是很高，但有时是有效的。这演示了在强化学习环境中政策梯度方法是如何非常有用的。策略本身不必是可微分的，因为我们只是在进行梯度的有限差分近似。第一个例子是我看到的。。。

在Peter Stone的工作中，他探讨了政策梯度方法或政策搜索方法在机器人领域的有效性，特别是在Robocup比赛中。Robocup是一个旨在培养具有击败人类世界杯冠军实力的机器人足球队的计划。通过不同的Robocup联赛，包括四足机器人联赛，机器人尝试相互进球，挑战在于使它们看起来有像真实足球比赛中的动作。该计划的目标之一是在2050年前建立一支强大的机器人足球队，其表现可以胜过人类球员并赢得世界杯。

此外，您还必须了解走路的步态。您希望机器人能够快速移动，但又不希望摔倒。因此，优化步态是一个关键问题，尤其在赢得比赛时，因为您需要机器人在赛场上快速移动。Peter Stone长期以来一直是Robocup的领军人物。他们的目标很明确，就是让这些AIBO学会一种快速行走的方法。为了实现这一目标，真实的经验和数据至关重要，因为这是昂贵的，您需要让这些机器人来回移动，希望它们能够快速优化自己的步态。您不希望频繁更换电池等东西，因此您希望用尽可能少的数据来实现这一目标。因此，在这种情况下，他们想要做的是制定参数化策略，并尝试优化这些正确的策略。这就是领域知识的重要性所在，在这里，注入领域知识是至关重要的。所以，他们使用连续椭圆来指定步态是如何工作的，通过对其进行这12个连续参数的参数化。

这段内容讲述了 Peter Stone 和他的团队在 Robocup 上的经验，他们尝试使用有限差分方法来优化政策参数。在他们的研究中，他们使用真实机器人进行政策评估，并且定期进行干预以更换电池。他们在仅有三个 AIBO 机器人的情况下完成了这项研究，每次迭代大约执行 15 个策略，每个策略评估 3 次。尽管数据量不是很大，但考虑到信号嘈杂，每次迭代大约需要 7 小时。这表明了他们的研究方法在实践中的应用。

在这个案例中，研究者们需要在五分钟内选择一个学习率。他们尝试了多次迭代，关注的是稳定行走的速度作为优化标准。过去人们尝试手动调整策略，其中包括了UT Austin Villa团队。他们发现使用有限差分方法比其他方法更快地搜索更好的策略。这种方法并不需要大量数据，且在几个小时内就能取得结果，表明政策梯度方法可以比之前更好地实现目标，而且不需要大量数据。与之前的情况不同，这个案例没有特定的迭代次数限制。

这个研究大致指出，AI智能体通常会尝试数十到数百种策略，而不是数百万个步骤，这使得它们在数据效率上非常高效。虽然这种方法提供了大量信息，但也存在一些影响性能的因素。因此，在论文中作者讨论了对性能有实际影响的因素，这些因素有很多会影响性能。在选择初始策略参数时的重要性，这是因为在这种方法中无法保证全局最优值，只能找到局部最优值，因此起点对于找到哪个局部最优值至关重要。这种方法只能保证找到局部最优，同样所有策略梯度风格的方法也是如此。无论你从哪里开始，你最终会达到最接近的局部最优值，但不能保证那是全局最优值。

在这种情况下，大量随机重新启动或具备领域知识都是至关重要的。另一个关键问题是你对于局部扰动的敏感程度。我认为最关键的是策略的参数化。就像你如何定义你学习的可行政策空间一样，因为如果政策空间不恰当，你将无法学到任何东西。在幻灯片26中关于开环策略的问题，你能否更加详细地解释一下呢？是的，在开环策略部分的问题是关于开环策略的本质。我们所学习的这些策略不一定需要具有适应性。开环策略本质上是一种计划，即一系列操作，无论可能有什么其他输入。因此，通常我们将策略视为从状态到动作的映射，但它们也可以只是一系列动作。所以当我们谈论开环策略时，这是一种非反应性策略，因为它只是一系列动作，无论机器人处于何种状态，它都会继续执行。因此，即使在中途有很大的干扰， ...

无论风大与否，Transformer的下一步动作都是一样的。它不必是反应性的。好的。所以，但总的来说，嗯，你知道，有限差分是一个合理的尝试。嗯，通常我们会想要使用梯度信息并利用我们的函数策略实际上是可微的这一事实。所以，我们现在要做的是，嗯，分析计算策略梯度[噪音]对不起。这在目前使用的大多数技术中是最常见的。嗯，我们假设它在非零的地方是可微的，并且我们可以显式地计算它。因此，当我们说“我们知道”时，这意味着这是可计算的。我们可以明确地计算出这一点。所以，现在我们将，嗯，只考虑基于梯度的方法。所以，我们只是，[噪音]我们只会收敛到局部最优。希望我们能达到局部最优，这是我们在这种情况下所能期望的最好结果。好的。所以，我们要讨论——人们经常讨论似然比策略，嗯，他们将按如下方式进行。

让我们考虑一下偶发事件。在这种情况下，我们可以认为存在轨迹，即从初始状态经历状态、动作、奖励，一直到达最终状态的过程。我们用Tau下标的R来表示轨迹的奖励总和。因此，在这种情况下，政策的价值就是按照该政策获得的预期折扣总和的奖励。这可以表示为我们观察到特定轨迹的概率乘以该轨迹的奖励。简而言之，在这个政策下，观察到任何轨迹的概率乘以轨迹的奖励是多少。奖励是轨迹的确定性函数。一旦你知道了状态、行动、奖励等信息，那么你的奖励就是所有这些的总和。因此，在这个特定情景中，我们的目标是找到策略参数Theta，使得um是它的arg max。

这里我们所做的改变是要着重关注当前情况。因此，请注意，策略参数仅以我们在该策略下可能遇到的轨迹分布的形式呈现。这有点类似于之前讨论的模仿学习。在模仿学习中，我们讨论了关于状态分布以及状态和动作分布的内容，并试图找到一种与专家演示的相同状态-动作分布的方法。今天我们不会过多讨论状态-动作分布，而是专注于我们特定策略下可能遇到的轨迹分布。那么，关于这个梯度，具体是多少呢？我们希望获得这个函数相对于 Theta 的梯度。因此，我们将按照以下方式做：我们将重新表述关于 Theta 下轨迹的概率。换句话说，对于每个 Tau 求其概率。

好的，让我们来梳理一下这段话的内容。首先，我们要对某个东西进行鞭打，然后确保获得相同的符号。接着，我们需要做的是将某个值乘以或除以相同的数值。换句话说，我们需要对给定Theta的Tau概率进行一系列操作，包括除以、乘以，以及求导数等。此外，这里涉及到概率的对数，因此如果对给定Theta的Tau概率的对数进行求导，则...（原文未完）。

这个公式实际上等于在给定 Theta 的情况下，Tau 的概率乘以在给定 Theta 的情况下 Tau 的导数。因此，可以重新表达为Tau r的总和，与Tau的p乘以相对于给定Theta的Tau p的对数。目前看起来似乎并不是很有用，但这种转换很快将展示其用处。特别是，当我们想要完成所有这些工作，而又不了解动态或奖励模型时，这种转变将变得非常有用。我们需要在不知道动态模型的情况下评估策略的梯度，这个转换技巧将帮助我们实现这一点。因此，在我们这样做的时候，

这个概念通常被称为似然比。如果我们进行转换，可以说：“好吧，我们可以观察到，通过这样做，实际上与取对数是完全一样的。” 那么，为什么这种转换可能是有用的呢？我们这里讨论的是什么？我们计算的是所有轨迹的总和。当然，我们未必能够访问所有可能的轨迹，但我们可以对它们进行采样。因此，您可以想象多次运行您的策略，对多个轨迹进行采样，观察这些轨迹的奖励，并对给定 Θ 的轨迹概率进行求导来进行近似。通常情况下，我们只需要运行该策略 m 次即可实现这一点。然后，对于给定 Θ 的 τ 的概率 p，我们将以一种近似的方式进行计算。在这一步中，我们只需平等地考量采样过程中获得的所有轨迹，然后观察该轨迹的奖励，以及对数概率 p，即 [噪音] 嗯，给定 Τ 对 Θ 的。

在这个案例中，发生了什么？简言之，梯度如何计算？我们将奖励乘以路径概率的对数，以得到与特定单词相关的奖励与Theta的乘积。这样做的原因是评估路径或样本的质量。我们希望根据样本的质量来调整其对数概率路径，以便提高参数并获得更好的样本。因此，在我们的策略中引入一些参数，这些参数将促使我们执行路径带来高回报的操作。因此，如果我们将f(x)视为奖励，我们可以将其视为我们的策略或参数化策略。

我们的目标是增加空间中物体的重量，以获得更高的回报。 因此，如果这是我们的 \( f(x) \)，这是我们的奖励函数，这是我们轨迹的概率，那么我们希望调整我们的策略，以增加产生高奖励轨迹的概率。 这样，我们将更有可能发现具有高价值和高回报的事物。 那么，问题来了，如果要实现这一点，我就需要近似第二项，也就是对数项。 在某些参数下，相对于轨迹概率的导数。 因此，必须能够计算出在一组参数下轨迹的概率是多少，我们可以通过以下方式实现。因此，这将是 \( S_0 \) 的 \( \mu \) 的对数概率的 \( \Delta \Theta \)。 这样，我们就可以计算初始起始状态的概率，再乘上...

乘积项 J 是指在时间点从0到t时，观察到下一个状态的概率与进行该操作的概率的乘积之和，根据当前政策。因此，最终结果还需加上另一个括号。由于这是对数形式，我们可以对其进行分解。因此，这个表达式等于S零的Mu乘以Delta Theta的对数，再加上所有Delta Theta的对数之和，因为它是对数形式。J在时间点从0到t时减去[噪音]Transformer模型的对数1。请注意，一般情况下我们并不知道这个数值，它是未知的。我们只希望最终得到一个表达式，这意味着我们不需要这个数值。在此处引用的是我们所经历的路径。

J的总和是从0到t减1。这是我们实际的策略参数。那么，有人能告诉我为什么这种分解是有用的吗？不管我们是否需要，让我们将所有这些东西参数化。嗯，即使我们并不需要了解动力学模型是什么，这种方法看起来也是可行的。每个人都花点时间，和你的邻居谈谈，然后告诉我哪些项会变成零。接下来我们对Theta进行求导。这些术语中有哪些是Theta的函数。（重叠声音）

请记住，Theta 决定了您的策略参数。Theta 决定了您在特定状态下采取的行动。好的，现在我要做一个快速民意调查，我将项目分为一、二和三编号。第一项目是否受Theta影响？如果是请举手，如果不是请举手。太好了，明白了，Theta与第一项目无关。因此，它的值为零。第二项目是否与Theta无关？请举手。太好了，所以第二项目的值也为零。因此，唯一剩下的就是第三项目，这很好。所以，这是一种很好的情况，为什么我们要做这种奇怪的对数变换呢？因为通过这种奇怪的对数变换，它允许我们将采取行动的概率作为乘积来接受状态转换，然后我们可以将其分解为总和。一旦我们将其分解为总和，我们就可以分别应用导数，这意味着其中一些项直接消失，这真的很酷。因此，这意味着我们实际上并不需要知道转换模型是什么，也不需要有一个明确的代表。

请先提出您的问题和姓名。您的问题是关于系统动态是否取决于政策，很好的问题。实际上，系统的动态通常是与政策无关的。在这种情况下，代理可以根据其选择的行动来影响系统的动态，但一旦决定采取行动，系统的动态则与代理独立，这称为解耦。因此，不同的政策将导致不同的轨迹，但影响这些轨迹的只是所选行动的策略，之后环境会决定接下来会发生什么。你并不需要知道每个动作对环境产生的影响，因为奖励将会反映这一点，奖励也是状态的函数。因此，根据你的行动，你可能会访问状态空间中的不同区域。还有其他问题吗？如果您想了解如何对给定 Theta 进行概率估计，可以更详细地描述一下，但通常我们会根据观察到的数据集，尝试估计 Theta 的概率分布。

在这里我们正在讨论的问题涉及到 i 次操作是否会超过 m。在我们采取的做法是否正确的问题上，这是个很好的问题。简而言之，我们要做的是采用这一政策，运行它 m 次。在此过程中可能会得到不同的轨迹。我们要做的是分别计算每个轨迹的概率对数，然后将它们汇总起来。在确定性情况下，如果领域没有太多随机性并且策略也是确定的，你可能最终会得到多个相同的轨迹。一般来说，轨迹会是完全不同的，因此局部梯度的估计也是如此。因此，我们需要能够以分析形式为我们的政策相对于参数的导数提供一个形式。因此，我们需要评估如何参数化我们的策略。

如果我们希望进行分析，就需要对我们的策略进行参数化，这种参数化方式可以精确计算任何状态和动作。因此，我们将更深入地讨论一些方法，这些方法可以使计算变得更加分析和有效。在某些情况下，您可能需要通过蛮力、计算、有限差分或其他方法来估算值函数本身。但是，如果我们选择一种特定形式的参数化策略，那么计算将更具分析性。另一个需要评估的重要部分是评分函数，尽管我认为评分函数并不特别有用，但它仍然被广泛使用。这个函数被称为评分函数，这是我们刚刚讨论到的需要评估的量。因此，当我们计算函数的导数时，通过获取

当我们有m个样本来近似一个值，并对i从1到m求和。然后我们观察这条路径的奖励，并对每个步骤的得分函数求和。大家能理解后文了吗？是的，太好了。是的，这些是我们的评分函数。这些评分函数允许我们评估我们遇到的每个状态动作对，而无需了解动态模型。因此，政策梯度定理对此进行了概括。它是如何概括这一点的呢？请注意，在这种情况下，我们正在处理的是情景设置。在这种情况下，我们只使用原始奖励函数。因此，我们查看这条路径的奖励总和，然后，嗯，

我们可以通过政策参数的导数来衡量它。另外，我们也可以进行一些总结。比如，假设我们要打电话，这就是一个价值函数，而我们的目标函数可能略有不同。之前我们讨论过如何获得情景奖励、每个时间步的平均奖励或总体平均值。因此，我们可以让目标函数等于情景正常值，或者让其等于我提到的J AVR，即每个时间步的平均奖励，或者作为平均值。假设我们持续进行，希望对遇到的状态分布进行平均。所以我们需要考虑一下这个方面。这也是一个很好的场景。实际上，在所有这些情况下，我们都可以进行类似于情景案例中的推导。我们发现，我们目标函数的导数现在可以适用于这些不同的目标函数中的任何一个。

在Sutton和Barto的第13章中，我们引用了他们关于政策梯度的内容，通过计算当前政策相对于其下的期望值的导数，乘以Q值来更新政策参数。他们在讨论中提到了许多不同的问题，并指出这些内容可以扩展到持续性问题。总结一下，我们可以通过运行我们的政策m次，对每次运行得到的完整状态序列、行为和奖励进行平均，得到政策梯度的无偏估计，尽管存在很大的噪音，因此估计将是公正但嘈杂的。

如果你回想一下之前了解的蒙特卡洛方法，这个概念可能会有些熟悉，是吧？我们的政策即将到期，我们会获得类似于蒙特卡洛中获得的奖励，这是一个梯度的无偏估计，虽然会有一些噪音。那么，要使其真正实用，需要采取什么措施呢？有许多不同的技术可以做到这一点，但今天我们要开始讨论的是时间结构和基线。在我们继续探讨之前，我要开始研究，你知道，修复时间结构和基线，基于我刚才提到的蒙特卡洛估计。

在降低估计方差方面，我们可以考虑使用引导法。引导法是一种替代于分割蒙特卡罗方法的技术。通过在偏差和方差之间进行权衡，类似于在Q学习和DQN中使用的时间差异方法，引导方法有助于减少方差并加速信息传播。我们可以尝试用其他变量替换R，或者在R之外使用协变量，来减少方差。因此，在这个过程中，我们可以做一些尝试，利用我们暂时的资源，尽力降低方差。

好的，根据您提供的内容，经过整理和翻译后如下：

嗯，对于之前熟悉重要性采样的人来说，这与每个决策重要性采样密切相关。基本上，我们要利用的事实是奖励只能在时间结构域内。让我写出来。所以，我们之前提到过，回报对于策略参数的梯度期望等于从0到t的轨迹期望rt的负数乘以t的总和。例如，您获得的所有奖励之和乘以t的总和等于从0到t减去您的策略参数的导数的负一。这就是我们之前讨论的情况。因此，我们只需将所有奖励相加，然后将其...

我们将对轨迹中每个动作状态对的策略梯度进行求和。嗯，因此，我们关注的是单个奖励，而不是所有奖励的总和。让我们来看看。对于 rt prime 的期望值关于 Θ 的导数。这只是我们在单个时间步长可能遇到的奖励，沿着轨迹，这相当于 rt prime 乘以累积到时间步 t 的总和，这等于导数对 t prime 的期望值。所以，这几乎与之前一样。唯一的关键区别是我们只对 t prime 求和。总之，你可以把它看作是一条削减版的轨迹。我在研究状态的影响，以及我，法学硕士。

我一直达到的，嗯，rt prime 时所达到的行动和奖励。好的。所以，我不必对未来的事情进行总结。因此，我们可以使用这个表达式，现在我们可以对所有时间步进行求和。那么，这意味着预期奖励是多少，或者相对于时间步长 t prime 的奖励的导数？现在，我要总结一下，这将与我的第一个表达式相同。所以，我要做的是，我会说，Theta的[NOISE]V等于er的Theta的导数，我将总结该内部表达式。因此，我要对t prime等于0到t减1 rt prime求和，然后插入第二个表达式。好的。所以，我所做的就是把它放在那里，然后对t质数求和等于0，一直到t减1，然后，

我要做的是通过以下观察重新排序这些内容。想一想，在这些特定情况下，$$\log \Pi_{\Theta}(s_t)$$出现在多少个项目中？那么，如果我们看一下，对于$$a_1s_1$$的$$\Phi_{\Theta}$$的对数。因此，当您查看出现的次数时，您会发现它既出现在早期奖励中，也出现在所有后期奖励中。这将出现在$$r_1$$中，它将出现在$$r_2$$中，它将一直出现在$$t-1$$的$$r_t$$处，因为我们总是对$$t$$之前的所有内容求和。现在，我们要做的是接受这些条款并重新组织它们。因此，其中一些术语出现了很多次，而另一些术语，即$$\log \Pi_{\Theta}$$的$$a_{t-1}, s_{t-1}$$，只会出现一次。

它只是重新负责帮助决定最终奖励。因此，我们可以利用这种洞察力重新组织这个方程。现在，我们说，t 的总和的期望值等于从0到t减1。请注意我之前将t置于内部，t'置于外部，而现在我要做的是将t移到外部。我会说，在第t次，t'的总和等于从t到t减1的rt'。所以，我做的只是重新整理这个等式。在倒数第二行，这是否应该是关于Theta的值函数的导数？对，在最左边的地方。对不起，你的意思是应该是这个的导数？

是的，没错。在这种情况下，我们所做的是重新组织和总结。我们只是以稍微不同的方式回顾了术语，但这将会以一种更易理解的方式呈现出来。所以，让我们把它向上移动，我会把它向下移动。好的。所以，现在我们依然在讨论时间结构。这会给我们带来什么启示？嗯，第二个术语看起来应该有点熟悉。这里指的是，从时间步开始，一直到结束时我们所获得的奖励是什么？即回报。所以，我们之前定义过，当谈论蒙特卡洛方法等时，我们可以仅仅考虑在时间步 t 处的奖励 rt。这实际上就是回报。从第 i 集的时间步 t 开始到结束所获得的回报。因此，这个概念看起来与我们在蒙特卡罗方法中所熟悉的非常相似。

我们能够通过状态和动作推断出从该状态和动作开始直到该集结束所获得的总奖励。因此，我们可以将Theta的导数重新表示为所有轨迹的平均值，并对所有时间步进行求和。实际上，我们将策略梯度乘以回报，这样就可以得到一个稍微更低方差的估计。这意味着我们不需要单独对所有单词求和，然后将其乘以所有这些导数的总和，只需对对数进行求和。对于某些奖励条款来说，本质上就是这样。

因此，我们可以降低这种情况下的方差。在某种程度上，就像对每个奖励进行操作一样，因为你可以将其表达为奖励的总和。对于这些奖励中的每一个，你必须根据梯度的导数，也就是政策参数的导数，来计算完整轨迹的总和。现在，我们说，你不需要将其乘以所有这些。你只需将其乘以与特定奖励相关的因子即可。这意味着你将会得到稍低的方差估计量。

所以，当我们这样做时，最终我们可以实现所谓的"强化学习"。有谁听说过"强化学习"？是的，一些人听说过，但不是所有人。REINFORCE 是其中最常见的强化学习策略梯度算法之一。因此，您得到了 REINFORCE 算法。

那么它是如何工作的？算法的工作原理是在其中随机初始化 theta。你总是需要首先确定如何对策略进行参数化，所以在某个地方你已经做出了决定 - 就是决定如何对策略进行参数化。

现在，您将随机设置策略的值。然后，对于每一轮，您将使用该策略执行一次轮回。在这个轮回中，您将收集一系列行动和奖励，这些是根据您当前策略的采样得到的。因此，根据当前策略的样本，您将获得一条路径，然后对于路径中的每一步，您将更新策略参数。在这一轮中的每个时间步，我们都会更新策略参数。这将涉及到与之前相同的过程，乘以一定的学习率。在这个过程中，我不会用W，而是用α乘以Theta的导数，log Pi Theta，在Gt时的状态，其中Gt就是本轮回中的阶段。

从 st 开始的奖励总和是多少？所以，这就是正常的回报，就像我们用蒙特卡罗方法得到的那样。我们使用来自状态和行动的奖励来估计线性价值函数，就像在这里所做的一样。除此之外，我们将更新策略参数，并且会反复这样做许多次，最后返回 Theta 参数。是吗？我有一个问题。对于每一集，您是否从更新后的政策中进行抽样？我们需要和你谈谈。希望你准备好了。是的。嗯嗯。是的。刚才问的是对的。所以，在这种情况下，我-在完成一集的所有更新后，便可以进行这些增量更新。然后，在完成所有增量更新后，你会得到另一个带有新更新参数的剧集。是的？嗯，既然我们每次更新都会这样做，这会是一种有偏见的方法吗？

好的，根据您提供的内容，经过整理后变为：

因为我们每次都进行估计，这应该是对I的无偏估计——它仍然应该是梯度的无偏估计。虽然是随机的，但是我们没有以同样的方式存在状态和行为的概念。这将是渐近一致的，状态和动作的概念在这种情况下是不同的，因为我们只有这些策略参数。因此，我们不会在这里估计状态和操作的价值。这当然是渐近一致的，我认为这仍然是不偏不倚的。如果我稍后重新考虑，我会发一篇Piazza帖子，但我认为这仍然只是对梯度的无偏估计。这是一个好问题。好的，我回到我的幻灯片笔记，我想我想要的最后一件事...

在你提到的这段对话中，似乎讨论了关于如何计算差异以及在政策参数中如何尊重这种差异的问题。提到了Softmax、高斯和神经网络等不同的政策类别，以及如何将其参数化来获得采取行动的概率。简单来说，就是通过特征的线性组合和指数权重来确定我们的策略参数，以便我们可以得到实现所期望政策的概率。

在给定状态下采取行动的概率是多少呢？我们将对这些加权特征使用指数函数。因此，我们可以得到 $e^{\phi^T \Theta}$，然后除以所有操作的总和。所以，请注意，在我们的动作空间是离散的情况下，这是一个合理的方法。在许多 Atari 游戏以及许多不同的场景中，你会有一个离散的动作空间，因此你可以取这个指数，然后进行标准化。通过对指数求和，我们立即得到了我们参数化的策略类。如果我们想计算关于对数的导数，这将非常方便，因为我们这里有指数项和对数项。我们正在记录这个过程，希望能够从这个参数化的策略类中计算这个值。

我们得到的是这种类型的参数化策略的对数 Theta 的导数正好等于我们的特征。所以，这就是我们使用的任何特征表示，就像在运动机器人案例中，这会是完全不同的。这可能是，你知道，角度，或关节，或类似的东西。所以，这就是我们使用的任何特征减去您的参数的 Theta 预期值，指数为，在该策略下可能采取的所有操作的预期值。所以，这有点像说你观察到的特征与平均特征、动作的平均。这是可微分的，你可以解决它，然后它会给你一个分析形式。另一个非常流行的东西是高斯策略。为什么这可能是件好事？

嗯，这可能对我们很有帮助，因为我们通常有持续的动作空间。因此，如果我们有离散的动作空间，这将非常合适。通常，在控制和机器人领域，我们经常遇到这种情况。所以，您会有许多不同的参数，并且希望能够为它们设置连续的数值。嗯，在这里我们可以说，我们使用 s 的 µ。它可能是状态特征乘以某些参数的线性组合。好的。为简单起见，现在让我们假设我们有一个方差，但这是固定的。因此，我们也可以考虑不是这种情况，但假设我们有一些固定的方差项，这不是一个参数。这不是我们试图学习的内容。我们只是试图学习定义 µ 函数的 Θ，然后我们的策略将是高斯的。因此，动作将从高斯分布中取得每个特征的均值。好的。所以，我们将当前状态与均值进行比较。

然后我们会选择一个相对于当前状态的动作，在这种情况下，得分函数就是高斯函数的导数。 因此，得分就是高斯函数的导数，最终结果是状态的负 mu 乘以状态特征的参数除以西格玛的平方。 因此，这也可以通过分析来完成。 另一种常见的网络是深度神经网络。 这些是人们常用的形式之一。 在接下来的讨论中，我们将谈到另一种常见的方法。 在我们结束之前，我们会花大约五分钟进行一些早期的课堂反馈。 了解一下什么可以帮助你学习，你认为哪些方面可以改进，这对我们很有帮助。 所以，现在我开放时间，如果你们能到广场上填写这份问卷就太好了。 所有信息都将是匿名的。

我希望在周三能收到你们对此的反馈。 让我看看是否有需要重新做。 我要发布了。 好的，让我看看是否——我会固定在顶部，这样更容易找到。 [噪音] 现在应该固定在最顶部了。 是的。 接下来是课堂反馈调查，如果你去广场，这是一个非常简短的调查。 您可以向我们提供反馈信息。 那太好了。 下一步，我们将继续讨论策略搜索。 我们将讨论基线，这是减少方差的另一种方法。 这是一个非常活跃的研究领域，有许多关于深度强化学习和策略梯度的研究，所以我们将讨论其中一些工作，然后你将有机会尝试一下——在中期考试后获得有关策略梯度的实践经验。 因此，我们将在期中考试后发布与此相关的作业，这将是第三个作业。