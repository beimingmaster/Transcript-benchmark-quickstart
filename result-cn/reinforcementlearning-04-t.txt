好的。在开始技术讨论之前，我们先处理一些事务。我们现在要讨论的是课程内容。我们已经在广场上宣布了这些内容。如果你还没有收到我们在 Piazza 平台发布的内容，请确保你已经注册了 Piazza，或者给我们留言以便获取信息。这些课程的目的是深入探讨材料，并讨论作业内容。这些会议是有组织的讨论，而不是办公时间，你可以在办公时间询问有关作业的问题。这些课程旨在更深入地理解课程内容，我们的灵感来自于同事和学生在其他课程中对这些活动的喜爱程度，以及去年提出的一些机会，希望更深入地研究这些材料。所以，我们在 Piazza 平台上发布了这些内容。你可以选择加入这些会议，它们是可选的。如果你参加了足够数量的活动，我们将给予参加活动的学生额外百分之一的学分。具体的细节已经公布，我认为这个安排是很合理的。

如果我翻译得对的话，老师会发邮件给我。我认为这已经在广场上宣布过了。所以，如果你去广场，可以报名参加许多不同的活动。报名的目的是确保我们有足够的空间，但我相信我们可以容纳几乎所有你想参加的会议。最后一次课程将通过 Zoom 进行，专门为 SCPD 学生设计，但欢迎所有人参加。我们追踪参加会议的方式是，我们会提供一个代码，在资料中标明，然后你只需要在代码中标记你的出席情况。我们会记录最后一次会议，这样，如果由于某种原因你没法参加任何会议但想参加，你可以稍后查看资料，然后使用代码记录你的参加情况。我们将遵守斯坦福的荣誉准则，只有这样做的人才能上传代码。有人对会议及其内容有任何疑问吗？再次强调，这些会议是可选的，是深入了解资料的一种方式。有些人真的很享受这类活动。你可以看看自己的感觉，这是一个实验。

好的。除了会议之外，您还有其他问题吗？作业已经发布，这周的办公时间不变。如果有疑问，请随时联系我们或使用Piazza平台。那我们开始吧。请在提问或发表评论时留下您的名字，我会很感激的。今天我们将探讨在没有世界模型的情况下做出决策，重点是无模型控制。我们将讨论智能体在不了解世界运作方式且没有明确构建模型的情况下如何做出正确决策。在这种情况下，模型指的是环境的奖励和/或动态模型。今天我们将探讨不涉及构建语言、动态或奖励模型的方法，而是直接从经验中学习。在上次的讨论中，我们提到即使不知道世界运作方式，

尽管我们目前并没有明确的动态奖励模型，但我们会尝试评估所提供的政策。现在，我们将探讨强化学习中常见的一个真正问题，即当智能体对世界的运作一无所知，但仍希望最大化其预期奖励的折扣时，该如何做出决策。因此，在评估政策的优劣时，了解了政策的表现后，我们便可以开始思考如何学习好的政策。实际上，在课程一开始时就讨论过，如果有一个世界模型，你将如何学习做出好的决策或者计算出最佳决策。这便是我们现在需要重新考虑的内容。尤其是现在，我们将开始探讨优化和探索问题。虽然我们暂时不会做出总结，但很快就会进行。我们已经提过一些规划方面的观点，但现在我们将开始思考如何进行探索和优化。

因此，当我们考虑进入这个领域时，我会经历更多这些挑战。 我们需要重新思考如何确定具有高预期奖励贴现额的保单，因为智能体采取的行动可能需要一段时间才能看到结果，这时延迟会产生后果，我们需要考虑这种探索过程。当我们考虑马尔可夫决策过程，并且可能不构建模型时，这些问题会出现在哪里以及人们是如何建模的呢？我认为，强化学习采用无模型方式的第一个真正成功案例可能是西洋双陆棋，大约在1994年。在这个棋盘游戏中，他们训练了一个采用神经网络的代理来玩西洋双陆棋。尽管神经网络在 90 年代初可能已经过时了 10 到 15 年，但当时人们重新开始使用神经网络。

嗯，嗯，Gerald Tesauro 将其应用于双陆棋，并取得了一些非常好的结果，这可以说是在更大范围内展示强化学习的一个例子，您可以解决这些复杂的游戏。许多其他问题也可以在马尔可夫决策过程（MDP）中进行建模，无论是游戏、机器人、客户广告选择还是入侵物种管理。在许多情况下，我们并不知道模型的情况下操作。因此，今天我们要考虑的是，特别是在这些情况下，我们认为模型是未知的，但我们是否可以对其进行采样。有时候你确实知道模型，但对其进行采样非常昂贵。因此，对于如计算可持续性或气候建模之类的东西，也许你能够制定一个很好的世界模型，但实际运行起来会非常昂贵，因为模拟气候实际上是非常困难和耗时的。即使如此，你的模型也许会是这样。我的第二点是，当我们主要考虑在世界中学习时，我们可以想象一个机器人在世界中行走，这是一项昂贵的任务，因为...

机器人正在实时地完成这项工作。 但您也可以考虑让 AI 智能体与模拟器进行交互，这也是非常昂贵的。因此，我们今天要讨论的主要是所谓的政策学习，即我们通过直接经验获取关于世界的信息，然后尝试用这些信息来评估和改善我们的决策政策。同时，我们还将开始更多地讨论离策略学习，即通过获取关于世界的数据，并使用这些数据来替代直接经验来评估做事情的方法。这样，我们可以结合尝试不同事物的经验，去尝试理解我们自己尚未尝试过的事情。这三个概念非常重要。第二个概念也是非常重要，所以我会简要解释一下。想象一下，现在只有一个状态，就好像你处于状态 S1，你进行动作 A1，然后留在状态 S1，然后再进行动作 A1。或者你在 S1，你进行动作 A2，然后你再次在 S1，然后执行动作 A2。你希望能够结合这些经验，以便了解如何做到这一点。

即使你从未实际经历过这样的情况，在这种情况下，你希望能够根据以往的经验进行推断。因此，这种政策将属于一种非政策学习——一种与我们之前尝试过的政策不同的学习方式。当我们谈及Q-learning时，我们将更深入地讨论这个问题。那么，让我们从广义策略迭代开始。如果回顾政策迭代，我们之前在几堂课中提到过，政策迭代是在我们了解世界模型时首次看到的。因此，这是一种计算我们在给定正确事物上做正确事情的方法，即最大化我们期望的折扣奖励总和的政策。那么，当我们了解世界运作的方式时，我们如何实现这一点呢？我们需要动态和奖励模型。在这种情况下，我们可能会随机初始化一些策略。重新初始化意味着我们在所说的S中，pi等于S的某个A，通常是随机选择的一个。

我们执行了策略评估程序，首先计算策略的当前值，然后进行策略更新。我们利用所有可用信息，类似于进行贝尔曼备份，在计算一步更新的策略时，将当前的价值函数 V π 插入其中，结合已知的动力学模型和奖励模型。我们讨论了这样一个事实：通过这种方法，我们实际上实现了单调的政策改进，也称为政策改进定理。在这个过程中，我们利用动态模型和奖励模型，可以确保新的策略至少与旧策略同样好，或者更好。在有限状态空间下，这一过程最终可以实现。

在这种情况下，由于策略的数量是有限的，所以执行动态规划时保证会收敛。 只有 A 到 S 种可能的策略。 因此，在整个过程中，我们最多需要 A 到 S 次迭代。 在每次迭代中，我们要么选择更好的策略，要么保持不变。 一旦找到相同的策略，就算完成了。假设我们无法访问动态规划或奖励模型，有人知道我们如何实现相同的目标吗？我们可以维护另一个转移概率矩阵，这个矩阵可以通过与环境交互时计算得出。是的，更好的方法是，如果我们尝试从基本估计世界的动态规划和奖励模型，那么我们可以如何应用它呢？我们可以使用我们上次看到的一些方法来计算值函数，然后使用我们估计的动态规划和奖励模型进行政策改进。这是一个完全合理的做法，并且可能有其他想法可以帮助我们。是的，所以，这是我要介绍的内容 -

嗯，我觉得我们可以尝试直接估计特定状态或状态-动作对的值，而无需依赖模型来计算。这样我们可以放弃使用模型，直接估计特定状态的价值和状态-动作对的价值。是的，实际上是这样吗？是的。她实际上提到的正是我们今天要讨论的内容，即我们将专注于无模型控制。因此，我们不打算直接建立模型。个人而言，我很喜欢简单而有效的模型，但今天我们不打算考虑这些，我们将完全按照刚刚提出的方法进行操作，即计算Q值函数。请记住，Q值函数始终是状态和动作的函数。我们将直接估计Q值函数，然后可以利用该函数进行策略改进。那我们要如何做呢？这是一种朝着策略Q值评估的蒙特卡罗方法，看起来与评估策略价值的蒙特卡罗方法非常相似。

好的，让我们进行修改。因此，在之前的情况下，如果我们对于V这样做，我只是将其写成一种对比。因此对于V，我们只计算了状态数。现在我们已经统计了状态动作对的数量。之前我们可以在这里追踪G，这可以被理解为我们之前在S的所有集合中看到的奖励的总和。现在我们将为S、A执行此操作，然后我们将结束大语言模型的价值函数。我们将有一个Q pi。所以基本上几乎所有我们以前只有S的地方现在都有了S、A，看起来会非常相似。因此，我们假设我们仍然提供了一个策略，我们可以对一个情节进行采样，然后我们计算每个时间步长的G值。请注意，现在，与之前不同，我们要考虑事实上，它与该时间步的特定状态和特定操作相关联，然后针对每个状态操作对进行访问，而不仅仅是在我们第一次或每次看到该状态操作对时访问该状态动作对。

我们可以像之前处理首次访问或每次访问那样来处理。在这种情况下，我们只需更新计数、累加总奖励，然后估计 Q 函数。这个流程基本上和之前完全一样，只是现在我们在状态-动作对上执行所有操作。一旦确定了这样的策略，现在的政策改进相比之前更为简单。有了 Q 函数的估计后，我们可以直接计算出其 arg max 值。因此，我们将新策略定义为前一个策略的 arg max。那么，截至目前，在我们讨论的政策类型中，有人认为此方法存在问题吗？到目前为止，我们主要讨论的是确定性政策，即存在一种从状态到动作的映射，并且我们一直在考虑这些情况下映射的确定性。因此，我们总是为特定状态选择特定操作。是的，在继续之前，请先告诉我你的姓名。哦，你叫什么名字？

是的，您提出了一个重要问题：在探索和执行之间如何进行权衡是一个关键的挑战。如果我们只是不断地采样已知的路径，我们将永远无法了解其他可能的路径的信息，这可能会导致错过更好的决策。因此，我们需要不断尝试新的路径，即使我们无法确切知道哪条路径最优。这样做的好处是我们可以获取更广泛的经验，从而改进我们的策略。

有人可能会认为我们只需在一条路径上不停地采样即可，但这种方法会导致我们陷入局部最优解中。因为在每个状态下我们只能采取一种行动，而不能了解其他潜在的行动带来的结果。因此，我们永远无法知道如果我们在某个状态下选择了A2而不是A1会发生什么。这意味着我们的经验将受限于当前策略下的单一路径，从而限制了对其他可能性的探索。

为了避免陷入局部最优解，我们需要进行一定形式的探索，以便开拓新的可能性。通过不断尝试不同的路径，我们可以拓展我们的状态空间，从而获得更多关于不同行动结果的信息。这样一来，我们就能够更好地改进我们的策略，而不会陷入僵化和局限的情况。

在我们的政策中可能存在一些随机性，或者需要随着时间进行调整，但我们实际上可以尝试不同的行动，即使是同一行动的不同陈述，并清楚知道该采取什么行动。是的，首先给出一个名字。我的名字是 。我们是否事先知道整个动作空间？这是一个很好的问题。关于我们是否事先知道整个动作空间，是的，我们假设我们至少在这次交流中了解了整个空间，总的来说是的。由于您可能会在计算后得出一个比初始值低的值，您在下一次遇到时会尝试其他操作吗？这提出了一个很好的观点，与我们如何初始化 Q 值有关。您可以采取的一种方法是刚刚提到的，您可以在初始化您的 Q 函数时设定一个非常高的值，实际上，这被称为乐观初始化。这实际上可能是一种非常有用的探索策略，通过在某个环境中采取这种初始化方式，您可以获得一个收敛到最优策略所需的数据量的证明。因此，乐观初始化通常是一种非常有效的方法。

一个非常重要的方面是谨慎初始化，确保诸如数值等值符合经验。通常情况下，这种做法是可取的。尽管今天不深入讨论乐观初始化，但我们会在后续课程中涉及优化问题。因此，马尔可夫假设通常无法准确评估Q值。是的，但重点是，每当我们定义政策时，我们只会基于当前状态来定义，如果奖励仅取决于拥有的状态，则这就遵循了马尔可夫假设，尽管奖励并非马尔可夫。尽管如此，在制定政策时会像考虑马尔可夫环境一样行事。因此，在现实世界中可能存在违反马尔可夫性质的情况，我们谈论的所有政策都是基于马尔可夫环境的假设。这些政策只是将当前状态映射到行动，并非是历史的函数。因此，它们可能会起作用，也可能不会，因为现实世界可能违反MDP性质，如果如此，您将被限制在一组特定的政策中。

仅考虑从当前状态到行动的映射并不足以做出好的决定，因为决策过程取决于整个历史。这就是扩展蒙特卡罗方法的初步思路，它能够帮助估计行动值 Q，并进行政策改进。然而，为了真正改进尝试的政策，我们需要收集经验，因为我们目前并不了解世界的真实动态。因此，我们需要综合考虑政策评估、改进和探索，这可能需要一种交叉方式。或许乐观地初始化一切可能会有所帮助来进行探索。总的来说，我们需要全面考虑如何去获取这些信息。

估计 Q π 的困难在于确保访问每个状态-行动对至关重要。为了解决这个问题，一种简单的方法是通过平衡探索和利用，即有时随机地选择行动。一种常见的策略是 ε-greedy 策略，其中 ε 是一个小于1的值。在这个策略中，以 1-ε 的概率选择当前状态下根据 Q 值函数给出的最佳行动，以 ε 的概率随机选择行动。这种方法可以确保对每个状态-行动对进行访问，从而保证算法的有效性。

采取行动A时，以概率epsilon做出决策。因此，通过以概率1减去epsilon来做出其他选择，根据您的小组或对Q函数的估计，您当前认为最好的是选择另一操作的概率epsilon。这是一种简单的策略，但足够有效。在执行此操作之前，我们可以进行一个简短的示例来确保我们的想法是一致的。因此，让我们考虑如何对我们的小型火星探测器进行蒙特卡罗策略Q评估。火星探测器现在有两个可执行的动作，避免更多的推理。我列出了奖励函数。举个例子，如果您选择动作A1，您将获得之前讨论过的奖励1、0、0、0、0、0和10。现在我要调整一下，也就是说，您的动作确实-您的奖励取决于您的状态和...

你的行为导致在A2上的行动现在处处为0，然后最后我们得到了一个加5的奖励。我们设定伽马值为1，假设我们当前的贪婪策略是在任何情况下都采取行动A1，并且我们使用0.5的ε贪婪度。我们从ε贪婪策略中抽取了一条轨迹。再次强调，ε贪婪策略意味着将ε设置为0.5，这意味着一半时间我们会采取当前的贪婪策略A1，而另一半时间我们将采取A1或A2。因此，举个例子，这可能导致一个轨迹，比如状态3采取行动A10，在状态2，现在这是我们随机抽样的情况。我们掷了一次硬币。我们说，“这一次我要随机选择”，然后我必须再次掷硬币来决定我是采取行动A1还是A2，然后我选择了A2。我得到了0的奖励，然后轨迹的其余部分如下，我向您提出的问题，请随意和邻居交谈，当然，目前对于…

在使用蒙特卡洛估计的轨迹中，动作 A1 和动作 A2 结束时的所有状态的 Q 值估计是什么？在这种情况下，我们进行了第一次访问确保。

嗯，有些地方听起来似乎有点混乱。我会帮你重新梳理一下这段对话。

“呃，[噪音] 我对我们在 Epsilon 桌上选择的操作有疑问。”
答：是的。关于何时采取何种行动是很重要的。或者我们应该考虑传入该行动。问题在于，当你点击时，如果你选择做一些随机的事情，你是否应该包括通常会采取的行动，如果你很贪心？

“在某些方面，这就像只是选择一个不同的 Epsilon。”是的，“我最近聊天的次数相对较少，所以他们可能会有任何澄清问题。”

“[笑声]抱歉，我有个主意。”是的。所以，如果每个人都准备好了，那么你们觉得怎么样？

“嗯，在这种情况下，你会拥有 S3，因此你还没有看到的所有内容。”

在这种情况下，未经观察的动作状态对（Q）将保持为零。换句话说，当我们减少S3的Q到零时，S2h的Q会随之变为零，接着S3A1的Q也会变为零。因此，唯一非零的将是S1、A1的Q，在这个例子中是1。因为您已经看到它一次，每次看到它您会得到奖励1。这是一个答案。还有其他不同的回答吗？在这种情况下，所有观察到的状态动作对的Q值都将为1，而其他未观察到的状态动作对的Q值将为零。这是另一个答案。这在TD案例中是正确的。你之前提到的是对的，无论是上周、昨天还是星期一。还有其他人有第三个答案吗？你能再重复一下第二个答案吗？第一个选择是只更新S1、A1的Q值。第二个选择是我们现在观察到的所有Q值都将是1。

也许我在那里误解了。 我们现在有两个向量，一个是针对A1的Q值，另一个是针对A2的Q值，它们是不同的。 因此，我们有时采取行动A1，有时采取行动A2，我们只能更新我们观察到的行动的回报。 那么对于S3，我们应该采取什么行动呢？是A1吗？那么这意味着对于S3，它将是1，对于S3的Q值A1，因此我将填充所有的0、1、2、3和4。嗯，我们在S3中尝试过采取行动A2吗？没有。因此，它也必须是0，因为我们从未尝试在那里采取行动A2并获得回报。那么我们从S2采取什么行动呢？是正确的。因此，在这种情况下，我们得到的值是1。所以基本上，我们分享了你的经验。那么现在如果你要取最大值，

在我们这里，我们正在详细分析样本。由于我们处于蒙特卡洛案例中，我们将看到的是 TD 案例，或者稍后我们将称之为 Q 学习。然后，我们将把所有奖励加起来直到一个剧集结束。这里的G是所有这些步骤的总和。我们将Gamma保持为1，仅仅是为了进行数学计算，这只是一个添加项。我们是否应该考虑Q S1A1或Q S3A1可以只看一半？当每次访问时是否会有所改变？在这种情况下，它不会发生改变，因为当您访问两次S3时，到剧集结束的奖励总和始终为1。因此，您有两个1的计数，然后我们除以2。实际上，情况可能会有所不同。

在剧集结束时，如果你获得了不同的奖励金额，情况将会有所不同。是这样吗？同样的情况也出现在零样本情况下。请提醒我一下。是的，没错吧？也许我有所误解。是的，我以为我们应该说所有都是一样的，但我弄错了。你认为这两个行为有什么不同吗？其中一个属于投影，恩，是零样本的。我明白了。对于那件事我很抱歉。好的。好的。所以现在我们需要正式证明这样做是正确的。因此，我们将证明，就像我们之前在改进政策时所做的那样，如果你选择一个政策，嗯，π i，那就是，嗯，通过对Q函数进行贪婪尊重，这将确保产生单调改进，当你使用ε-贪婪策略时也是如此。所以，如果你使用某种ε贪婪政策，那么你可以收集数据，例如，

根据新政策，如果你持乐观态度，你将获得更好的价值。具体来说，我们有一个e-贪心策略Pi_i，然后我们会调用一个关于Q(Pi_i)的e-贪心策略，即Pi_i+1。因此，我们有一个贪心-ε-贪心策略Pi_i，在过去进行了一些探索和一些贪心。我们使用它来收集数据，然后评估该策略，得到Q(Pi_i)。现在我们要提取新的政策，进行政策改进。我将证明这是一个单调的改进。有人对上述内容有疑问吗？我们要试图证明的是，对于Q函数，s Pi_i的Pi+1，会比之前的值更好。至少要比之前的旧政策Pi_i的值一样好或更好。我们现在定义的方式是，这里的Q函数将是一个求和。

我们的策略是随机的，即根据概率πi+1在某个状态下采取行动的概率，乘以状态动作值函数Qπi。接着我们将根据具体内容重新定义它，来解释什么是ε-贪婪策略。在ε-贪婪策略中，我们要么以概率ε随机选择一个动作，然后将剩余的概率质量分配给所有可能的动作。这就是我们得到该方程的方式。换言之，这是策略的随机部分。因此，通过概率ε我们选择一个动作，然后始终遵循该动作。也就是说，我们以概率1-ε贪婪地选择动作，依据当前的Qπi选择最佳动作。因此，我们需要重写这个概念，第一个项保持不变，将对第二项进行拓展。

在这段话中，涉及到了数学运算和概率相关的内容，我将其重写如下：

这里没有添加任何内容，我只是把最后一项乘以1。但我用1除以ε来表示1。现在我要重新描述这部分内容。我要重写第一项，加上1再减去ε，当最大值超过a时，我会这样重写它：我们将利用这样一个事实，即当我们定义ε-贪心策略时，总结某个状态下所有动作的情况，这些动作就是我们在该状态下采取的概率。

因此，所有概率之和必须为1。我首先要除以一个值，然后乘以另一个值，最终表示为1减去ε除以1减去ε。之后我重新设为1，因为必须等于1，我们需要采取特定状态下的动作。策略必须满足这个条件，我们采取任何动作状态的概率必须等于1。然后根据这个表达式进行计算，因为我们要选择最佳动作。所以根据定义，最佳动作必须至少与采取其他任何动作一样好。因此我们要做的是将Q值加入其中。噪音所以它必须比我们之前看到的要小，因为基本上我们只是将Q值加入其中，并且我们不再选择最大值。

Q值是指所有Q值最多等于最大值，其他情况下它们通常更差。这样说对吗？一旦我们这样做，就可以消除1减去ε减去1的ε，我们会得到什么？在这里我们有两个看起来非常相似但不同的术语。我们拥有一个。让我们来看看。我们需要有人来将其解开。我们将继续这个过程。是的。右边是一个ε。好的。那么现在我要拿出来。如果我扯开这些项…

第一项和第三项是相同的，现在我们要减去一项并添加一项。这样整体就变成了中期。 [噪音] 只是之前的数值。是吗？第一行，我们将它更改为，对于给定状态 s，不是总和，Pi_i的A减去Epsilon到，[噪音]减去A的基数上的Epsilon 在这种情况下？是的。嗯，那个，[噪音]仍然是1减去Epsilon，[噪音]我的意思是，看起来就像——就是这样。知道了。这是否回答了你的问题？我想是这样。是的。所以，我们从第一个减去Epsilon到下一个步骤。[噪音] 因此，我们有一个一减Epsilon除以一减Epsilon，[噪音]，然后，我将它重新表达为A [噪音] 的总和，给定S减去Epsilon的A的Pi_i除以总和A，然后，如果对A求和，那就是第二项，

就是这个 Epsilon，第一项是 1。好的？是的。那不是[重叠]你能提醒我你的名字吗？[重叠]Pi，那个，什么？姓名？哦。Pi I 就是 Pi，就像 Pi I 加一，负 Pi，然后 Pi I。Pi I 加一。[噪音]你在想哪一行？[噪音]。其中- [重叠]意思是，我很抱歉。第二行。是的。你写了E Pi I 加[NOISE]一，负五[NOISE]，五Pi乘五。我只是不明白你的问题- 所以你在第二行是对的还是-？在里面。好的。Pi 加一是——抱歉。问题是什么？[噪音]。是的。这？是的。

对的。这段对话是关于强化学习中的一些概念讨论的。其中提到了关于将Q值推入总和中的计算方式，以及怎么通过贪心探索获得状态动作对的证据，并用这些证据来估计Q函数来改进策略。虽然有些部分内容比较抽象，但总体来说是在讨论如何通过算法和探索来优化决策过程。

如果你不进行探索，你的 Q 函数将保持不变。但是现在你正在进行探索，可以发现新的东西，如果更好的话，会在 Q 函数中体现出来。如果探索效果不佳，那就继续使用旧的。是的，这就是意思。因此，如果精确计算，你会看到这种单调改进。这是一个重要的部分。所以，如果你得到一个 Q 函数，发现有其他未尝试的行动可以改进，你会调整政策以尝试这些行动。在单调改进方面，假设 Q Pi I 已经被准确计算。这就是我们在规划时的想法，我们知道奖励模型和动态模型是什么，然后使用随机性计算价值函数。如果在这种情况下这样做，就可以保证单调改进，因为我们有...

Pi 的精确值，以及 Q Pi I 的精确值对于政策改进至关重要。若进行此类改进，保证是单调的，否则可能不可靠，特别是在只有 Q Pi I 的近似值时。在尝试新操作后，可能会对其效果作错误评估，这点至关重要。因此，在函数逼近中，考虑到无法准确计算 Q Pi I，迭代多次是必要的，以确保获得完美的 Q Pi I。在政策改进时，即使存在改进的机会，也需要考虑评估当前政策的时间频率。这是否意味着最终会收敛到类似 Q 函数的最优解呢？这是一个有趣的问题，需要综合考虑。

是的，我们讨论这个问题也很好。所以，这只是逐步的单调改进，在完全收敛方面会发生什么，我们会在一秒钟内讨论这一点。对吧？请提醒我名字。[噪音]问题与解答，当我想到 V Pi 时，我认为它是关于状态的函数。但是对于给定的状态来说呢？嗯。所以，让我重新回顾一下 Pi 是什么以及我们如何定义这个函数。现在我们将其看作是从状态到动作的映射，但它可以是一个随机函数。因此，它可以是动作的概率分布。这就是说，我可以有50% 的概率选择行动 A1，或者有50% 的概率选择行动 A2。例如，从某种需求角度来看，是吧？对的。我的意思是取决于你想要如何实现它，就像关心的那样。本质上，我认为是因为你处于某种状态，然后你对所采取的行动有一些概率分布，以确定你将采取什么行动。政策 - 所以当我们对此进行扩展时，我们在这里做什么，因为我们在考虑给定状态时的行动政策是什么。

我们说采取这项最大行动的概率是1减Epsilon。因此，根据Epsilon的概率，我们将选择其中一项行动。我们总结了可以采取的每项行动，将总和拆分为采取单个行动的概率和该行动的Q函数，以及采取其他每项行动的概率和这些行动的价值。这类似于我们的预期值。确实，在后面。因此，当我们谈论贝尔曼运算符时，我们指出如果获得相同的值函数-[噪音]，就可以停止迭代。在这种情况下，您是否必须尝试每个动作才能确定您已经完成了吗？这是一个政策改进中一个很好的问题，如果您采取与您相同的政策，那么您已经完成了，无需进一步改进。问题在于，是否真的如此，或者存在其他一些附加条件？嗯，这也取决于具体问题。

为什么我们不继续讨论下一部分呢？你知道，在什么条件下这些算法会收敛并收敛到最优呢？哦，你之前有问题吗？是的，这也表明只有在 Epsilon 为 1 时我们才能确保严格平等的探索。所以你的行动是完全随机的吗？嗯，问题在于如果策略是完全随机的，你能获得最优的结果吗？嗯，理论上是可以的。我的意思是，如果你能够在收敛时获得最优结果，就像你的 Q 函数收敛一样，那么你的策略就是最优的。相对于利益，你能够保证这种质量吗？不，我不认为可以。因为如果你的行为完全是随机的，通常情况下这只是你开始的方式，然后你试图改进。你能再解释一遍吗？我的意思是，如果你在意探索是否收敛，那么有些方法看起来比其他方法更好。因此，即使是随机行动，某些行动也会比其他行动获得更多奖励，这种差异会反映在你的 Q 函数中。在我们继续往后讨论之前，还有其他问题吗？可能有一个，

是的，还有一个问题。在您进行探索时，会排除argmax操作吗？我们应该怎么做？我们是不是应该排除argmax操作？就好比，通过探索您将了解，还有一部分是贪婪的。请问您叫什么名字？谢谢。嗯，不，您不需要排除它，不要排除。当您在探索时，您不会排除argmax操作。您选择全部。嗯，如果您愿意，这相当于一种定义。您可以这样做。但在最简单的版本中，包括这里的证明，我们假设当您随机行动时，只是从任何状态中随机抽样。这通常也更容易实施。好的，很好的问题。让我们在这里也记录下来。这是几位同事提出的另一个非常好的问题。嗯，我知道，随着时间的推移这意味着什么？我称之为单调改进，我们有什么保证？所以，我们的保证是，如果您无限次地组合所有状态动作对，并且您的行为策略收敛到贪婪策略。

你在这里描述的是在强化学习中的一种行为策略。这种策略是相对于当前 Q 值而言的贪婪策略。当一个情况趋向无穷时，让在给定状态 s 下，动作 a 的概率以 1 进入 argmax Q, s, a。这意味着在极限情况下，你会收敛到总是采取 Q 函数所建议的贪婪行动。这被称为GLIE。因此，你会无限次访问所有状态-动作对，但同时会使噪音收敛到零，变得对 Q 函数更加贪婪。有多种方法可以实现这一点，最简单的方法是随着时间推移逐渐减小你的epsilon或者ε贪婪策略。

好的，我来为您重新整理一下这段内容：

例如，您可以以1的速率将Epsilon减少到零，这就足够了。 这不是必需的，这与你想要通过经验来做的事情是分开的。 这只是为了在这些条件下进行展示。 然后，通常我们能够证明我们将会收敛到蒙特卡罗和时间差分方法的最优策略和最优值。 所以总的来说，当我们讨论其他一些算法时，通常会再次涉及到GLIE问题，以及对于学习Q函数有一些条件，那么您将会得到收敛到最优策略的保证。是的，就像我们见过的Epsilon一样。 是的，问题是这是保证的唯一方法，这里正在发生一些有趣的不同事情。可以保证您收敛到最优Q函数，而不一定收敛到最优策略。因此，您可以将Epsilon保持在非常高的水平。

您可能已经了解了关于最佳 Q 函数的大量信息，但您可能会在实践中遇到一些挑战。我们稍后会详细讨论这个问题。好的，让我们深入讨论蒙特卡洛控制。在这里提供了一个引入。因此，如果我们要进行蒙特卡洛在线控制，我们可以结合学习 Q 函数并尝试改进，而不仅仅是评估，这与之前讨论的有所不同。因此，我们可以像之前讨论的那样初始化 Q 函数和计数。然后，我们可以构建一个ε-贪婪策略。在这种情况下，ε-贪婪策略始终是以概率 1 减去 ε 为参数的策略。我们以概率 ε 选择 Q 函数的最大动作，表示为选择行动 a 的概率为 ε。所以我们只是将这个随机性混合在其中。

或者选择贪婪的方法。是的。如果我理解正确的话，您选择的操作将以概率 \(1 - \epsilon + \frac{\epsilon}{\lvert A \rvert}\) 选择最优动作，对吗？是的。好的。很多人会问同样的问题。所以基本上，您会贪婪地选择以概率 \(1 - \epsilon + \frac{\epsilon}{\lvert A \rvert}\) 超过a的动作。然后剩下的概率部分将用于探索。因为即使您以随机方式选择，也有可能选到当前最佳的动作。所以，嗯，这似乎与我们之前看到的非常相似。我们会采样一组，在完成这个集合后，在这种情况下，我将它定义为首次访问，不过您可以在每次访问时都这样做。每次访问我都能。同样地，嗯，这种方法也有其优点和局限。因此，在先前的情况下，如果您每次访问都这样做，但通常可以使用更多的数据，那么可能会得到略微偏差的估计器。噪音会稍微减小。嗯，在这样的情况下，我们所做的。

在蒙特卡洛在线控制中，我们通过维护动作对的计数并更新 Q 函数来改进策略。在完成一集后，我们更新参数k和ε，通常设定ε等于k的倒数，然后重新定义基于Q的新ε贪婪策略，进而进行下一集的学习。而在火星漫游者示例中，我们会有两个Q函数。在这种情况下，当遇到两个动作在相同状态下具有相同值函数时，这将导致平局。打破平局的方式可以根据具体情况做出选择。同时，也需要阐明新的ε贪婪策略。这些调整能帮助我们不断优化智能体的决策过程。

好的，我们的贪婪政策是什么？S1 的贪婪策略是什么？A1。S2 的贪婪策略是什么？二。那么我们对 S3 的贪婪策略是什么？一。那么其他一切又是什么呢？领带。根据你的实现，你要么总是可以定义你的贪婪策略，要么你只是想随机打破关系并跟踪它。可能会不断地随机打破关系。从经验上来说，这可能会更好，而不是预先定义一个贪婪策略，你可能总是查询 Q 的 argmax。如果你得到联系，就随机打破它们以获得更多探索。那么如果我们定义一个ε-贪婪策略，其中 K 为 3，并且我们的ε为 K 分之一，那么我们遵循的概率是多少？随机的。所以 K 是三，ε等于三分之一。因此，这意味着以三分之一的概率，

在讨论一个随机选择的策略时，有三分之二的概率我们会采用 π 贪婪策略。这个策略会影响特定剧集的更新。在无限探索蒙特卡罗的极限中选择贪婪行为，最终会收敛到最佳状态动作值。接下来，我们将讨论 TD 方法。类比于之前介绍的 Q-蒙特卡洛方法，让我们将注意力转向 TD。在 TD 中，我们会用到之前的数据，即状态 S 的动作值函数 V^π。更新公式为 V^π(S) = V^π(S) + α( R + γV^π(S') - V^π(S) )。

在采样噪声的情况下，由于我们只有S'的一个样本，并且正在自举，所以我们使用先前的估计V pi。因此，这就是TD学习的两个关键方面，我们正在引导和采样。在蒙特卡洛方法中，我们进行抽样但不进行引导。TD学习的一个好处是，我们可以在每个元组之后进行更新，而不必等到整个集合结束。因此，就像在蒙特卡洛方法中一样，我们用Q替换了所有的V，我们在这里也将完全做同样的事情。

现在，我们要考虑的是通常被称为时间差分控制方法的方法。我们现在可以使用时间差分更新来估计Q pi功能，例如ε-greedy策略，然后通过将π设置为Q pi的ε-greedy版本来进行蒙特卡罗改进。这是我们可以做的一件事。有一个名为SARSA的算法。

SARSA代表状态-动作-奖励-下一个状态-下一个动作，因此是基于这一原则运作的。首先，我们随机初始化我们的ε-greedy策略。然后，我们采取一个行动并观察奖励和下一个状态，接着采取另一个行动并观察另一个奖励和下一个状态。接着，我们根据以下方式更新我们的Q值：我们更新之前状态-动作对(ST, AT)的Q值为之前的值，即Q(ST, AT)。需要注意的是，我们不再通过π来索引它们，因为我们有这样的运行估计，而且我们的策略可能会改变。因此，我们得到的Q函数不再针对单一策略，而是对多个样本的平均值，并且随着时间的推移我们的行为可能会改变。因此，ST、AT的Q值等于ST、AT的Q值加上Alpha乘以(RT加上ST的Gamma乘以Q)。

这个方程的重要之处在于我将要采取的实际行动。你处于某一状态，做出一个动作，接收奖励，进入下一个状态，然后再做出另一个动作。一旦你知道下一个要执行的操作是什么，你就可以在SARSA中进行更新，因为你正在记录已经采取的操作。有了这些信息后，你可以按照正常方式改进策略。你可以设定ST等于argmax Q，类似于E-greedy包装器。与蒙特卡罗方法有两个不同之处。

我们正在更新元组，包括状态、动作、奖励、下一个状态和下一个动作。一旦完成这些，就可以更新我们的 Q 函数。我们可以一边进行这些更新，而不必等到整个剧集结束。同样，我们也不必等到剧集结束才能改变在环境中的行为。就像之前讨论的轨迹中，我们可能会多次遇到相同的状态。在这种情况下，可以调整我们在这些状态下的策略。这对于处理非常长的剧集特别有帮助。总的来说，我认为频繁更新策略通常很有帮助。有什么原因吗？哦对了，它们都是一样的，只是你可以选择在接下来的部分中将其写成 V 的形式，也可以不写。你可以将其设为 1 减去 Alpha 乘以...

在强化学习中，我们常用值函数来评估不同状态下采取行动的价值。通过更新值函数，可以帮助AI智能体做出更好的决策。在值函数更新中，有两种常见的方法，一种是Q-learning，另一种是SARSA。

在Q-learning中，值函数的更新公式将旧值加上奖励再加上下一个状态的最大值乘以一个衰减因子。而在SARSA中，更新公式是通过将旧值加上奖励再加上下一个状态动作对的值来进行更新。这两种方法在更新值函数时的处理方式有所不同，但目的都是帮助AI智能体选择最佳行动。

Q-learning通常会选择未来状态动作对中的最大值来更新值函数，这种方法在某些情况下可能会有一些缺点。对于一些特定场景，选择次优值反而可能更有益，因为追求最大值可能会导致智能体在早期做出错误决策。因此，在一些情况下，采取更乐观的态度可能比简单追求最大值更为有效。

总的来说，不同的值函数更新方法在不同情况下可能会有不同的效果，需要根据具体情况来选择适合的方法。在实际应用中，可以根据具体问题的特性来选择Q-learning或SARSA来更新值函数，以获得更好的决策效果。

他们指出，与Q学习相比，SARSA在早期阶段表现更出色，因为SARSA可以更现实地考虑下一步采取某个行动会带来的后果，而不像Q学习那样过于乐观。如果策略充满随机性，SARSA在早期阶段的表现更加稳健。不过从经验来看，一般来说两者最终会收敛于相同的结果。因此，对应的更新方程可能是 Q(s_t, a_t) = Q(s_t, a_t) + α * (r_{t+1} + γ * Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t))。但实际上，在讨论如何从未来的行动中获得信息时，你已经根据过去的行动进行了更新。因为你是在利用已学到的信息来做决策。

那么，先将那个问题搁置一边。为何我们要像讨论未来的行动一样讨论它呢？这样做的目的是什么呢？嗯，好吧。我觉得这与SARSA的特定术语没有关系，我的意思是，我想说的是，你需要等到最后一个"动作"（A）很重要。所以，不是这样说——在我们考虑TD学习之前，当你处于某一状态下采取某个动作会得到某种奖励并导致下一个状态时，你才能更新你的Q函数。现在我们只是说你必须等到你真正决定在下一个状态下要做什么。因为这将影响你如何更新Q函数，也是你将目标引入其中的方式。所以，就收敛性而言，嗯，需要考虑到一些不同的因素，嗯，所以，如果我们是，嗯，我们需要两件事。一是我们正在更新我们的Q函数，并且它将逐步更新，因此，正如我们之前讨论的，我们对Alpha需要有一些限制条件。

当 alpha 等于 1 时，通常 Q 函数不会收敛，因为这意味着您不会记住过去的信息。而当 alpha 为 0 时，您将不再进行更新。因此，通常需要一定的步长，这样可以缓慢增加但仍然能够收敛。 因此，这些都是充分条件。如果有类似于 Alpha T 的参数，其中 T 为非零值，则可以将其定义为 1 除以 T。根据经验，通常需要选择不同形式的学习率。Alpha T 通常被称为学习率参数，实际经验表明您不太可能使用它。通常，您可能会根据经验选择其他不同的方法，最终可能会使用一些小常数或者慢慢衰减的常数，这通常取决于具体领域。而从理论角度来看，这已经足够保证收敛。

哦，你的政策本身必须符合GLIE的条件。这意味着随着时间的推移，它会变得越来越贪婪，但在这个过程中你仍然可以对所有状态操作对进行无限次采样。然而，请注意，并非总是可能的，就像，如果你处于一个领域，在某个点之后无法再继续前进，这意味着你无法回到某些状态。举例来说，假设你正在驾驶直升机，但你损坏了直升机。因此，你无法回到那个状态，那么你就无法满足GLIE的条件，因为某个时刻你损坏了你的直升机，接下来你就无法确定继续驾驶直升机会发生什么。直升机在空中。因此，有些情况可能很难满足GLIE的要求，但我们通常会忽略这些情况，即使有一些非常有趣的工作。那么，我们该如何处理这些情况呢？在这些情况下，有人可能会认为这更像是一个临时问题，所以也许你有大约100架直升机，那么当一架直升机坠毁时，这被视为终止条件，然后你可以驾驶下一架直升机起飞。 因此，在信息的限制和无限探索的限制下，你可能也可能不会变得贪婪，但你在其中操作。

在某种程度上，我们进行有限的探索。稍后我们将更深入地讨论如何以更智能的方式进行这种探索，以及如何通过提供有限示例来确保我们能够了解需要多少数据才能学习一个良好的策略。这就是我之前提到的，通常我们不会使用特定步骤类型的方法。这里Q加上alpha，alpha乘以the-the。是的，没错。这是针对SARSA算法的。因此，这就是SARSA算法更新Q函数的具体方式。明白了吗？嗯，是的。对于蒙特卡洛方法来说，在GLIE策略下，Epsilon会降到1/t。一般来说，我们有类似的条件吗？这是个很好的问题。问题是，对于蒙特卡洛方法，我们是否有足够类似的条件呢？如果你只是，嗯，

如果您是第一次了解这些概念，那么这些概念可能有点晦涩难懂。因为您将获得的估计是一个无偏估计，它在无限次模型训练中只与所有可能状态和操作对中的少数几个相一致，从而确保您的回报会收敛。如果您以这种逐步逼近的方式来处理，当您考虑到 Alpha 的情况时，您需要类似的条件来确保其收敛性。我的意思是，您怎么知道这种条件会产生推动力？您怎么知道是否遵循了 GLIE（Greedy in the Limit with Infinite Exploration）的准则？就像在蒙特卡洛方法中，我们确实有条件概率。就像厄普西隆衰变一样，只要它——哦，对了，好问题。就像，您如何确保某些条件符合 GLIE 的要求。嗯，一个充分的条件是 Epsilon 大于 1减去呃，它大于 T 或者 1大于 I。您知道吗，这类工作就像，是的。哦，如果您想知道是否存在这种充分条件？是的，就像，当且仅当 Epsilon 被设置为 0 时，这种情况才会是 GLIE，但是是否存在一些例外或类似情况呢？是的，我认为它与序列非常相似，就像您说的那样。

在我们讨论与Q-Learning相关的问题时，选择特定操作而不是选择最大值是有原因的。 SARSA会选择特定动作，Q-learning则会选择最大动作。如果在早期阶段能更好，因为 SARSA 在处理负奖励时效果更好，而Q-learning在早期表现更好，因为其乐观性有助于探索。当然，可以结合SARSA和Q-learning，但这可能需要更清晰的思路。两者在不同情况下表现更好，取决于是否处理真实的负奖励。例如，Sutton和Barto的悬崖行走示例就是一个这样的情况。

有些行为会导致AI智能体喜欢从悬崖上掉下来，因此有一些行为非常糟糕。早期的乐观意味着你可能会做出许多糟糕的决定，并在一段时间内承受许多负面后果。虽然许多其他领域并非如此，但这取决于许多因素。当然，你可以将它们混合在一起。

关于Q学习，有一点很有趣，我们可以重新思考如何改进这一点。当我们变得贪婪地估计了最佳的Q值时，实际上与我们在SARSA中做的非常相似，不同之处在于，当我们更新这个Q值时，我们实际上只需要取MAX。因此，状态ST、动作AT的Q值将等于之前的值加上α或者加上动作A上的最大值。另外，现在需要注意的是，您可以尽早更新它，无需等待下一个动作值，因此您只需要观察这部分即可。

在大多数情况下，当你不需要实际观察下一个动作时，你可以执行策略改进。在这种情况下，你只需要更新当前状态下的策略，即更新你刚刚采取的动作的策略函数。特别是在大型状态空间中，这种方法可能会有所帮助。因此，我们已经讨论了初始化Q值是否很重要。尽管它不是渐近性质，也就是说，Q函数是否能收敛到正确的内容取决于其他条件，无论你如何初始化它，它都会最终收敛到正确的结果。然而，从经验上看，初始化Q值确实很重要，因此通常乐观地初始化Q值是非常有帮助的。因此，在探索时，我们将更多地讨论这一点。对于之前幻灯片上提到的第六行，应该是使用max还是argmax呢？非常感谢。

现在，如果我们进行 Q 学习。让我们来看看。我想，我会把这个当作一个练习，你稍后可以做，但是你可以做完全相同的练习用于 Q-learning，看看这些更新是如何传播的。就像在政策评估中蒙特卡罗与 Q 的比较一样，Q 学习也存在一些相同的问题。Q 学习只会更新你当前所处状态的 Q 函数。因此，即使在同一集合中稍后出现，你也会获得非常高的奖励。你不会像蒙特卡罗那样在剧集结束时反向传播信息。因此，Q-learning 更新通常比蒙特卡罗慢很多。就像输入一样，它会影响你学习做出更好决策的速度。要保证带ε-贪心的Q-learning收敛的条件，与 SARSA基本相同。我们需要确保事情是GLIE，我理解了，并稍作修改。

因此，如果您只是想要确保算法收敛，就需要频繁使用所有状态动作对。这需要在Alpha上具备这些条件。考虑到相同的条件，为了使Q函数收敛，您需要确保如何更新li，就像您在学习速率一样。您需要无限次地访问所有状态动作对，这样才能收敛到最佳的Q值。然后，如果您想确保您所遵循的策略是最佳策略，那么您需要成为一个GLIE。您还需要让您选择的策略变得越来越贪婪。在结束之前，让我简单介绍一下最大化偏差。最大化偏差是一个有趣的问题。为什么我们需要讨论这个问题呢？让我们回到这一点。那么在Q学习中，我们在做什么呢？在Q学习中，我们计算Q函数，然后进行ε-greedy策略。现在，我们需要更多的数据，正在进行重新更新。

我们的Q函数非常贪婪。因此，在平衡更新内容和获取更多证据之间，我们总是采取这种策略，并尝试利用这些知识进行一些随机探索。最大化偏差可能会导致问题。让我们考虑一个特定的例子：假设有一个单状态MDP，即只有一个状态。有两个动作，它们实际上都有随机奖励的均值为0。你可以将它们想象成高斯函数。我们讨论的是奖励实际上是随机的，而不是确定的。在这种情况下，无论你采取动作a1还是动作a2，你的期望值都是零，但在特定情节（特定步骤）上获得的值可能不为零，可能是1、-1或其他值。尽管平均值为零，但在特定步骤中你可能会得到不同的值。因此，sa1和sa2的Q值都为零，相同的值。

这些都是最佳的 Q 值和 S 值。 因此，让我们假设有一些先前的样本。 您已经尝试了多次操作 a1，也尝试了多次操作 a2，并计算了对此的经验估计。在这个唯一的状态下，我们对这些可以进行平均处理。假如我们设伽玛等于零。那么，我们实际上只是在估计即时奖励，没有考虑未来的奖励。我们只在这里考虑我们以前尝试过的这些行动。当我们平均得到所有奖励时，我们得到的是什么？现在我们的目标是对 a1 和 a2 的 Q 函数进行经验估计，并找到贪婪策略。问题是这可能存在偏见。即使 Q 的 k- 的这些无偏估计本身都是无偏的，也就是说，这两个估计 Q(a1) 和 Q(a2) 是无偏的，但当你取它们的最大值时，可能会存在偏差。让我们详细说明一下。因此，我们的 V Pi hat 等于 Q(a1)、Q(a2) 上 max 的期望值。

因此，我选择这两者中的最大期望值作为我的策略定义方式。我的策略是根据经验选择看起来最好的一个。从Jensens的定理我们知道，如果交换最大值和期望值，那么该值大于等于零。这恰好等于零的最大值。因此，关键部分在于这一点，这等于真正的V Pi。也就是说，无论我们计算什么，都可能是V Pi的有偏估计。那么为什么会发生这种情况呢？如果你只有有限数量的样本，比如说，如果我尝试动作a1有限次数，

在有限次试验中，可能会出现一些偶然情况，看起来会比较积极，如0.1而非0。当采取行动时，我会最大程度地利用这些情况。即使统计上存在一定机会，我也会立即利用看起来更有利的情况。这就是为什么会出现这种最大化偏见的情况。在马尔可夫决策过程中也可能会发生类似的情况。可以参考由Johns-Johnson Tsitsiklis和Shie Mannor撰写的论文，其中展示了这种偏见如何在MDP中产生。从本质上讲，如果你对这些Q函数有所估计，就会对数据中看起来不错的情况产生偏好，因此可能会出现最大化偏见的情况。因此，有人提出了尝试解决这种情况的方法，称为双Q学习。我们的想法是，我们将拥有两个不同的Q函数，而不是只有一个Q函数。

我们将要创建两个独立的 Q 值的无偏估计器，一个用于决策，另一个用于估计价值。这样做可以确保我们拥有一个无偏估计量。之所以采取这种方式，是为了在解决问题时提供一些帮助，避免过度偏向那些看起来不错的选择。现在，你在对收集的样本进行分析，试图估计每个行动的价值，并调整你的决策策略。Q 学习本质上是双 Q 学习，意味着我们会有两个不同的 Q 函数。在更新时，我们会以50%的概率更新一个，另一个以50%的概率更新。在这种情况下，我会略过其他幻灯片，向您展示它们之间的差异。有时，这些差异可能是相当显著的。

因此，在这种情况下，情况有些类似于我们在这个领域中采取不良行动的时间百分比。在这种情况下，您可能会遇到这样的情况：尽管实际上是错误的做法，但它是随机的。因此，相较于使用少量数据，奖励是确定性的这种更好但没有随机性的另一选择，似乎更能获得更好的结果。然后，Q-learning 可能会因为这种最大化偏差而受到很大损失。嗯，如果您使用相同的 Q 函数来立即定义您的策略，然后估计该策略的价值，双 Q 学习在这种情况下会更有效。因此，在实施这些方法时需要考虑这一点，而且成本也很小，因为您只需维护两个不同的 Q 函数即可。正确的。我知道这概念有点复杂，但请确保在上述详细信息中展开解释。嗯，在今天上传额外的幻灯片之后，请务必了解的主要部分是对如何在政策控制方面进行蒙特卡罗的了解，对于 SARSA 和 Q-learning 也同样重要。了解它们的更新速度是非常有用的。

无论是您必须等到剧行结束，还是信息传播速度慢，都需要了解算法收敛到最优 Q 函数的条件。感谢您。