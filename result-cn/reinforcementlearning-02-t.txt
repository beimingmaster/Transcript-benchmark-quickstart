好的，上次我们开始讨论强化学习的一般概述，引入了模型、值和策略的概念。让我们澄清一下，这三个概念分别是什么。有人能立刻记住在强化学习背景下的价值、模型或策略是什么吗？

在强化学习中，策略是一组行动，代理应该在特定情况下采取的行为方式。简而言之，策略定义了从当前状态到要采取的操作的映射。这可能是一个好策略，也可能是一个坏策略。我们评估策略的好坏通常根据它们预期奖励的折扣总和。

那么，有人还记得模型是什么吗？模型在强化学习中起着类似世界的代表作用，描述了环境如何随着代理的行动而变化。通常，一个模型结合了奖励模型、决策模型和动态模型，指定了对当前状态的反应，以及。

决定世界可能如何改变的行动，可以基于随机模型或确定性模型。奖励模型指定了在特定动作中代理所获得的预期奖励。今天我们要讨论的是，当你了解世界的模型时，你可以知道在特定状态下采取行动的可能后果，或者在采取行动后下一个状态的可能分布。这将有助于我们如何做出决策，特别是在涉及行为可能导致延迟后果时，你可能需牺牲即时奖励以最大化长期回报。通常，我们会考虑动态模型和奖励函数的统计或数学模型。政策是一个函数，将智能体的状态映射到行动，而价值函数则是奖励的预期贴现总和，来自某个状态。

今天我们要讨论的是建立马尔可夫决策过程。我认为这是一个不错的构建，因为它可以让人们思考，在某种情况下，即使你无法控制世界，世界仍然会以某种方式发展。对于一个被动体验世界的代理来说，这样的过程可能会获得什么回报呢？然后我们可以开始考虑控制问题，即智能体如何在世界中选择行动，以最大化未来预期奖励的总和。今天我们要关注的是马尔可夫决策过程，这是大多数课程的重点，我们研究的是一个与世界互动的代理。代理可以采取行动，通常表示为一些影响世界状态的动作，然后会获得一个状态和奖励。我提到过一个事实：这实际上可能是一种观察，而不是一个状态。

在考虑马尔可夫世界时，我们通常会想到一个智能体，只关注当前状态，最近的观察，比如无论机器人的激光测距仪检测到墙壁在左侧还是右侧，而不考虑之前的行动序列和观察历史。然而，我们也可以结合完整的历史信息来创建马尔可夫模型。在大部分情况下，我们会考虑直接传感器获取的信息。

马尔可夫过程指的是智能体用来进行决策的状态，是历史的充分统计量。这意味着为了预测下一个时间步的未来状态分布，我们使用时间步 t 进行表示。给定当前状态 s_t 和采取的动作 a_t，这就是动作的意义。

如果我们真的能够记住整个历史，那么历史回忆将是之前所有行动和奖励的顺序。 接着我们谈到目前为止所看到的情况。基本上，这让我们能够说，基于当前的一些整体统计数据，未来是独立于过去的。 因此，当我们考虑马尔可夫过程或马尔可夫链时，我们并没有考虑到有任何控制。没有任何行动。观点是，随着时间的推移可能会出现随机过程。无论我是否在股票市场投资，股票市场都会随着时间变化。你可以将其看作是马尔可夫过程，因此我可以被动观察特定股票市场如何随时间变化。马尔可夫链是随机状态的序列，其中过渡动力学满足马尔可夫特性。具体来说，马尔可夫过程的定义是，

嗯，讨论的是一个具有有限或无限可能状态的系统。你有一个动态模型，确定下一个状态在给定前一个状态情况下出现的概率。在这个系统中没有奖励，也没有采取任何行动。如果状态是有限的，你可以用一个矩阵表示它。这个转移矩阵展示了从一个特定状态开始达到下一个状态的概率分布是怎样的。

举例来说，假设我们回到之前讨论的火星漫游者的例子。在这个火星漫游车示例中，考虑到火星可能有各种不同类型的着陆点，我们设想火星漫游车从某一点出发。然后，它可以向左或向右移动，在不同的动作下（或者我们可以将这些动作表示为a_1或a_2），试图在环境中移动。

在这种情况下，过渡动态描述了系统如何从一个状态过渡到另一个状态，在这个过程中我们没有实际采取行动，只是在考虑它可能会以某种方式移动，比如说引擎可能会开启。

例如，你可以这样阐述：从特定状态 s_1 开始，有 0.4 的概率在下一个时间步长转移到另一个状态，而有 0.6 的概率保持在当前状态。那么，哪个维度表示起始状态呢？这是一个很好的问题。对于马尔可夫链，我们通常考虑其稳态分布。长时间运行后，它们的平稳分布将收敛到某种状态分布上，与起始状态无关。关于矩阵中哪个维度表示 - 你现在的状态？是的，在这种情况下，你可以想象，如果你从某个状态开始，那么这就是你在状态 s_1 的起始位置。

然后，使用点积，我可能会检查我在混合方面的表现是否正确。它可能在这一侧，也可能在另一侧，然后我可能已经对其进行转换。嗯，我猜你必须在这里做另一边的[噪音]处理。是的，会发生翻转。那么，您将获得初始状态。所以，1, 0, 0, 0, 0, 0, 1, 2, 3, 4, 5, 6，然后乘以P，这将为您提供下一个状态分布s'。对吗？[噪音]嗯，嗯，那么根据计算，像奖励这样的概率是多少，我猜是基于从状态1到状态2 [噪音]的奖励的概率？这是一个很好的问题，所以你知道，其中一个转移概率是考虑到[噪音]，这与他们说的有关，在这种情况下，我们只考虑马尔可夫链，所以没有奖励，也没有任何行动。[噪音]嗯，这只是指定某种进程的状态。就像你，假设你的智能体，嗯，对其引擎进行了一些配置。[噪音]你不知道那是什么，它被放在火星上，然后它开始移动。也就是说，这是如果...

在状态下启动的转换概率可以这样描述。假设初始状态为[NOISE] s_1，则保持在状态 s_1 的概率为0.6。因此，考虑到机器人的电机配置不同，下一个时间步仍保持在该状态的概率也为0.6。这种运作方式揭示了世界的动态特性，说明这个马尔可夫过程是你所处环境的一种状态描述。我们讨论的是这个世界的运作方式，这就好比假小火星车的世界。有任何疑问吗？对，当乘以P时，血清一云作用需要进行转置，我们可以看到所有的[噪音]细节。让我写下来并修正一下矢量符号，应该是这样的。

一、二、三、四、五、六。这是您当前可能处于的初始状态示例。初始状态意味着您的 AI 智能体最初处于状态 S1。接下来，如果您想了解下一个状态的概率分布，您可以使用转换模型 P 进行计算，具体取决于符号以及是否使用了该转换模型的转置。这将决定结果是在左侧还是右侧。从上下文来看，这应该是很明显的，但如果有疑问，请随时询问我们。这个过程的意义是什么呢？也就是说，通过矩阵乘法运算这个向量，它告诉您，如果您从状态 s_1 开始，下一个状态会是怎样的？接着，它会告诉您，您仍然以 0.6 的概率处于状态 s_1，以 0.4 的概率处于状态 s_2。这就形成了您的新的状态分布。

我认为应该换位思考。这只是一个例子，展示了您将进入下一个状态的概率分布。您对这个有任何疑问吗？好的。所以，这只是一个表明随着时间推移世界运行方式的转换模型，我用矩阵表示法将其编写得更加简洁。但如果更容易考虑的话，可以通过给定前一个状态推断下一个状态的概率来思考。因此，您可以逐个列举这些状态，如果状态数量是有限的，可以将其表示为矩阵形式。因此，如果您想考虑在这种情况下，AI 智能体随着时间推移可能会发生什么，或者过程可能是什么样子，您可以对情节进行采样。假设您的初始状态是 S4，然后您可以使用一个单独的独热向量来表示它。您将其乘以相应的概率，这样就得到了您可能进入的下一个状态的概率分布，并且世界将从中抽取一个状态。因此，您的 AI 智能体一次只能处于一个状态。例如，如果我们看状态 s_1，

AI 智能体有0.6的概率在状态 s_1 中放弃，或者有0.4的概率转换到另一个状态。因此，世界会从这两种结果中采样一个，可能是状态 s_1。在这种情况下，我们有与状态 s_4 类似的动态。从状态 s_4 转移到状态 s_3 的概率是0.4。进入状态 s_4 的概率是0.4，停留在当前位置的概率是0.2。因此，如果我们要对随着时间推移 AI 智能体可能经历的情况进行采样，可以从状态 s_4 开始，然后可能转移到状态 s_5，再可能是 s_6、s_7、s_7、s_7。因此，只需从这个转移矩阵中采样就可以生成特定的轨迹。就像这个世界，你知道世界的动态是什么，然后自然会选择其中一个结果，就好像从某种概率分布中抽样一样。是否有人对此有疑问呢？好的，这只是让您了解一个特定的情节。我们对这些情节感兴趣，因为稍后我们将讨论这些情节的奖励，以及如何比较我们可能在这些情节中获得的奖励，但就目前而言，这只是一个过程。

这里只是在介绍一系列状态。 接下来，我们要添加奖励。 在这里涉及到马尔可夫链。 那么，什么是马尔可夫奖励过程呢？虽然我们还没有采取行动，但现在有了一个奖励函数。 因此，我们仍然保留了之前的动力学模型。 现在有了一个奖励函数，用来表示当处于特定状态时，预期获得的奖励是多少。 我们也可以引入一个折扣因子，用来平衡即时奖励和未来奖励，或者考虑到我们对未来奖励的重视程度。 因此，类似之前的情况，当我们的状态是有限的时候，奖励可以用矩阵形式表示，也可以表示为一个向量，因为它仅表示在每个状态下获得的预期奖励。在火星漫游者 MRP 的情况下，我们可以说，状态 s_1 的奖励是1，状态 s_7 的奖励是10，其余状态的奖励都是零。 是的。

这些术语是否都与当前状态有关？我记得你之前提及过有个选择。那我们为何不考虑一下这一点呢？这是一个很好的问题。我记得之前提到过，马尔可夫决策过程的奖励可以是当前状态、动作中的状态，或者下一个状态的状态动作的函数。目前我们还处于马尔可夫奖励过程中，还没有采取任何行动。因此，在这种情况下，您定义奖励的方式要么是当前状态，要么是状态和下一个状态。一旦我们开始考虑奖励，我们就可以开始考虑回报和预期回报。所以，首先让我们明确一下什么是时间跨度。时间跨度只是一个情节中的时间步数。这有点像代理行动的时间跨度有多长，或者这个过程持续了多长时间，有可能是无限的。如果它不是无限的，那么我们将其称为有限马尔可夫决策过程。我们上次简短地谈到过这些。嗯，但我们经常需要考虑到这样的情况，嗯，一个特工可能永远在行动，或者这个过程可能永远持续下去，没有停止的可能。

今天股市上涨。根据我们的预测，这种情势可能会持续一段时间。我们暂时不会尝试短期评估，更偏向长期观察。因此，我们已经做了一些准备。回报的定义是指从当前时间点开始，通过贴现计算获得的奖励总和，可以持续到无限范围内。简而言之，回报表示：如果在时刻T我获得直接奖励，然后转移到另一个状态，通过Gamma值度量奖励回报，之后再次转移，通过Gamma的平方值度量，如此循环。价值函数则是回报的期望值。在确定性过程中，这两个概念是相同的。但通常情况下，在随机过程中它们会有所不同。因此，我这里所说的确定性是指，无论从哪一个状态开始，只要总是进入相同的下一个状态。

在考虑下一个状态时，期望值就扮演着返回的角色。然而，通常我们对于随机决策过程感兴趣，因为平均值会因为不同的运行而有所不同。举个例子来说明，让我们先谈谈折扣因子。折扣因子有其独特的用途，一定程度上也是为了数学上的便利而引入的。通过折扣因子，我们可以确定只要奖励函数是有界的，那么预期未来收益的总和的价值函数也将是有界的。根据经验，人们通常表现出像有折扣因子的倾向，即对未来奖励的重视程度低于即时奖励，这也是许多企业的操作方式。当折扣因子Gamma等于0时，代理行为是短视的，只关注即时奖励，而当Gamma等于1时，

这意味着未来的奖励对于你来说与当前的奖励一样有益。 然而需要注意一点，如果你只是出于数学上的便利而使用折扣因子，那么如果你的情况始终保持有限，数学上来说使用折扣因子设为1是可以的。有人对折扣因子有任何疑问吗？是的，我的问题是，折扣因子gamma是否总是必须以等比数列方式递增，或者我们这样做的原因是什么？这是一个很好的问题。我们在这里定义了使用以指数几何方式增加的gamma是必要的。这是一个不错的选择，具有非常好的数学特性。还有其他问题吗？好的，那么有什么例子可以给出呢？

当我们考虑火星漫游者的奖励定义时，假设我们从状态 s_4 开始，然后经过四个时间步骤到达状态 s_7。在这种情况下，根据定义，状态 s_4、s_5和s_6的奖励为零，只有在时间步 s_7 上，我们获得了奖励为 10。但是这个奖励会受到折扣因子的影响，这里折扣因子是 1/2，因此奖励值会受到 1/2 的三次方的影响。在决策过程中，有时会设定一个有限的时间步数限制，因为在与网站交互时客户通常平均进行两到三次互动，且许多情况下状态空间是有界的，所以过程持续固定的时间步后可能会被重置。

因此，该特定剧集的回报仅为1.25。当然，我们可以对任何特定的剧集定义这一点，这些剧集可能会经历不同的状态，即使它们从相同的初始状态开始，因为我们有一个随机的Transformer模型。所以，在这种情况下，AI智能体可能只会停留在s_4、s_4、s_5、s_4中，并且不会获得任何奖励。在其他情况下，它可能会持续向左移动。因此，当我们考虑期望值函数时，需要对许多函数进行平均。当我们对所有这些进行平均时，我们可以在不同的时间步长获得不同的奖励。那么，如何计算呢？现在你可以通过模拟来估计它，这有点受到我之前展示的启发。因此，你可以假设一个初始的状态分布。

AI 智能体可以从一个或多个起始状态出发，直接推导出其运行流程。假设我们有一个Transformer模型转换矩阵和一个奖励模型，可以像之前展示的那样进行推导。这个过程可以重复很多次，然后取平均值。这将逐渐收敛到价值函数，因为价值函数实际上代表了预期回报。通过模拟实践，可以利用数学边界来估算需要进行多少次模拟才能使经验平均值接近真实预期值。精确度大致会下降到完成的模拟次数N的平方根倒数部分。这意味着，如果想要确定马尔可夫奖励过程的价值，可以进行模拟来得到一个价值的估计，而不需要假设马尔可夫结构的条件。

事实上，并没有充分利用马尔可夫奖励过程这一概念。这只是一种估计总回报的方法——总结奖励。因此，在某种程度上，这很好，如果你在基于某些数据进行估计时使用它，或者假设情况是，你知道这是一个动态模型，但它也是基于数据估计的，这可能是错误的。这可以让您对该过程的运行方式有更好的估计，如果您真的可以将其推广到世界上。然而，它并未充分利用这样一个事实：如果世界确实是马尔可夫的，我们可以添加额外的结构以获得更准确的估计。那么，更准确的估计意味着什么呢？我的意思是，如果我们想要以更低的计算成本估计价值，这种更好意义上的方法是什么。因此，马尔可夫结构使我们能够根据当前状态获得的即时奖励来分解值函数。这样，我们只需考虑价值函数，而不需要关注过去的历史。

将未来奖励的折扣总和乘以折扣因子加权，并且将未来单词的折扣总和表示为 V，我们可以用 V(s'）来表示。

好的，一旦我们完成这些工作，就可以分析和求解价值函数。请记住，所有这些都是已知的，也就是众所周知的。我们现在需要做的是计算 V(S)。

在这种情况下，考虑到我们正在处理无限范围的过程，价值函数是固定的。如果涉及到自循环，那就更好了，因此如果可能会回到相同状态，就没有问题。您确实需要精确定义这个矩阵。可以采取正向推导，也可以采取反向推导。大多数情况下是这样的。所以，如果我们想直接解决这个问题，这是可以的，它是可解析的，但需要矩阵求逆。如果存在N个状态，假设有N个状态，通常计算量在N平方和N立方之间，具体取决于所使用的矩阵求逆方法。是的。这个矩阵是否可能没有逆矩阵，或者是否有某种属性（比如列之和为一）使其无法逆转？这个问题是否存在没有可逆元的可能性？这是一个很好的问题。

嗯，我认为这基本上是不可能没有反驳的。我正在努力思考在某些情况下是否可能违反这一点。如果是的话，抱歉，请继续。好的。噪音。是的。所以，我认为有几个，如果有一个...如果最终是一个零矩阵，这取决于事物的定义方式。但我会仔细检查，然后在广场上发送一张便条。是的。好吧，实际上我认为关于过渡矩阵的最大方面，让我仔细检查一下，这样我就不会说任何不正确的话，然后我会在广场上发送一条说明。这是一个好问题。这就是计算这个的分析方法。另一种方法是使用动态规划。因此，在这种情况下，它是一种迭代算法，而不是一次性算法。因此，在这种情况下的想法是，您将值函数。

无论您在任何地方的初始化为零，实际上都可以初始化为任何值，这并不是很重要。只要一直这样做直到收敛即可。我们要做的是使模型的预测接近我们希望看到的结果，即行李员备份。基于马尔可夫性质，我们说状态的价值等于我们获得的直接奖励加上未来奖励的贴现总和。在这种情况下，我们可以简单地使用这一原则来推导出迭代方程，通过使用当前状态的值来指导和计算下一个状态的值，并对所有状态都执行这一操作。这样计算复杂度降低了一些，因为它仅为状态数量的平方，您需要对每个状态执行该操作，然后对所有可能的下一个状态求和。通常我们会进行完全收敛，这意味着在这种情况下，我们所做的是制定一个规范。因此，通常我们会计算 V_k 减去 V_k-1。我们需要持续这样做，直到其值低于某个阈值 epsilon。

因此，这种方法的优点在于每次更新迭代都更加经济，而且开始行动时也会带来一些好处。 当我们开始行动时，另一种方法并不那么容易实施，但我们也会看到其相关性。 因此，有两种不同的方法来尝试计算马尔可夫奖励过程的值，也可以说有三种方法，一种是模拟，第二种是分析。 分析方法要求我们逐步确定一组有限的状态，第三种方法是动态规划。 目前，我们只在状态空间有限的情况下定义所有这些，但稍后我们将讨论当状态空间无限时的情况。 因此，现在我们终于可以进入马尔可夫决策过程了。 马尔可夫决策过程与马尔可夫奖励过程是相同的，唯一不同之处在于现在我们会采取行动。 因此，我们仍然有动态模型，但现在我们有一个为每个动作单独指定的动态模型，并且我们还有一个奖励函数。 正如之前卡米拉所问，我认为奖励可以是当前状态、状态和动作到状态和下一个状态的函数，在今后的大部分时间里，

在一个马尔可夫决策过程中，智能体使用其状态和行动函数。智能体处于一个状态，采取行动后即时获得奖励，然后转移到下一个状态。因此，当您观察这个过程时，您会看到类似于状态（s）、行动（a）、奖励（r）的内容，然后转移到状态s'。因此，马尔可夫决策过程通常被描述为一个元组，其中包含状态、行动、奖励、动态模型、折扣因子等。由于动态模型的定义方式，可能出现这样的情况：当您采取一个旨在使您转移到状态s的特定行动时，您可能无法完全成功地到达该状态。这是因为当您深陷于某个状态时，下一个状态可能是随机的。这也是为什么有概率存在的原因。在许多情况下，我们没有完美的环境模型。也许如果我们拥有更好的模型，那么事情就会更趋于确定性。因此，我们使用随机性来近似这些不确定性的模型。

也许你的 AI 智能体有些问题，导致有时候会在地毯上卡住，有时候又能继续前进。我们可以将这描述为一个随机转移矩阵，有时会停留在原地，有时会跳转到下一个状态。或者当遇到沙子或其他情况时也会是这样。就如同当你驾车前往SFO时，有时会遇到交通堵塞，有时则不会。你可以想象在状态空间中引入更多变量，来尝试使结果更为确定性，又或者简单地说，“有时上班遇到很多红灯，所以迟到了，而其他时候没有红灯，所以准时到达。”

接下来，让我们考虑一下火星漫游者的马尔可夫决策过程（MDP）。现在，我们定义了两个动作 A1 和 A2。你可以将它们视为向左或向右移动的代理，但更常见的做法是将它们视为确定性操作，更容易理解这个特定示例。因此，我们可以建立转移矩阵来准确展示前一个动作导致的下一个状态。所以在这种情况下，如果代理尝试…（原文不完整，未完待续）

当处于状态 s_1 时执行动作 a_1，将保持在该状态。否则，通常会迁移到下一个状态。如果尝试执行动作 a_1 并执行动作 a_2，则向右移动，直到到达状态 s_7，在那里停留。因此，就像课程一开始所述，马尔可夫决策过程策略规定了在每个状态下应采取的行动。策略本身可以是确定性的，也可以是随机的，这意味着可以基于所处状态选择下一步动作，也可以具有确定性映射。它表明每当处于特定状态时，总是执行动作 a_1。在课程中，我们主要考虑确定性策略，而在进入策略搜索时，将更多讨论随机策略。因此，若您有马尔可夫决策过程和策略，则会立即指定马尔可夫奖励过程。一旦指定了策略，就可以将其视为引导马尔可夫奖励过程。

您只能指定您所在的州的行动分配，所以您应该考虑奖励是什么。在该策略下，您在任何状态下获得的预期奖励。同样地，您可以根据采取不同操作的权重对您的Transformer模型进行平均，从而定义马尔可夫奖励过程的转换模型。因此，考虑马尔可夫决策过程和马尔可夫奖励过程之间的这些联系是有用的，因为这意味着如果您有一个固定的策略，您可以使用我们刚刚为马尔可夫奖励过程描述的所有技术，主要是模拟、分析、解决方案或动态规划，来计算策略的价值。因此，如果我们回到迭代算法，那么它与以前完全相同，与马尔可夫奖励过程完全相同，只是现在我们通过策略对奖励进行索引。因此，为了了解特定策略的价值是什么，我们可以通过模拟、分析、解决方案或动态规划进行计算。

通过选择策略来确定行动的操作，从而实例化奖励函数。这样做的目的是简化确定性策略，并根据在该状态下采取的操作来索引要查找的转换模型。这种方法被称为特定策略的备份。这样一来，我们能够清楚地说明在该策略下状态的价值，即当前状态下遵循该策略所获得的即时奖励加上根据该策略所获得的未来奖励的折扣总和。无论最终处于何种状态，我们都将继续遵循这一策略。这就是指定的V^pi_k-1的内容。如果我们通过继续遵循最近转移到的任何状态的策略来获得未来的折扣奖励，会发生什么。因此，在火星漫游者的马尔可夫链中，当进入下一个状态时，系统会进行这种预期的继续奖励。

让我们来看看目前的马尔可夫决策过程。对于这两个动作，奖励函数仍然如下：如果您处于状态 1，则执行任何动作将获得 +1 的奖励；而在任何状态下，执行任何动作将获得状态 s_7 的奖励 +10，其他情况奖励为零。现在假设您的策略一直是执行动作 a_1，且折扣因子为零。在这种情况下，这个策略的价值是多少？这个问题提醒了策略价值的迭代计算方式。除了状态 s_1 和 s_7 外，其余状态值均为零，其中状态 s_1 为 +1，状态 s_7 为 +10。恰如其分。由于我没有再次展示转移模型，所以这是一个有点棘手的问题。

说的完全正确。在这里，Transformer是什么并不重要，因为伽玛等于零。这意味着所有东西都会消失，所以你只会得到即时的回报。因此，如果您的折扣系数为零，那么您只关心即时奖励。因此，该政策的即时奖励是因为所有动作和状态s1的奖励始终为+1。所有其他动作和所有其他状态的奖励都是零，除了在状态s7中，无论采取什么行动，都是10。因此，这就等于1。这就是值函数的含义。好的。那么让我们看看另一个例子。现在我们有完全相同的过程。我已经写下了状态s6的动力学模型的具体选择。因此，让我们想象一下，当您几乎总是向右在状态s6时，您有50%的几率在行动A1下停留在那里，或者有50%的几率进入状态s7。这就是上面所说的。

然后还有许多其他动态模型，我们不需要担心进行这个计算。 状态 s_1 的奖励仍然是+1，状态 s_7 的奖励仍然是+10，中间所有状态的奖励仍然是0。 让我们继续考虑，我们仍然尝试评估采取动作a_1政策。 正如我们刚才提到的，V_k等于1, 0, 0, 0, 10，现在我们要做的是再进行一次备份。 所以我们要从V_k=1开始计算V_k=2。那么，在这个具体政策下，对于状态s_6，每个人应该花时间计算其价值是多少。因此，您可以使用这个等式来计算，因为我知道我之前的价值函数是什么，我已经指定了它为1, 0, 0, 0, 10。现在我要做一次备份，我只需要您计算一个状态的值。

如果你愿意的话，你可以帮其他州做这件事。嗯，如果用这个方程来计算 s_6 的新值是多少？它只需要输入奖励的价值是多少。该值是动态和旧值函数的特定数字。我举这个例子的原因是为了展示信息在此计算中如何流动。所以你从一开始就开始。让我先过去一下吧。因此，当您开始时，您将在各处将值函数初始化为零。您所做的第一次备份基本上将价值函数初始化为到处的即时奖励。然后，您将继续进行这些备份，本质上您正在尝试计算此政策下每个状态未来奖励的预期折扣总和。因此，如果你考虑一下这一点，那就是状态 s_7 很好这一事实的信息，会有点向后流动到其他状态，因为他们说“好吧，我一直处于状态 s_4 我不现在没有任何奖励，但在这个过程中的几个时间步骤中我可能会得到任何奖励，因为我可能会达到真正伟大的+10状态。”

因此，在我们进行策略评估的迭代过程中，我们开始将有关未来奖励的信息传播回早期的状态。 现在我要请你往前迈进一步。比如说对于状态s_6，如果它之前的值为零，现在我们进行一次备份，它的新值是多少？所以如果我们再问一个问题，不妨让我们想一想。如果我们不知道状态 s 的值函数，我们能否通过反向跟踪来找到它呢？这是一种计算值的方法，让我们耐心等待提出问题，因为它确实是一种计算值函数的方法。所以我们的做法是，我们已经将值函数初始化为零。尽管这不是真正的值函数，只是一种初始设定。这个过程允许我们不断更新每个状态的值，直到它们不再变化。这样就能为我们提供预期的折扣奖励总额。

现在可能会有这样一个问题，他们是否能够保证停止变化？我们稍后会讨论这部分。我们会了解到一个事实：整个过程肯定是一个收敛过程，所以不会永远持续下去。因此，价值函数之间的距离会缩小。这就是折扣因子的好处之一。因此，如果没有更直接的问题，我建议我们都花一点时间，然后与邻居比较一下这个计算过程中得到的数字。只是为了快速检查贝尔曼方程是否有意义。好的。所以，无论你走到哪里，希望我们有机会与你旁边的其他人进行比较，检查是否有任何理解。在我们继续之前，我想回答之前提出的一个问题，即分析解决方案是否总是可逆的问题。让我们回到那个话题。因此，在这种情况下，因为P是一个随机矩阵，它的特征值总是小于或等于1。如果你的折扣因子小于1，那么I（单位矩阵减去gamma乘以P）总是可逆的。这就是这个问题的答案。

因此只要γ小于1，这个矩阵就总是可逆的。好的。那么让我们回到这个，我们将需要任何方式来实现我们想要的其他一些重要属性。在这种情况下那是什么？因此，其直接奖励是零加上γ乘以[NOISE]0.5，我们保持在该状态的概率乘以s_6的前一个V，再加上我们进入s_7的V的0.5概率。这将等于0加0.5乘以0，再加上0.5乘以10。这只是一个例子，如何计算一个贝尔曼备份。这又回到了我原来的问题，即你似乎正在使用不带π上标的V_k来评估它。哦，抱歉，这应该是π。这只是一个错别字。那是正确的。问题只是是否应该是π在那里。是的。

是的，您说得很对，谢谢您的指正。好的，现在让我们开始探讨马尔可夫决策过程的控制。目前只需留意一点。因此，我引领我们完成了，或者我们只是通过迭代来进行政策评估，您也可以通过分析来完成，或者通过模拟来完成。然而，作为一个很好的类比，我们现在要开始考虑控制。那么，我所说的控制意指什么呢？在这里，控制将是这样一种事实：最终我们不仅关心评估策略，通常我们也希望我们的智能体实际上学习策略。因此，在这种情况下，我们不会讨论学习策略，我们只会谈论计算最优策略。因此，存在唯一的最优值函数是非常重要的。Soum，MDP和无限水平有限状态MDP的最优策略是确定性的。因此，这就是为什么我们只需要关注确定性策略就足够，在有限状态MDP的无限视野中，这是一个很好的理由。

在进行计算之前，让我们先考虑可能存在多少个策略。假设有七个离散状态，在这里代表机器人所在的位置。共有两个动作，我们将它们标记为"a_1"和"a_2"，而不是简单的“左”和“右”，因为前者并不代表一定会实现这个动作。我们也可以将这个情境视作常见的随机场景，所以用"a_1"和"a_2"来表示。现在问题是，针对MDP（马尔可夫决策过程），有多少种确定性策略，最优策略是否总是唯一的？我们只需要花一分钟或两分钟，随便与邻居讨论一下针对这特定情况有多少确定性策略。然后，一旦你回答了这个问题，考虑一下状态的数量|S|和动作的数量|A|，即这些集合的基数。有多少种可能的确定性策略呢？第二个问题是这些策略是否总是唯一的。我可以尝试猜测一下在这种情况下有多少确定性策略吗？

在这段对话中，讨论了从状态到动作的映射，以及策略在不同状态下的选择数量。策略指的是在给定状态下采取特定行动的映射关系。在这种情况下，每个状态有两种行动选择，共有七个状态。一般来说，策略的数量是动作空间到状态空间的映射。虽然策略是有限的且存在界限，但并非总是唯一的。虽然价值函数是独一无二的，但在某些情况下可能存在并列的情况。

操作或策略可以具有相同的价值，这是有可能的。这取决于具体的情况。你的意思是像独特的最优值函数吗？是的。因此，问题是我能否解释存在唯一最优值函数的含义。我指的是状态的最优值。因此，预期回报的贴现总和，可能会有不止一个最优策略，但至少存在一个可以使该状态价值最大化的最优策略。这看起来只是一个值。或许我们稍后在讨论契约性质时会更加清晰。因此，对于每个状态而言，这只是一个标量值。这个值准确表示了预期贴现回报的总和，这是在最优策略下的最大预期贴现回报的总和。是的。关于我们的[听不清]策略—— 当我们第一次定义策略时，我认为我正在描述整个哈希表，

在每种状态下只有一个行动，而不是所有可能的组合。 令人惊讶的是，由于行动，每张地图上的数字都是 2 到 7，而不仅仅是州的数量。 为了更清楚地表达，你知道，这是什么意思吗——有多少种策略，以及可能性——看起来可能是线性增长的，但实际上是指数级增长的。 嗯，我们在这里定义了决策策略的方式，确定性决策策略是从状态到行动的映射。 因此，这意味着我们可以为每个状态选择一个行动，这就是最终会导致指数级增长的原因。 在这种情况下，让我们假设只有六个或两个状态，而不是七个状态。 现在我们有s_1和s_2。 你可以有行动a_1-a_1，你可以有行动a_1-a_2，你可以有行动a_2-a_1，或者行动a_2-a_2。 你必须这样做，所有这些策略都是不同的。 这就是空间最终呈指数增长的原因。 当然。 当你有A的S次方时。我假设A指的是每个州的可能行动，假设...

在不同的州可以采取不同的行动。然而，是否能够施加不同的限制，这是一个问题。通常情况下，我们假设所有操作都适用于所有状态，但实际情况并非总是如此。在很多情况下，某些行动可能是特定于某个州的。医疗干预的空间通常很广，但有时某些行动可能只适用于特定的州。总的来说，每个州都有可能拥有不同的操作子空间，因此可以通过对每个州相关的操作集进行乘积来确定操作的数量。然而，目前来看，我们可以简单地将其视为一个统一的行动空间，适用于所有州。

至于最优策略和有限视野问题，在马尔可夫决策过程（MDP）中是确定性的，并且是静态的，也就是说不会随时间步长而改变。我们之前已经开始讨论这个问题了。

所以，这意味着在状态 s_7 下，存在一个最佳策略，即使在不同的时间步（比如时间步1、时间步37或时间步242）都会选择同样的行动。直觉上，如果总是有可能采取行动，那么未来的时间步就是无穷的。因此，如果当前总是选择在状态 s_7 执行操作 a_1，当再次遇到 s_7 时，仍然会做出相同的选择，因为这被认为是最佳的。正如我们刚才讨论的，最优策略并不一定是唯一的，因为可能存在多个策略具有相同的价值函数。那么，我们如何计算这些呢？一种选择是策略搜索，关于这一点我们将在几周后讨论函数逼近和处理非常大状态空间的方法。即使在表格情况下，我们也可以考虑进行搜索。因此，刚才讨论的确定性策略的数量是 A 到 S。

策略迭代通常比枚举更有效。这里的枚举指的是评估每种可能的策略，然后选择效果最好的策略。如果有大量计算资源，可以考虑使用枚举，但通常效率会更高，会选择策略迭代。在策略迭代中，我们尝试不同的策略，并逐步改进，直到不能再改进为止。初始时，我们将随机选择一种策略进行评估，然后根据效果来更新策略。

因此，我们最初从一些随机策略开始，然后根据当前对最佳策略的猜测（π_i）进行迭代。我们随机初始化策略，然后讨论是否在下一秒内进行改变或返回到原策略，通过执行值函数策略来评估。使用相同技术评估策略，因为这是一个固定策略，这意味着我们处于马尔可夫奖励过程中。接下来进行政策改进，真正的创新是在政策改进方面。为了定义政策改进，引入了国家行动价值，与之前讨论的状态值（V）不同。

首先，我将执行动作 a，这可能与我的策略要求的不同，接着在下一个时间步骤中将遵循策略 π。这意味着我将获得立即奖励，然后转移到新的状态，取决于当前状态和采取的动作，之后将执行策略 π。这就定义了 Q 函数，策略改进的作用在于提升已有策略的价值。策略评估是计算策略价值的过程，现在我想看看是否可以对其进行改进。需要牢记的是，我们已经知道动态模型和奖励模型。因此，我们可以通过 Q 函数计算来进行改进。我们可以根据策略得到的先前值函数进行计算。

我们计算 Q^pi，这表示无论我们采取哪种操作，对于所有的A和S，它都可能相同。因此，我们对所有的A和S执行这个操作，然后我们计算出一个新的策略，使Q值最大化，这是一个改进步骤。因此，我们只需要进行这个计算，然后选取最大值。根据定义，根据定义，这个值必须大于或等于Q^πi(s, πi(a))，因为a等于πi(a)，抱歉，应该是πi(s)。因此，arg max要么与之前的策略πi相同，要么不同，只有在有一个替代操作的Q函数更好时，你才会以不同的方式进行选择。

因此，根据定义，$Q^{\pi}_i(s,a)$

接下来的一个关键问题是，为什么我们要这样做，这个想法是否明智？因此，当我们审视这一点时，让我们仔细看一下这些内容。我们可以得出，这将带来一些有趣的政策改进步骤，涉及到一些不同的事情。所以，我想强调一下其中的微妙之处。我们在此计算了 Q 函数，然后我们就得到了这个结果。我们得到了 Q^π_i(s,a)。

根据定义，新的策略至少应与旧策略一样好。这意味着新的Q函数的最大值必须至少等于旧值，这是奖励函数对旧策略价值的定义。这一点是值得鼓舞的。然而，值得注意的是，采用arg max后会获得新策略。这意味着正在计算新的Q函数。这个新的Q函数代表了什么呢？它代表了，如果采取行动，然后从那时开始便按照旧政策行动，选择使每个状态价值最大化的行动。通过对每个状态执行此过程，将会定义一个新策略，对吧？

就像我认为这可能是相同的，也可能是与您之前的政策不同的政策。这是奇怪的事情。所以，这就是说，如果您遵循arg max A，然后从那时起遵循您的旧政策，您将保证比以前做得更好。但奇怪的是，从那时起我们就不再遵循旧政策了。我们将永远遵循这项新政策。因此，请记住，我们正在做的是彻底改变我们的策略，然后我们将评估所有时间步长的新策略，而不仅仅是第一个时间步长，然后从那时起遵循旧策略。所以，至少应该有点不清楚这是一件好事。应该是，好吧，所以你是说，如果我采取这一不同的行动，然后遵循我的旧政策，那么我知道我的价值会比以前更好。但你真正想要的是这项新政策总体上更好。因此，很酷的事情是，您可以通过进行此政策改进来证明它比旧政策单调更好。所以，这只是说，我们是说，

如果我们对一项行动采取新政策，然后永远遵循这个新政策，即使这个新政策被称为πi，我们可以保证价值函数至少与之前一样好，因为我们始终遵循这个新政策。为什么我们说政策价值上取得了单调提升，尽管我们说保证至少与以前一样好呢？首先，让我们澄清一下什么是单调改进。我的意思是，如果所有州的新政策价值都大于或等于旧政策价值，那么改进就是单调的，因此新政策必须至少与旧政策相同，或者更好。我的观点是，如果旧政策不是最优的，那么在所有存在严格不平等的州中，新政策都会比旧政策更好或至少相同。为什么这会奏效呢？这是因为有几个原因支撑这一点。让我们简单地浏览一下证明。因此，在这里我们所说的是，V^pi_i(s)... （未完，建议继续完善）。

这些是我们政策中的旧数值。这些值代表了我们以前的政策观念。在这里，必须确保其值小于或等于Q^pi_i(s, a)。

我们首先将其与我们得到的 Q^pi_i 的 a 的最大值相乘。根据定义，由于我们知道 s 的值和动作 a 的值，我们可以得出 s' 和 a' 的概率也将小于或等于 Q^pi_i(s', a')。

重点是要注意大于或等于这一点。 因此，如果您将其完全嵌套在内部，您将得到 pi_i+1 的值。因此，有两个关键技巧。 首先要注意的是，请注意 V^pi_i 始终不高于 Q^pi 上所有动作 a 的最大值。然后利用 pi_i+1 的定义重新表达这一点。接着通过 Q^pi 确定 V 的上限，并不断地扩展它。这样，您可以执行这些步骤，然后将其重新定义 - 当您使用 pi_i+1 代替所有操作时，您现在已经定义了 pi_i+1 的值。因此，这让我们知道根据定义，新的 pi_i+1 的值至少与之前的值函数一样好。所以我将它放在那里。好的。接下来可能会有一个问题是我们...

我们知道我们会得到这种单调的改进。问题是，如果政策不改变，它还能再改变吗？策略迭代是否有最大迭代次数？那么，迭代是什么意思呢？在这里，迭代次数用 i 表示。我们可以通过多少个政策迭代呢？那么，为什么不花一分钟时间考虑一下这个问题，也许可以和你周围从未见过的人谈谈，看看他们对这两个问题的看法。策略是单调改进的，是否有我们之前读过的最大迭代次数？

只是为了今天的时间，因为我希望我们也尝试完成价值迭代，为什么不让别人给出，一个猜测政策是否可以，如果政策停止改变，它是否可以再次改变？我的意思是，如果按照政策 pi，

这里的问题在于如果 i+1 的状态转移概率 pi 等于所有状态的第 i 步状态转移概率 pi，那么会再次改变吗？有人想知道这个猜测是否正确。一旦停止改变，就再也不会改变了。所以是不可能的。第二个问题是，策略迭代是否有最大次数？是的，是有的。迭代次数不能超过策略的数量。这是正确的。因为我们知道最多只有一个最优策略。由于改进是单调的，你永远不会回到相同的策略。因此，存在最大迭代次数。明白了吗？太好了。这只是——嗯，我现在会跳过这个问题，这样我们可以讨论一下值迭代过程，但事实上，一旦策略不再改变，你的 Q^pi 也会保持不变。所以在最优策略确定后，你不能再改变它了。一旦融合完成，你将一直停留在那里。好的，策略迭代以某种方式计算出策略中的最佳值。

策略迭代的理念是持续改进一种能在无限空间内产生价值的策略。与之不同的是价值迭代方法。值迭代方法表明，在有限步骤内计算最佳值，从一步开始，逐步增加步数。这种方法不断迭代以逼近最佳值。因此两者是不同的，对吧？策略迭代意味着你始终持有一种政策，并了解其价值，尽管可能不够好。而值迭代表示您始终知道最优值在策略中的情况，但需要采取行动达到k个时间步骤。它们计算不同的内容，但最终会收敛到相同的结果。当涉及到值迭代时，考虑到贝尔曼方程是很有用的。贝尔曼方程和贝尔曼备份操作符是马尔可夫决策过程和强化学习中经常讨论的主题。我们之前提到的政策价值约束条件指的是...（文本不完整，无法继续完整翻译）。

将其直接奖励和未来奖励的贴现总和称为贝尔曼方程。 马尔可夫决策过程约束称为马尔可夫过程。和我们之前看到的类似，可以将其视为备份运算符，这意味着可以将其应用于旧的价值函数并将其转换为新的价值函数。在某种政策评估中，也可以使用这些运营商。不同的地方是在于在那里取最大值。取最大值为已获得的最佳即时积分加上未来奖励的折扣总和。有时使用BV表示法代表Bellman运算符，这意味着将旧的V插入其中并执行此操作。那么，价值迭代是如何进行的呢？该算法如下总结。首先，将所有状态的价值函数初始化为零。然后循环直到收敛。

如果你正在考虑一个有限的横向问题，在你今天没有足够的时间到达对方地平线的情况下，你可以考虑设想：你将能够到达那个地平线。对于每个状态，你都会有贝尔曼备用算子。也就是说，对于我在第k个时间步的状态的价值，我会考虑到最佳的即时行动，以及上一个时间步获得的旧价值函数和未来奖励的折扣总和。 Vk代表，考虑到我还要执行k个时间步，对于状态s的最佳值是什么。因此，将其初始化为零是个好主意，因为在这种情况下，如果您希望达到最优结果，就如同您有多个时间步一样，这也是合理的。如果没有更多的时间步可供采取行动，您的值将为零。第一个备份会告诉您，如果只能采取一个行动，您应该立即采取哪个行动是最佳的。然后你会继续向后追溯，继续讨论，如果我必须行动两个时间步会怎样？如果我必须执行三个时间步呢？

在每种情况下，您可以采取的最佳决策顺序是什么？对于贝尔曼操作来说，如果我们回顾一下策略迭代正在做的事情，您可以通过固定策略来实例化这个贝尔曼操作符。因此，如果您看到顶部带有“啊，嗯，π”的B，并且说，好吧，您不是在执行最大化操作，而是指定您要执行的操作是什么。因此，您可以将策略评估视为基本上只是重复应用此贝尔曼备份的固定点，直到V停止收敛并停止变化。所以，在策略迭代方面，这与我们在这些贝尔曼运算符中看到的并在执行此最大化操作之前看到的非常相似。想了解一下我们是否可以稍微了解一下收缩运算符。这就是值迭代的作用。这与策略迭代和评估非常相似。让我谈谈收缩方面。因此，对于任何运算符，嗯，

让 $O$ 表示一个运算符，$x$ 表示 $x$ 的范数。这里的 $x$ 可以是像值函数那样的向量，在这种情况下我们可以将其视为 L2 范数、L1 范数或者无穷范数。所以，假设一个运算符是一个压缩算子，这意味着当你将它应用于两个不同的事物时，你可以把它们看作值函数，然后它们之间的距离会在之后缩小，或者至少在应用运算符后不会增加。所以为了举例说明，我会保存这个例子以便以后使用。如果你想查看这个例子，请随时在课后找我，或者我可以在广场上演示。这就是压缩算子的正式定义。在这种情况下，我们将其视为两个向量间的距离，在应用此运算符后不会增加而是会减小。所以，判断值迭代是否收敛的关键问题在于贝尔曼备份是一个压缩算子。只要 $\gamma$ 小于 1，它就是一个压缩算子。这就意味着，如果你这样做——假设有两个不同的贝尔曼...

当我们对两个不同的价值函数分别执行贝尔曼备份时，它们之间的距离会缩小。为了证明这一点，我们考虑存在两个不同的价值函数，分别标记为 k 和 j，它们可以是任意不同的值，并不一定与值迭代相关。举个例子，一个值函数可能是1、3、7、2，另一个可能是5、6、9、8。

我们可以将这两个不同的价值函数表示为向量，并在应用贝尔曼备份操作后重新计算它们。在贝尔曼备份操作中，我们考虑的是最大化立即奖励加上未来奖励的折扣总和，其中我们插入了两个不同的价值函数。如果我们对这两个函数分别选择最大化操作后的结果，它们之间的距离会缩小，具体表现为价值函数之间的相似性增加。

通过设定最大值 a，可以最大程度地减少差异。然后，你可以取消奖励。这是第三行的情况。接下来要做的是限制并确定这两个值函数之间的差异，取这两个值之间距离的最大值为上限。因此，你可以选择这两个值函数之间最不同的部分，然后将其从总和中移除。现在，你在对总和为1的概率分布求和。这给了你这个结果。因此，这意味着只要小于1，贝尔曼备份就必须是一个收缩运算符。应用贝尔曼运算符后，两个值函数之间的距离不能比之前更大。所以，我认为一个很好的练习是，假设它是一个收缩算子，这意味着它必须收敛到一个固定点，必须有一个唯一的解决方案。因此，如果您反复应用贝尔曼运算符，您将到达一个固定点，该固定点是唯一的。

矢量值是一个有趣的概念。 如果您只在意收敛后的结果，初始化和值将会影响吗？这也是个很好的问题。嗯，我认为我们可以到这里结束课程了。幻灯片中还有更多内容可以讨论，如有任何问题，请随时在 Piazza 上向我们提问。谢谢。 [噪音]