好的，让我们开始吧。作业二应该进行得顺利。如果您有任何疑问，请随时联系我们。关于提案项目，如果您有任何疑问，请在我们的办公时间内或通过 Piazza 联系我们。 现在有人有其他问题需要我回答吗？好的，今天我们要开始讨论策略梯度方法。在当前强化学习领域，策略梯度方法可能是应用最广泛的方法。因此，熟悉这些方法非常有用。每当我们谈论强化学习时，我们都会不断提到这些主要属性，我们希望智能体能够学会做出决策，优化这些决策，处理延迟的后果，进行探索，并通过统计方法在真实的高维空间中有效地完成这一切。上次我们讨论的模仿学习是...

通过不同的方法，我们可以为我们的 AI 智能体提供额外的结构或支持，使其能够尝试更快地学习。模仿学习是一种利用人类示范提供结构支持的方式。在讨论函数逼近时，我们已经看到了其他排序结构或人类先验知识的方法。因此，当我们考虑定义 q 时，例如将 q 定义为 s、a 和 w，其中 w 是一组参数时，我们实际上在根据如何表示价值函数来强加某种结构。这种选择可能非常强大，就像假设价值函数是线性的那样，也可能非常微弱，比如使用深度神经网络。因此，当我们规定这些函数的近似值和表示形式时，我们隐含地做出了选择，决定我们希望为模型注入多少结构和领域知识。

为了我们的 AI 智能体的学习，我们今天要开始讨论的话题是策略搜索，这是一个可以非常自然地输入领域知识的方法。在我们今天的一些机器人示例中，我们将看到策略搜索的作用，这也是一种非常有效的学习方式。在我们深入了解无模型强化学习，并尝试扩展到非常大的状态空间之前，策略搜索起着重要作用。有一些人向我提出了关于非常大的动作空间的问题，这是一个非常重要的话题。尽管本季度我们不会深入讨论这个问题，但我们会讨论动作空间何时是连续但低维的情况。我们已经开始讨论状态空间何时是高维且非常大的情况。因此，我们讨论了通过某种参数化方式来近似问题，比如使用参数 Theta 或者我们经常使用的 w，来表示函数的参数化。我们会利用我们的价值函数来进行讨论。

在强化学习中，我们通常定义一个值函数，用于计算特定状态或状态行动的预期奖励贴现总和，然后我们可以从这个值函数中提取策略。然而，今天我们要讨论的是直接参数化策略。在这种情况下，我们不再使用值函数，而是直接对策略进行参数化。在表格设置中，策略只是一个简单的状态到行动的映射，可以看作是一个查找表，为每个状态记录要采取的行动。现在，我们不再尝试编写这样的策略表，而是要将策略参数化，即使用一组权重（或Theta参数）来表示。尽管今天我们主要使用Theta参数，但可以将其视为权重。这种参数化策略的方法是我们今天要讨论的重点。通过参数化策略，我们将有更多灵活的方法来表达策略。和状态行动价值函数类似，这种方法会对学习空间产生重大影响，因为它有效地定义了我们可以探索的策略空间。

在你可能学习的政策类别中，我们再次着重介绍无模型强化学习，这意味着我们并不假设能够获得关于世界动态或奖励的先验模型。在本季度初，我们列出了一些图表，现在我想回顾一下。我们之前一直在讨论价值相关的内容，但并没有太多涉及模型，虽然模型也很重要。现在我们已经讨论了很多关于价值函数、基于方法的问题，接下来我们要谈论策略，具体说是直接策略搜索方法。正如你所料，有很多工作尝试将这两者结合起来，通常被称为演员-评论家方法。你会尝试明确维护参数化策略，以及明确维护参数化评论家或价值函数。所以，政策是我们接下来要讨论的一个问题。好的，今天我们将开始讨论基于政策的方法。那么，为什么要这样做呢？ 嗯，【噪音】嗯。

这实际上可以追溯到上周我们讨论的模仿学习。在模仿学习中, 我们谈到了一个事实：有时人类很难编写奖励函数，因此对他们来说，模仿演示策略可能更容易。同样，在某些情况下，也许编写策略空间的参数化比编写状态-动作价值函数空间的参数化更容易。另外，它们通常在高维或连续动作空间中更有效，让我们能够学习随机策略。虽然到目前为止我们还没有讨论过，但我会给你一些例子，说明我们确实在哪些情况下需要随机策略。另外，它们有时具有更好的收敛性能，尽管这可能会有些争议，这取决于我们将其与无模型或基于模型的方法进行比较以及我们如何进行大量计算。因此，计算可能是一个重要因素。真正最大的缺点之一是它们通常只会收敛到局部最优值。

因此，你会最终收敛到一个好的策略，这是一个很好的策略，但通常不能保证收敛到全局最优值。有一些技术可以确保收敛到局部最优解和全局最优解。今天我将重点介绍其中一些技术，但一般来说，在深度强化学习中使用的几乎所有方法都是基于策略梯度，仅能收敛到局部最优解。另一个挑战是，通常我们通过尝试评估策略然后估计其梯度来实现这一点，这可能会导致样本利用率低下。因此，当采用基于梯度的方法时，可能需要大量数据来估计梯度。

那么，为什么我们需要某种随机策略呢？在之前提到的表格设置中，是因为我们想要这样吗？如果回想一下第一堂课，当我们有一个表格马尔可夫决策过程（MDP）时。

在确定性且最优的策略π下，MDP设置中无需随机策略，因为总是存在与最优策略具有相同值的确定性策略。在表格MDP案例中，这是不必要的。然而，并非总是在表格MDP案例中采取行动。举个例子，谁熟悉石头剪刀布？大多数人都熟悉。如果不熟悉，可能用其他名称玩过。在剪刀石头布游戏中，两人可以选择石头、剪刀或布，他们之间存在胜负关系。在这种情况下，如果有确定性策略，

在这样的情况下，我会改变策略，而大语言模型非常容易受到损失。但是，采用统一的随机策略基本上是最佳选择。这里所说的最优性指的是什么？在这种情况下，我所指的是，如果你胜利了，你可以得到加一的回报，如果你失败了，你可以得到零或减一的回报。在这门课程中，我们不会涉及太多关于多智能体案例的讨论，但这是一个非常有趣的研究领域。在这种情况下，环境并非未知。环境会对我们采取的策略做出反应，而且可能是具有对抗性的，因此我们需要制定可以应对对手的稳健策略。因此，第二种情况是Aliased Gridworld。那么，在这种情况下，为什么随机性如此重要呢？因为我们并不是真正处于随机环境中，而是处于对抗环境中，我们还有其他代理与我们互动，他们的行为是不固定的，可以根据我们的策略进行调整。因此，环境不是像石头、剪刀、布那样，不管我们做什么行为，环境都不会做出预定的选择。

在过去，AI智能体能够对这些情况做出反应。但是，由于具有非平稳性和对抗性，它表现得有些复杂。另外，它不是马尔可夫过程，因此只能部分可观察且存在混叠现象，这意味着无法准确根据传感器来区分多种状态。举个例子，机器人配备了激光传感器，可以感知走廊上的位置。然而，在很多不同的走廊中，第一个障碍物的距离在所有180度方向上看起来都相同。这就是一个简单的例子。

在网格世界中，假设智能体的传感器无法区分灰色状态，并具有特定的特征，例如可以探测到它的周围是否有墙壁。基于这些特征，智能体可以判断自身朝向的不同情况，比如北、东、南、西方向是否有墙壁。这样，智能体可以基于具体情况作出相应的应对方案。

在另一种情况下，可能会有相同的灰色状态。因此，如果我们使用某种近似值函数来进行基于值的强化学习，它将考虑到这些特征，比如我应该采取什么行动以及周围是否有墙壁。或者我们可以采用基于策略的方法，也会考虑这些特征，但是直接尝试根据这些特征做出决策，这些决策可能是随机的。因此，在这种情况下，AI智能体正在努力学习如何在这个世界中导航，它的目标是到达奖励点。所以它会避开骷髅和交叉骨头，因为那些会带来负奖励。由于状态相同，智能体无法区分自己是在这里还是在那里，因此它必须在两个状态下做出相同的行动。所以它要么向左走，要么向右走，无论选择哪个方向都不是最佳的，因为如果它真的在这个位置，那么最好的选择是向那个方向移动。

不是在这里和下面。 嗯，在这种情况下，AI 智能体可以区分它是在这里还是在那里，但它可能会来回移动，或者做出糟糕的决定。因此，它可能会陷入困境，并且永远无法确定何时可以安全地下降并获取资金。因此，它学习了一种接近确定性的决策策略，因为我们通常学习的策略，无论是贪婪的还是随机的，通常都会表现不佳。但是，如果在某个状态下有随机策略，你可以随机选择。你可能会说，“我不确定我是处于这个状态还是那个状态，所以我可能向东走，也可能向西走，有50%的概率。”然后通常会很快达到目标状态。因为请注意，当达到目标状态时，AI 智能体可以确定下一步应该采取什么行动，因为这看起来与两个状态的情况不同。因此，一旦处于中间状态，AI 智能体就能找到正确的行动。这只是一个例子，说明在某些情况下，随机策略比确定性策略更有价值，因为这个环境不符合马尔可夫性质，是部分可观察的。

好的，让我们重新整理一下这段内容。所以，有些原因可能会导致我们偏向直接基于政策进行建模，除了其他原因。具体来说，当我们拥有参数化策略时，我们希望找到的目标会变得更加清晰。在一个世界不是马尔可夫的情况下，即部分可观察且随机的情况下，随机策略是否会更好呢？这取决于我们所要建模的对象。相较于完全随机化，使用部分可观察马尔可夫决策过程（POMDP）策略可能更好，因为它会在灰色区域中做出一些决策，而不是完全随机化。通过这种策略，您可以跟踪您在环境中的位置的估计，以便确定您所处状态的信念状态，并最终能够准确地识别。

当你处于这种状态时，你需要做出确定的右转或左转决定。这取决于你愿意如何对情景进行建模。当我们开始进行参数化策略搜索时，我们的目标是找到能够产生最佳价值的参数。找到最佳价值的策略类别与之前看到的非常相似，我们可以考虑各种情景和无限的连续设置。在情景环境中，智能体通常会执行多个时间步长，比如说H个步长，但这可能是可变的，有可能一直到达最终状态为止。我们需要考虑预期值是什么，价值在哪里，以及价值是多少？我们从起始状态或起始状态的分布中获得的预期折扣奖励是多少？然后我们的目标是找到能够产生最高价值的参数化策略。在持续环境下，即在线环境中，另一种选择是……

我们将不会执行H步骤，而是永远执行下去。没有终止状态可用，我们可以取状态分布的平均值。因此，就像我们之前讨论的那样，在特定策略下引起的马尔可夫链上出现的稳态分布。因为我们前面提到过一个事实：一旦调整了策略，基本上你就进入了马尔可夫回报过程。您也可以把得到的状态分布看作是马尔可夫链。所以，如果我们永远执行动作，我们可以说从平均角度来看，我们在固定分布下达到的状态的价值是多少？另一种方法是考虑每个时间步的平均奖励。现在，为了简单起见，今天我们主要关注情景设置，但我们也可以探讨类似技术针对其他形式设置的应用。因此，和以前一样，这是一个优化问题，类似于我们在值函数近似情况下看到的情况，对于线性值函数的情况，我们使用深度神经网络。

为了优化我们的模型，我们需要使用一种优化工具来搜索最佳参数。一种选择是进行无梯度优化。虽然我们不太倾向于在策略搜索中采用这种方法，但有许多无梯度优化的方法可供选择。我们的目标是找到能最大化 V Pi Theta 的参数。类似于 Q 函数中的情况，现在我们有了 Theta 来定义一个策略，希望找到能使其取得最大值的参数。因此，我们致力于尽可能有效地寻找函数的最大值，有许多方法可以实现这一目标，而不需要函数是可微分的。在某些情况下，这些方法实际上可能非常有效。我的同事在这方面做了一些出色的工作，我们开发了一种自动识别外骨骼辅助模式的方法，可以最大程度地减少。

人们行走时的代谢能量成本是一个噪音问题。在优化过程中，用户首先体验一个控制律，通过将一阶动态模型拟合到两分钟的瞬态数据来估计稳态能源成本。然后改变控制律并再次估计代谢率。重复这个过程来形成一代内规定数量的控制律。然后使用协方差矩阵适应进化策略来创建下一代。每代的平均值代表最优控制参数值的最佳估计。经过约一个小时的优化后，能源成本平均降低了24%，与没有任何帮助相比。这项工作是由我的同事史蒂夫·柯林斯 (Steve Collins)完成的。

我们是否可以帮助人们表现更出色呢？这个例子想要说明的是，在许多情况下，外骨骼可以发挥作用。有很多中风患者，行动不便的人，以及失去肢体的退伍军人。在这些情况下，一个挑战是如何确定外骨骼的参数，以便为人们行走提供支持，因为外骨骼的参数通常需要根据个体的生理状态进行调整。不同的人需要不同类型的参数，但你希望快速地实现这一点。因此，你希望能够快速了解每个人的情况，以帮助他们在行走时得到最大的支持。正确的控制参数是什么？所以史蒂夫的实验室把这看作是某种政策搜索问题。你让人们佩戴设备，然后尝试不同的控制方法，看看哪种方法可以使外骨骼提供最佳支持。你在测量他们的代谢效率，也就是他们需要多大的努力来行走，特别是呼吸方面。这可以帮助确定哪种控制方法最有效。

接着，根据这些信息，您可以确定下一组控制法则，并以闭环方式尽快完成所有任务。我之所以提出这一点，一是因为它非常有效，同时也是一篇很好的科学论文，说明了它相较于先前技术的更高效性。其次，它使用了CMA-ES，这是一种无梯度方法。尽管我们今天讨论的大多数内容都是基于梯度的方法，但有一些很好的例子表明，在许多其他类型的应用中，不使用基于梯度的方法进行策略搜索是可行的。因此，我认为在您的工具箱中了解这些方法是很有必要的，不必局限于基于梯度的方法。而像CMA-ES这样的方法之一的确好处之一是其保证性能，这对于达到全局最优是非常有用的。因此，在某些情况下，您可能真的希望确保这一点，尤其是在高风险情况下。总的来说，最近人们反复注意到，有时这些方法的效果确实好到令人惊讶，它们在某些方面通常是一种强力方法。

有一种智能而强有力的方法，通常会非常有效。因此，在您研究的应用程序中，这些方法是值得考虑的。但是，尽管如此，有时它们虽然在某些方面非常出色，但在并行化方面帮助有限。在效率方面常常存在问题。因此，根据您所涉及的领域和数据结构，基于梯度的方法通常是有用的，特别是如果您对获得局部最优解感到满意的话。因此，我们今天要讨论的主题是基于价值的方法，例如梯度下降和其他试图利用决策问题结构的方法。因此，对于某些方法，如CMA-ES，可能并不了解这个世界是MDP或任何形式的顺序随机过程。我们将重点关注如何利用马尔科夫决策过程的结构在决策过程中。接下来我们谈一谈策略梯度方法。

嗯，再次，使用 θ 定义事物，这样我们可以明确参数，并且我们将重点关注偶发性马尔可夫决策过程（MDP），这意味着我们将在某个特定的时间内运行我们的策略，时间步数，直到我们到达最终状态，或者您可能知道，可能是 h 步。在这段时间内，我们会得到一些奖励，然后我们就会重置。因此，我们将寻找局部最大值，并且我们将针对定义策略的参数获取梯度，然后使用一些小的学习率[噪声]。所以这就是——这应该看起来非常相似，很类似，类似于基于 Q 和 V 的搜索。这里的主要区别是，我们不是对定义 Q 函数的参数求导，而是对定义策略的参数求导。所以，这里要做的最简单的事情就是，嗯，进行有限差分。

对于每个策略参数，只需微调它，如果对每个参数的每个维度进行微调，就可以获得梯度的估计。这里只是使用有限差分来估计梯度。在每种情况下，您可以使用一定数量的样本来执行此操作。假设您有一个k维的一组定义策略的参数，尝试微调其中一个，然后重复此过程，您将获得一组新策略的样本。通过对所有不同的维度都这样做，现在您就有了梯度的近似值。这种方法虽然有些复杂，效率可能不是很高，但有时是有效的。这演示了在强化学习环境中政策梯度方法是如何非常有用的。策略本身不必是可微分的，因为我们只是在进行梯度的有限差分近似。第一个例子是我看到的。。。

在Peter Stone的工作中，他探讨了政策梯度方法或政策搜索方法在机器人领域的有效性，特别是在Robocup比赛中。Robocup是一个旨在培养具有击败人类世界杯冠军实力的机器人足球队的计划。通过不同的Robocup联赛，包括四足机器人联赛，机器人尝试相互进球，挑战在于使它们看起来有像真实足球比赛中的动作。该计划的目标之一是在2050年前建立一支强大的机器人足球队，其表现可以胜过人类球员并赢得世界杯。

此外，您还必须了解走路的步态。您希望机器人能够快速移动，但又不希望摔倒。因此，优化步态是一个关键问题，尤其在赢得比赛时，因为您需要机器人在赛场上快速移动。Peter Stone长期以来一直是Robocup的领军人物。他们的目标很明确，就是让这些AIBO学会一种快速行走的方法。为了实现这一目标，真实的经验和数据至关重要，因为这是昂贵的，您需要让这些机器人来回移动，希望它们能够快速优化自己的步态。您不希望频繁更换电池等东西，因此您希望用尽可能少的数据来实现这一目标。因此，在这种情况下，他们想要做的是制定参数化策略，并尝试优化这些正确的策略。这就是领域知识的重要性所在，在这里，注入领域知识是至关重要的。所以，他们使用连续椭圆来指定步态是如何工作的，通过对其进行这12个连续参数的参数化。

这段内容讲述了 Peter Stone 和他的团队在 Robocup 上的经验，他们尝试使用有限差分方法来优化政策参数。在他们的研究中，他们使用真实机器人进行政策评估，并且定期进行干预以更换电池。他们在仅有三个 AIBO 机器人的情况下完成了这项研究，每次迭代大约执行 15 个策略，每个策略评估 3 次。尽管数据量不是很大，但考虑到信号嘈杂，每次迭代大约需要 7 小时。这表明了他们的研究方法在实践中的应用。

在这个案例中，研究者们需要在五分钟内选择一个学习率。他们尝试了多次迭代，关注的是稳定行走的速度作为优化标准。过去人们尝试手动调整策略，其中包括了UT Austin Villa团队。他们发现使用有限差分方法比其他方法更快地搜索更好的策略。这种方法并不需要大量数据，且在几个小时内就能取得结果，表明政策梯度方法可以比之前更好地实现目标，而且不需要大量数据。与之前的情况不同，这个案例没有特定的迭代次数限制。

这个研究大致指出，AI智能体通常会尝试数十到数百种策略，而不是数百万个步骤，这使得它们在数据效率上非常高效。虽然这种方法提供了大量信息，但也存在一些影响性能的因素。因此，在论文中作者讨论了对性能有实际影响的因素，这些因素有很多会影响性能。在选择初始策略参数时的重要性，这是因为在这种方法中无法保证全局最优值，只能找到局部最优值，因此起点对于找到哪个局部最优值至关重要。这种方法只能保证找到局部最优，同样所有策略梯度风格的方法也是如此。无论你从哪里开始，你最终会达到最接近的局部最优值，但不能保证那是全局最优值。

在这种情况下，大量随机重新启动或具备领域知识都是至关重要的。另一个关键问题是你对于局部扰动的敏感程度。我认为最关键的是策略的参数化。就像你如何定义你学习的可行政策空间一样，因为如果政策空间不恰当，你将无法学到任何东西。在幻灯片26中关于开环策略的问题，你能否更加详细地解释一下呢？是的，在开环策略部分的问题是关于开环策略的本质。我们所学习的这些策略不一定需要具有适应性。开环策略本质上是一种计划，即一系列操作，无论可能有什么其他输入。因此，通常我们将策略视为从状态到动作的映射，但它们也可以只是一系列动作。所以当我们谈论开环策略时，这是一种非反应性策略，因为它只是一系列动作，无论机器人处于何种状态，它都会继续执行。因此，即使在中途有很大的干扰， ...

无论风大与否，Transformer的下一步动作都是一样的。它不必是反应性的。好的。所以，但总的来说，嗯，你知道，有限差分是一个合理的尝试。嗯，通常我们会想要使用梯度信息并利用我们的函数策略实际上是可微的这一事实。所以，我们现在要做的是，嗯，分析计算策略梯度[噪音]对不起。这在目前使用的大多数技术中是最常见的。嗯，我们假设它在非零的地方是可微的，并且我们可以显式地计算它。因此，当我们说“我们知道”时，这意味着这是可计算的。我们可以明确地计算出这一点。所以，现在我们将，嗯，只考虑基于梯度的方法。所以，我们只是，[噪音]我们只会收敛到局部最优。希望我们能达到局部最优，这是我们在这种情况下所能期望的最好结果。好的。所以，我们要讨论——人们经常讨论似然比策略，嗯，他们将按如下方式进行。

让我们考虑一下偶发事件。在这种情况下，我们可以认为存在轨迹，即从初始状态经历状态、动作、奖励，一直到达最终状态的过程。我们用Tau下标的R来表示轨迹的奖励总和。因此，在这种情况下，政策的价值就是按照该政策获得的预期折扣总和的奖励。这可以表示为我们观察到特定轨迹的概率乘以该轨迹的奖励。简而言之，在这个政策下，观察到任何轨迹的概率乘以轨迹的奖励是多少。奖励是轨迹的确定性函数。一旦你知道了状态、行动、奖励等信息，那么你的奖励就是所有这些的总和。因此，在这个特定情景中，我们的目标是找到策略参数Theta，使得um是它的arg max。

这里我们所做的改变是要着重关注当前情况。因此，请注意，策略参数仅以我们在该策略下可能遇到的轨迹分布的形式呈现。这有点类似于之前讨论的模仿学习。在模仿学习中，我们讨论了关于状态分布以及状态和动作分布的内容，并试图找到一种与专家演示的相同状态-动作分布的方法。今天我们不会过多讨论状态-动作分布，而是专注于我们特定策略下可能遇到的轨迹分布。那么，关于这个梯度，具体是多少呢？我们希望获得这个函数相对于 Theta 的梯度。因此，我们将按照以下方式做：我们将重新表述关于 Theta 下轨迹的概率。换句话说，对于每个 Tau 求其概率。

好的，让我们来梳理一下这段话的内容。首先，我们要对某个东西进行鞭打，然后确保获得相同的符号。接着，我们需要做的是将某个值乘以或除以相同的数值。换句话说，我们需要对给定Theta的Tau概率进行一系列操作，包括除以、乘以，以及求导数等。此外，这里涉及到概率的对数，因此如果对给定Theta的Tau概率的对数进行求导，则...（原文未完）。

这个公式实际上等于在给定 Theta 的情况下，Tau 的概率乘以在给定 Theta 的情况下 Tau 的导数。因此，可以重新表达为Tau r的总和，与Tau的p乘以相对于给定Theta的Tau p的对数。目前看起来似乎并不是很有用，但这种转换很快将展示其用处。特别是，当我们想要完成所有这些工作，而又不了解动态或奖励模型时，这种转变将变得非常有用。我们需要在不知道动态模型的情况下评估策略的梯度，这个转换技巧将帮助我们实现这一点。因此，在我们这样做的时候，

这个概念通常被称为似然比。如果我们进行转换，可以说：“好吧，我们可以观察到，通过这样做，实际上与取对数是完全一样的。” 那么，为什么这种转换可能是有用的呢？我们这里讨论的是什么？我们计算的是所有轨迹的总和。当然，我们未必能够访问所有可能的轨迹，但我们可以对它们进行采样。因此，您可以想象多次运行您的策略，对多个轨迹进行采样，观察这些轨迹的奖励，并对给定 Θ 的轨迹概率进行求导来进行近似。通常情况下，我们只需要运行该策略 m 次即可实现这一点。然后，对于给定 Θ 的 τ 的概率 p，我们将以一种近似的方式进行计算。在这一步中，我们只需平等地考量采样过程中获得的所有轨迹，然后观察该轨迹的奖励，以及对数概率 p，即 [噪音] 嗯，给定 Τ 对 Θ 的。

在这个案例中，发生了什么？简言之，梯度如何计算？我们将奖励乘以路径概率的对数，以得到与特定单词相关的奖励与Theta的乘积。这样做的原因是评估路径或样本的质量。我们希望根据样本的质量来调整其对数概率路径，以便提高参数并获得更好的样本。因此，在我们的策略中引入一些参数，这些参数将促使我们执行路径带来高回报的操作。因此，如果我们将f(x)视为奖励，我们可以将其视为我们的策略或参数化策略。

我们的目标是增加空间中物体的重量，以获得更高的回报。 因此，如果这是我们的 \( f(x) \)，这是我们的奖励函数，这是我们轨迹的概率，那么我们希望调整我们的策略，以增加产生高奖励轨迹的概率。 这样，我们将更有可能发现具有高价值和高回报的事物。 那么，问题来了，如果要实现这一点，我就需要近似第二项，也就是对数项。 在某些参数下，相对于轨迹概率的导数。 因此，必须能够计算出在一组参数下轨迹的概率是多少，我们可以通过以下方式实现。因此，这将是 \( S_0 \) 的 \( \mu \) 的对数概率的 \( \Delta \Theta \)。 这样，我们就可以计算初始起始状态的概率，再乘上...

乘积项 J 是指在时间点从0到t时，观察到下一个状态的概率与进行该操作的概率的乘积之和，根据当前政策。因此，最终结果还需加上另一个括号。由于这是对数形式，我们可以对其进行分解。因此，这个表达式等于S零的Mu乘以Delta Theta的对数，再加上所有Delta Theta的对数之和，因为它是对数形式。J在时间点从0到t时减去[噪音]Transformer模型的对数1。请注意，一般情况下我们并不知道这个数值，它是未知的。我们只希望最终得到一个表达式，这意味着我们不需要这个数值。在此处引用的是我们所经历的路径。

J的总和是从0到t减1。这是我们实际的策略参数。那么，有人能告诉我为什么这种分解是有用的吗？不管我们是否需要，让我们将所有这些东西参数化。嗯，即使我们并不需要了解动力学模型是什么，这种方法看起来也是可行的。每个人都花点时间，和你的邻居谈谈，然后告诉我哪些项会变成零。接下来我们对Theta进行求导。这些术语中有哪些是Theta的函数。（重叠声音）

请记住，Theta 决定了您的策略参数。Theta 决定了您在特定状态下采取的行动。好的，现在我要做一个快速民意调查，我将项目分为一、二和三编号。第一项目是否受Theta影响？如果是请举手，如果不是请举手。太好了，明白了，Theta与第一项目无关。因此，它的值为零。第二项目是否与Theta无关？请举手。太好了，所以第二项目的值也为零。因此，唯一剩下的就是第三项目，这很好。所以，这是一种很好的情况，为什么我们要做这种奇怪的对数变换呢？因为通过这种奇怪的对数变换，它允许我们将采取行动的概率作为乘积来接受状态转换，然后我们可以将其分解为总和。一旦我们将其分解为总和，我们就可以分别应用导数，这意味着其中一些项直接消失，这真的很酷。因此，这意味着我们实际上并不需要知道转换模型是什么，也不需要有一个明确的代表。

请先提出您的问题和姓名。您的问题是关于系统动态是否取决于政策，很好的问题。实际上，系统的动态通常是与政策无关的。在这种情况下，代理可以根据其选择的行动来影响系统的动态，但一旦决定采取行动，系统的动态则与代理独立，这称为解耦。因此，不同的政策将导致不同的轨迹，但影响这些轨迹的只是所选行动的策略，之后环境会决定接下来会发生什么。你并不需要知道每个动作对环境产生的影响，因为奖励将会反映这一点，奖励也是状态的函数。因此，根据你的行动，你可能会访问状态空间中的不同区域。还有其他问题吗？如果您想了解如何对给定 Theta 进行概率估计，可以更详细地描述一下，但通常我们会根据观察到的数据集，尝试估计 Theta 的概率分布。

在这里我们正在讨论的问题涉及到 i 次操作是否会超过 m。在我们采取的做法是否正确的问题上，这是个很好的问题。简而言之，我们要做的是采用这一政策，运行它 m 次。在此过程中可能会得到不同的轨迹。我们要做的是分别计算每个轨迹的概率对数，然后将它们汇总起来。在确定性情况下，如果领域没有太多随机性并且策略也是确定的，你可能最终会得到多个相同的轨迹。一般来说，轨迹会是完全不同的，因此局部梯度的估计也是如此。因此，我们需要能够以分析形式为我们的政策相对于参数的导数提供一个形式。因此，我们需要评估如何参数化我们的策略。

如果我们希望进行分析，就需要对我们的策略进行参数化，这种参数化方式可以精确计算任何状态和动作。因此，我们将更深入地讨论一些方法，这些方法可以使计算变得更加分析和有效。在某些情况下，您可能需要通过蛮力、计算、有限差分或其他方法来估算值函数本身。但是，如果我们选择一种特定形式的参数化策略，那么计算将更具分析性。另一个需要评估的重要部分是评分函数，尽管我认为评分函数并不特别有用，但它仍然被广泛使用。这个函数被称为评分函数，这是我们刚刚讨论到的需要评估的量。因此，当我们计算函数的导数时，通过获取

当我们有m个样本来近似一个值，并对i从1到m求和。然后我们观察这条路径的奖励，并对每个步骤的得分函数求和。大家能理解后文了吗？是的，太好了。是的，这些是我们的评分函数。这些评分函数允许我们评估我们遇到的每个状态动作对，而无需了解动态模型。因此，政策梯度定理对此进行了概括。它是如何概括这一点的呢？请注意，在这种情况下，我们正在处理的是情景设置。在这种情况下，我们只使用原始奖励函数。因此，我们查看这条路径的奖励总和，然后，嗯，

我们可以通过政策参数的导数来衡量它。另外，我们也可以进行一些总结。比如，假设我们要打电话，这就是一个价值函数，而我们的目标函数可能略有不同。之前我们讨论过如何获得情景奖励、每个时间步的平均奖励或总体平均值。因此，我们可以让目标函数等于情景正常值，或者让其等于我提到的J AVR，即每个时间步的平均奖励，或者作为平均值。假设我们持续进行，希望对遇到的状态分布进行平均。所以我们需要考虑一下这个方面。这也是一个很好的场景。实际上，在所有这些情况下，我们都可以进行类似于情景案例中的推导。我们发现，我们目标函数的导数现在可以适用于这些不同的目标函数中的任何一个。

在Sutton和Barto的第13章中，我们引用了他们关于政策梯度的内容，通过计算当前政策相对于其下的期望值的导数，乘以Q值来更新政策参数。他们在讨论中提到了许多不同的问题，并指出这些内容可以扩展到持续性问题。总结一下，我们可以通过运行我们的政策m次，对每次运行得到的完整状态序列、行为和奖励进行平均，得到政策梯度的无偏估计，尽管存在很大的噪音，因此估计将是公正但嘈杂的。

如果你回想一下之前了解的蒙特卡洛方法，这个概念可能会有些熟悉，是吧？我们的政策即将到期，我们会获得类似于蒙特卡洛中获得的奖励，这是一个梯度的无偏估计，虽然会有一些噪音。那么，要使其真正实用，需要采取什么措施呢？有许多不同的技术可以做到这一点，但今天我们要开始讨论的是时间结构和基线。在我们继续探讨之前，我要开始研究，你知道，修复时间结构和基线，基于我刚才提到的蒙特卡洛估计。

在降低估计方差方面，我们可以考虑使用引导法。引导法是一种替代于分割蒙特卡罗方法的技术。通过在偏差和方差之间进行权衡，类似于在Q学习和DQN中使用的时间差异方法，引导方法有助于减少方差并加速信息传播。我们可以尝试用其他变量替换R，或者在R之外使用协变量，来减少方差。因此，在这个过程中，我们可以做一些尝试，利用我们暂时的资源，尽力降低方差。

好的，根据您提供的内容，经过整理和翻译后如下：

嗯，对于之前熟悉重要性采样的人来说，这与每个决策重要性采样密切相关。基本上，我们要利用的事实是奖励只能在时间结构域内。让我写出来。所以，我们之前提到过，回报对于策略参数的梯度期望等于从0到t的轨迹期望rt的负数乘以t的总和。例如，您获得的所有奖励之和乘以t的总和等于从0到t减去您的策略参数的导数的负一。这就是我们之前讨论的情况。因此，我们只需将所有奖励相加，然后将其...

我们将对轨迹中每个动作状态对的策略梯度进行求和。嗯，因此，我们关注的是单个奖励，而不是所有奖励的总和。让我们来看看。对于 rt prime 的期望值关于 Θ 的导数。这只是我们在单个时间步长可能遇到的奖励，沿着轨迹，这相当于 rt prime 乘以累积到时间步 t 的总和，这等于导数对 t prime 的期望值。所以，这几乎与之前一样。唯一的关键区别是我们只对 t prime 求和。总之，你可以把它看作是一条削减版的轨迹。我在研究状态的影响，以及我，法学硕士。

我一直达到的，嗯，rt prime 时所达到的行动和奖励。好的。所以，我不必对未来的事情进行总结。因此，我们可以使用这个表达式，现在我们可以对所有时间步进行求和。那么，这意味着预期奖励是多少，或者相对于时间步长 t prime 的奖励的导数？现在，我要总结一下，这将与我的第一个表达式相同。所以，我要做的是，我会说，Theta的[NOISE]V等于er的Theta的导数，我将总结该内部表达式。因此，我要对t prime等于0到t减1 rt prime求和，然后插入第二个表达式。好的。所以，我所做的就是把它放在那里，然后对t质数求和等于0，一直到t减1，然后，

我要做的是通过以下观察重新排序这些内容。想一想，在这些特定情况下，$$\log \Pi_{\Theta}(s_t)$$出现在多少个项目中？那么，如果我们看一下，对于$$a_1s_1$$的$$\Phi_{\Theta}$$的对数。因此，当您查看出现的次数时，您会发现它既出现在早期奖励中，也出现在所有后期奖励中。这将出现在$$r_1$$中，它将出现在$$r_2$$中，它将一直出现在$$t-1$$的$$r_t$$处，因为我们总是对$$t$$之前的所有内容求和。现在，我们要做的是接受这些条款并重新组织它们。因此，其中一些术语出现了很多次，而另一些术语，即$$\log \Pi_{\Theta}$$的$$a_{t-1}, s_{t-1}$$，只会出现一次。

它只是重新负责帮助决定最终奖励。因此，我们可以利用这种洞察力重新组织这个方程。现在，我们说，t 的总和的期望值等于从0到t减1。请注意我之前将t置于内部，t'置于外部，而现在我要做的是将t移到外部。我会说，在第t次，t'的总和等于从t到t减1的rt'。所以，我做的只是重新整理这个等式。在倒数第二行，这是否应该是关于Theta的值函数的导数？对，在最左边的地方。对不起，你的意思是应该是这个的导数？

是的，没错。在这种情况下，我们所做的是重新组织和总结。我们只是以稍微不同的方式回顾了术语，但这将会以一种更易理解的方式呈现出来。所以，让我们把它向上移动，我会把它向下移动。好的。所以，现在我们依然在讨论时间结构。这会给我们带来什么启示？嗯，第二个术语看起来应该有点熟悉。这里指的是，从时间步开始，一直到结束时我们所获得的奖励是什么？即回报。所以，我们之前定义过，当谈论蒙特卡洛方法等时，我们可以仅仅考虑在时间步 t 处的奖励 rt。这实际上就是回报。从第 i 集的时间步 t 开始到结束所获得的回报。因此，这个概念看起来与我们在蒙特卡罗方法中所熟悉的非常相似。

我们能够通过状态和动作推断出从该状态和动作开始直到该集结束所获得的总奖励。因此，我们可以将Theta的导数重新表示为所有轨迹的平均值，并对所有时间步进行求和。实际上，我们将策略梯度乘以回报，这样就可以得到一个稍微更低方差的估计。这意味着我们不需要单独对所有单词求和，然后将其乘以所有这些导数的总和，只需对对数进行求和。对于某些奖励条款来说，本质上就是这样。

因此，我们可以降低这种情况下的方差。在某种程度上，就像对每个奖励进行操作一样，因为你可以将其表达为奖励的总和。对于这些奖励中的每一个，你必须根据梯度的导数，也就是政策参数的导数，来计算完整轨迹的总和。现在，我们说，你不需要将其乘以所有这些。你只需将其乘以与特定奖励相关的因子即可。这意味着你将会得到稍低的方差估计量。

所以，当我们这样做时，最终我们可以实现所谓的"强化学习"。有谁听说过"强化学习"？是的，一些人听说过，但不是所有人。REINFORCE 是其中最常见的强化学习策略梯度算法之一。因此，您得到了 REINFORCE 算法。

那么它是如何工作的？算法的工作原理是在其中随机初始化 theta。你总是需要首先确定如何对策略进行参数化，所以在某个地方你已经做出了决定 - 就是决定如何对策略进行参数化。

现在，您将随机设置策略的值。然后，对于每一轮，您将使用该策略执行一次轮回。在这个轮回中，您将收集一系列行动和奖励，这些是根据您当前策略的采样得到的。因此，根据当前策略的样本，您将获得一条路径，然后对于路径中的每一步，您将更新策略参数。在这一轮中的每个时间步，我们都会更新策略参数。这将涉及到与之前相同的过程，乘以一定的学习率。在这个过程中，我不会用W，而是用α乘以Theta的导数，log Pi Theta，在Gt时的状态，其中Gt就是本轮回中的阶段。

从 st 开始的奖励总和是多少？所以，这就是正常的回报，就像我们用蒙特卡罗方法得到的那样。我们使用来自状态和行动的奖励来估计线性价值函数，就像在这里所做的一样。除此之外，我们将更新策略参数，并且会反复这样做许多次，最后返回 Theta 参数。是吗？我有一个问题。对于每一集，您是否从更新后的政策中进行抽样？我们需要和你谈谈。希望你准备好了。是的。嗯嗯。是的。刚才问的是对的。所以，在这种情况下，我-在完成一集的所有更新后，便可以进行这些增量更新。然后，在完成所有增量更新后，你会得到另一个带有新更新参数的剧集。是的？嗯，既然我们每次更新都会这样做，这会是一种有偏见的方法吗？

好的，根据您提供的内容，经过整理后变为：

因为我们每次都进行估计，这应该是对I的无偏估计——它仍然应该是梯度的无偏估计。虽然是随机的，但是我们没有以同样的方式存在状态和行为的概念。这将是渐近一致的，状态和动作的概念在这种情况下是不同的，因为我们只有这些策略参数。因此，我们不会在这里估计状态和操作的价值。这当然是渐近一致的，我认为这仍然是不偏不倚的。如果我稍后重新考虑，我会发一篇Piazza帖子，但我认为这仍然只是对梯度的无偏估计。这是一个好问题。好的，我回到我的幻灯片笔记，我想我想要的最后一件事...

在你提到的这段对话中，似乎讨论了关于如何计算差异以及在政策参数中如何尊重这种差异的问题。提到了Softmax、高斯和神经网络等不同的政策类别，以及如何将其参数化来获得采取行动的概率。简单来说，就是通过特征的线性组合和指数权重来确定我们的策略参数，以便我们可以得到实现所期望政策的概率。

在给定状态下采取行动的概率是多少呢？我们将对这些加权特征使用指数函数。因此，我们可以得到 $e^{\phi^T \Theta}$，然后除以所有操作的总和。所以，请注意，在我们的动作空间是离散的情况下，这是一个合理的方法。在许多 Atari 游戏以及许多不同的场景中，你会有一个离散的动作空间，因此你可以取这个指数，然后进行标准化。通过对指数求和，我们立即得到了我们参数化的策略类。如果我们想计算关于对数的导数，这将非常方便，因为我们这里有指数项和对数项。我们正在记录这个过程，希望能够从这个参数化的策略类中计算这个值。

我们得到的是这种类型的参数化策略的对数 Theta 的导数正好等于我们的特征。所以，这就是我们使用的任何特征表示，就像在运动机器人案例中，这会是完全不同的。这可能是，你知道，角度，或关节，或类似的东西。所以，这就是我们使用的任何特征减去您的参数的 Theta 预期值，指数为，在该策略下可能采取的所有操作的预期值。所以，这有点像说你观察到的特征与平均特征、动作的平均。这是可微分的，你可以解决它，然后它会给你一个分析形式。另一个非常流行的东西是高斯策略。为什么这可能是件好事？

嗯，这可能对我们很有帮助，因为我们通常有持续的动作空间。因此，如果我们有离散的动作空间，这将非常合适。通常，在控制和机器人领域，我们经常遇到这种情况。所以，您会有许多不同的参数，并且希望能够为它们设置连续的数值。嗯，在这里我们可以说，我们使用 s 的 µ。它可能是状态特征乘以某些参数的线性组合。好的。为简单起见，现在让我们假设我们有一个方差，但这是固定的。因此，我们也可以考虑不是这种情况，但假设我们有一些固定的方差项，这不是一个参数。这不是我们试图学习的内容。我们只是试图学习定义 µ 函数的 Θ，然后我们的策略将是高斯的。因此，动作将从高斯分布中取得每个特征的均值。好的。所以，我们将当前状态与均值进行比较。

然后我们会选择一个相对于当前状态的动作，在这种情况下，得分函数就是高斯函数的导数。 因此，得分就是高斯函数的导数，最终结果是状态的负 mu 乘以状态特征的参数除以西格玛的平方。 因此，这也可以通过分析来完成。 另一种常见的网络是深度神经网络。 这些是人们常用的形式之一。 在接下来的讨论中，我们将谈到另一种常见的方法。 在我们结束之前，我们会花大约五分钟进行一些早期的课堂反馈。 了解一下什么可以帮助你学习，你认为哪些方面可以改进，这对我们很有帮助。 所以，现在我开放时间，如果你们能到广场上填写这份问卷就太好了。 所有信息都将是匿名的。

我希望在周三能收到你们对此的反馈。 让我看看是否有需要重新做。 我要发布了。 好的，让我看看是否——我会固定在顶部，这样更容易找到。 [噪音] 现在应该固定在最顶部了。 是的。 接下来是课堂反馈调查，如果你去广场，这是一个非常简短的调查。 您可以向我们提供反馈信息。 那太好了。 下一步，我们将继续讨论策略搜索。 我们将讨论基线，这是减少方差的另一种方法。 这是一个非常活跃的研究领域，有许多关于深度强化学习和策略梯度的研究，所以我们将讨论其中一些工作，然后你将有机会尝试一下——在中期考试后获得有关策略梯度的实践经验。 因此，我们将在期中考试后发布与此相关的作业，这将是第三个作业。


# a0000 @ 0000s-MBP in ~/mywork/commonLLM/opensource/nnnew/Transcript-benchmark-quickstart on git:main x [18:08:49] 
$ /usr/local/bin/python3 /Users/a0000/mywork/commonLLM/opensource/nnnew/Transcript-benchmark-quickstart/app.py
欢迎大家回来！在我们开始今天的内容之前，提前有人对后勤、期中考试或其他类似问题有任何疑问吗？下周一我们将进行期中复习，而期中考试将安排在周三进行。考虑到班级人数较多，我们会将大家分配到不同的房间，并提供相应的指示。还有其他人有关于后勤安排的问题吗？

是的，期中考试会在上课时间进行。考试说明也将发布在网上，但请记得携带一页书面笔记，其他材料请合上书带到考场。有关提交注释是否可以输入[NOISE]电脑打印的问题，我会再确认一下我们之前发布的政策，看看具体规定是什么。

感谢大家参与班级反馈调查，在正式开始之前，我想表达对大家的感谢。接下来让我们开始今天的内容吧。

对于我的学习，了解哪些内容对我有帮助，哪些没有帮助，以及对其他人也很有帮助。 230人已注册参加这门课程。 对于一些人来说，没有他们的反馈，我很难知道哪些内容对他们有帮助，哪些对他们没有帮助。 因此，我们只会采纳参与者的反馈。 大约65%的人认为学习节奏适中，27%的人认为速度太快，只有8%的人认为速度太慢。 我们将保持之前的学习节奏。 很多人希望这门课程像一学期的课程。 我会提到还有其他强化学习课程，强烈鼓励参加。 我提供高级课程，Ben Van Roy通常在春季提供更理论性的课程。 这一点很有争议，并不是我预期的。我们周四和周五上课。 出席率确实很低。 我们大约有

会议参加人数在三到七人之间。我们认为这个时间对每个人来说都很重要，因此需要做一些调整。由于需要平均分配时间，我要求所有助教将办公时间和会议时间进行比较。据估计，大约有四到五倍的人选择了办公时间而不是参加会议。因此，我们决定将会议时间改到办公时间，这样就能为更多人提供服务。让我将这个改动记录下来。所以我们将把会议时间改为办公时间。其他日子的课程仍然会继续进行，并且会被录制下来。因此，对于那些想在周四和周五参加会议的人，你仍然可以选择参加Zoom会议或观看录制的讲座。我们将会把会议时间改为办公时间，因为助教们反映他们有很多空余的办公时间，但却必须熬夜或无法联系到一些人。再次强调，我们的目标是为尽可能多的人提供服务。当我阅读这些回复时，我开始思考一个事实：在强化学习和一般的顺序决策中，我们需要考虑

我们总是在优化预期奖励，这就是我们在这里所做的，也就是我们所做的工作。我知道每个人的需求都有所不同，我们只是尽力做到最好并符合期望。这也是为什么智能辅导系统等其他工具可能更好的原因。很多人对在课堂上做工作的范例和进行推导给予了积极评价。许多人真的很喜欢我的 iPad 周一出现问题这一事实，因此我们在板上进行了调整。因此，我们将尝试进行更多相同数量或更多的推导。人们对作业也持积极态度。我们经常看到，我所做的就是整理所有自由回复，并试图找出共同的主题。我认为出现三到五次或更多次的问题是人们愿意解决的常见问题。人们更喜欢关注整体解释，以及将玩具示例与现实世界的示例联系起来。

因此，我们将努力做到这一点。我也会努力确保我在整个过程中都大声说话。一些人说有时候很难听清，所以我会尽量改进。如果您在后面听不到我的声音，请随时举手。人们想要更多有效的例子。因此，在会议中，我们会特别强调提供可行的示例。再次强调，如果这不是您最关心的问题之一，我们很抱歉这学期可能无法解决所有问题。有趣的是，我们经常会听到人们持完全相反的意见。有些人喜欢连续不断，例如推导过程，而有些人则不喜欢幻灯片之间有间隔，更喜欢我在课堂上进行推导。还有人提到一些觉得推导过程进行得太慢，而其他人认为进行得太快。再次重申，我们只是尽力满足每个人的需求。因此，今天我们将继续讨论策略搜索。

正如我之前提到的，这可能是您在强化学习中学到的最重要的内容之一。我认为现在这个理论应用得非常广泛，以优化功能。我们再次考虑策略搜索，因为在进行值函数近似时很多概念都是类似的。我们讨论的是参数化策略，通常会用θ来表示参数化，但也可以用W或其他符号。无论如何，我们都有一个参数化的策略。然后，我们会对这个策略的价值有所了解。我们的目标是找到一个最佳解来最大化这个策略的价值。之所以在模仿学习之后立即这样做，原因之一是将其与选择策略类的思想联系起来，即一种特定参数化方法。因此，本质上，这是在建立结构。总结一下，迄今为止，我们主要集中在没有模型的基于价值的方法上进行工作。

我们现在要开始研究直接策略搜索方法。今天我们还要更深入地讨论Actor-Critic方法，同时我们会同时维护这两种方法。我们维护显式参数化策略和显式参数化值函数。在过去几周，包括昨天和星期一以及今天，我们主要讨论的是我们希望能够在非常庞大的状态空间中运作的情况。所以，我要简单回顾一下上次的内容。为什么要这样做呢？好吧，通常我们可以确保收敛到局部最优解。对于基于值函数的方法，我们并不能总是有这些保证。这点可能非常重要，是个很大的优势。然而，缺点是，如果使用策略梯度方法，通常我们只能收敛到局部最优解。上次我向您展示了外骨骼的示例，他们使用了全局最优解方法。因此，还有其他方法可以基于策略的强化学习，并不总是能够找到全局解决方案，但基于梯度的方法通常可以。

我们正在讨论的另一个问题是，为了解决这个问题，一个方法是使用评估政策的工具，因为政策本身可能效率低下且方差高。在此之前，我们定义了一个策略梯度，现在我们考虑将这些内容参数化为θ。我们可以将值参数化为通过θ或π(θ)表示。我们经常讨论值函数，因为最终值函数取决于策略，而策略取决于参数。当考虑从这些算法中获得什么时，通常我们想要尝试收敛到一个非常好的局部最优解。对此我们通常控制不多，但我们经常可以控制收敛速度。我们希望通过最大限度地减少梯度来尽快地达到收敛目标，特别是当我们使用基于梯度的方法时。

好的，根据对应表，我将进行翻译和调整：

嗯，并尽可能地使用我们的数据。 因此，我们今天要讨论的一件事是，我们何时进行这种策略梯度技术。 所以我们会向下移动。 现在，我们将得到渐变。 我们将会有我们的功能。 这是 B pi，这是我们的参数化 pi。 当我们朝着某个局部梯度下降时，如果当我们更新我们的策略时，它是单调改进的，那就太好了。 那么有人能给我一个为什么我们可能想要单调改进的理由吗？ 是的。 帮助保证收敛。 答案是对的。 绝对可以帮助保证收敛。 虽然我和你们很多人一样热爱数学，但这是一个很好的理由。 但也许，我也在思考为什么我们也想要这样的经验原因。 是的，在后面。 [听不清] 就像在高风险的情况下一样。

事实上，我们之前看到的是我的一个学生，他昨天正在进行一次练习工作演讲。他展示的是DQN的图表，包括表现、奖励和时间。当然，并不是所有情况都像这样。通常情况下，当阅读论文时，数据会经过很多次平滑处理，但展示的效果通常是这样的。当经历多个情节或多个时间步骤时，比如在执行DQN时运行策略的性能，会出现非常波动的情况。那么为什么在高风险情境下这可能是不利的呢？是的，这点很重要。请先介绍一下名字。比如，在高风险情境中，如果你一开始表现不错，但后来表现下降，人们会感到不安，因为他们看到现在状况变得更糟，即使最后可能会回升。因此，我们谈论的是，如果这个系统处于高风险情境中，如果你意识到…

如果你的政策一开始表现良好，但接下来的效果却非常糟糕，即使可能稍后再改进，你的老板仍有可能开除你[笑声]。我是在开玩笑，不过我觉得人们通常对损失感到厌恶，这种情况通常是不可接受的。在一家公司里，如果你说这个季度我们做得不错，但下个季度可能会更糟，尽管最终我们会好起来。我们经常希望确保持续进步，不间断。但在一些情况下，比如患者治疗、高风险场景、飞机等，人们可能无法忍受听到我们会更糟。当然，这些都有例外，但我觉得在很多情况下，人们真的喜欢稳步改进。如果可能的话，我认为这种单调上升真的很重要。除了理论之外，它还能帮助我们证明一些东西，也可能是一种吸引人的部署方式。总的来说，人们一般喜欢冒险，厌恶损失，因此单调改进策略是有吸引力的。

大部分基于价值的方法，例如 DQN，并没有提供这种全局最优的保证。是否能够在所有情况下实现这种全局最优是一个可以进一步讨论的话题。需要明确的是，单调改进是基于您当前可以访问或观察到的数据。换句话说，如果您的生活环境的数据分布与实际环境有所不同，考虑到一些秘密数据，您可能无法准确地对其进行量化或改进。这种单调改进是指什么，以及在什么条件下可以保证或实现这种改进？我们是根据先前的数据进行操作，并对未来收集到的数据做出一些假设吗？是的，我们假设仍处于相同的决策过程中，并且该过程是静态的。也就是说，所使用的模型和奖励模型在不同情境下保持一致——尽管可能还未观察到所有状态，但是在不同情境下保持一致。因此，我们并未考虑这样一个情况：例如，客户偏好已经完全改变，或者其他情况的变化。

您知道，气候变化正在改变我们的环境。面对这一事实：如果世界保持不变，我们将会有持续改进。现在我要向您展示的另一点是，在某些情况下我们可以确保这种改进。另一个重要的方面是，我们希望看到期望的改进。一般来说，价值函数会包含期望奖励。因此，我们希望能够说出在其价值函数持续增加的环境中部署的一系列政策。那这意味着什么呢？这意味着我们希望 V_Pi1 小于或等于 V_Pi2 小于或等于 V_Pi3 依此类推，这是每次迭代或每轮部署的策略。但是这并不能保证对于单次运行来说这个策略更好。因此，您可以轻松理解，总体而言，您所部署的政策可以。

在研究中，有时候一个解决方案对于大多数情况（比如飞机设计或患者治疗）是更好的，但对于某些个别情况可能并不适用。目前有一个非常有趣、活跃的研究领域，即安全强化学习和安全探索。许多人正在思考这个问题，包括斯坦福大学的一些研究人员。我们的团队致力于研究如何有效地找到安全的解决方案。在这里，安全意味着可能不是简单地追求最大化期望奖励，而是希望最大化某种风险规避标准。我们希望找到真正有效的方法来实现这个目标。不过，有很多有趣的话题需要讨论，比如如何进行政策搜索，或者在不仅关注期望结果的情况下如何进行改进。今天我们要讨论的是，如何做到不仅仅是渐进改进，而是实现大幅度的改进。尝试实现小的渐进改进可能比确保真正大的改进更容易。有谁有直觉知道为什么这是真的吗？可能这是更难的一部分。

这段话讨论了政策改变如何影响状态分布。如果政策发生变化很多，那么访问不同州时状态分布可能会有显著变化。这意味着即使政策变化对状态分布有影响，可能会导致数据变得稀缺。然而，如果新旧政策几乎相同，那么你可能能够更好地估计价值。接下来讨论了超越前述内容的政策梯度方法，以一种更有效率的方式进行。同时也提到了探讨其他方法以提高效率，降低噪音，并朝向单调改进。

我们讨论的起点是在政策梯度方面能够采取的行动。其中之一是执行蒙特卡洛回归。蒙特卡洛回报是指有时候人们会使用Tau的大R，其中Tau代表一条轨迹。因此，你可以执行你的策略，直到世界终止，或者在每一步观察奖励，无论你如何定义情节。我们可以用G_i_t来表示第i集的时间步t中获得的奖励。尽管这是梯度的无偏估计，但会有一定的噪音。于是我们开始探讨在强化学习问题中可以使用的额外结构，假设世界是马尔可夫，试图减少估计的方差。上次我们讨论了使用时间结构，我们在板上做了一些工作。

直觉是在某个特定时间点获得的奖励，这个奖励不会受后续决策的影响。因此，在计算给定状态下采取行动的概率时，不需要考虑未来的行动会如何影响先前获得的奖励。现在，我们要讨论的是蒙特卡罗的基线和替代方案。那么基线是什么呢？基线是指我们在当前时间步长之后所获得的奖励的总和。这通常被称为GT，即在当前时间步到本集结束时所获得的奖励。我们需要从这个总和中减去一个取决于状态的基线。接下来要说明的是，通过减去仅取决于状态的基线，得出的梯度估计仍然是无偏的，但可能会有更高的方差。特别是，在很多情况下，最好的选择是期望回报。

这基本上是在讨论价值函数。那么为什么我们要这样做呢？然后我们可以看到，通过增加一个动作的对数概率，与它相比优势有多大。一般来说，这最终有点类似优势函数。为什么呢？好的。然后，我们要尝试在这里做什么呢？我们会说我们目前有这个高方差的估计。如果我们没有，想象一下没有这个，我们也没有那个。我们有我们上次讨论的标准估计。我想说服你的是，如果我们从状态函数中减去这个，那么我们减去的额外项的期望是零。这意味着我们的估计仍然是无偏的。所以我说我们最初的估计是无偏的。我们正在减去这个奇怪的东西，我们希望证明得到的估计仍然是无偏的。我们通过展示的方法是为了证明这个目标，其值等于零。这就是我们要尝试做的事情。

如果我们能够证明这一点，那么这就证明了为什么我们可以减去这个随机的东西。 接着我们就可以开始讨论那个随机的东西应该是什么。 但首先，我们只需要展示这个随机的东西是什么。 如果它只是状态的函数，那么它的期望是0。 那么我们应该怎么做呢？ 首先，从外部的角度来看，人们对tau（τ）抱有期望。 这代表了我们执行当前策略时可能遇到的所有轨迹。 所以我要做的第一件事就是将其分成两部分。 这仍然是tau。 我在这里所做的是将其分为第一部分，从时间步t一直到结束的第二部分。 我只是在进行简单的分解，就像我刚刚写出轨迹是什么，并将其分解为两部分。 所以我只是对轨迹进行分解。 一旦我们完成这一步，我们就可以看到基线项只是S_t的函数。 所以我们可以将其从这一项中取出来。 所以我们应该将其提取出来，因为它不依赖于……

在任何未来的时间步骤中，它是独立的。 在这种情况下，我们要做的下一步是记录下这些事实。 在这个内部项目中，我们只关注当前状态 S_t 和动作 a_t。 因此，我们可以不考虑所有未来的内容。 在这里，我们需要关注的是在给定状态 t 和参数 theta 的情况下，在时间步骤 t 采取动作 a_t 的概率。 所以我们不需要担心未来的状态或将来会采取的行动，我们是独立的。 现在，我们只需提取出所需的内容，首先是基线。 现在，我们要抛弃那些我们不需要依赖的内容。 在这里，我们只关注对所采取行动的期望值。

现在我要做的是阅读它，我们对 $a_t$ 的预期值是什么？问题是什么，你知道，预期值是什么吗？我们只是明确地写下来，这取决于我们的策略。因此，我们将对 $a_t$ 求和，也就是 $a_t$ 的概率，这当然仅取决于我们遵循的策略，乘以对数的导数。这就是我写出的期望值，然后我将取对数的导数。 [噪音] 所以这只是政策本身的导数，除以$a_t$的$\pi_{\text{s,t,}\theta}$。

好的。现在我们注意到分子上有一项，分母上有一项，我们可以将它们相互取消。因此，这样简化后，$S_t$ 中的[噪音] $b$ 乘以 $a_t$ 的总和，即策略的导数。我们只是简单地抵消了分子和分母。接着，我们可以注意到可以互换求和符号和微分符号。这一步是证明过程中的关键步骤。因此，现在我们要做的是移动微分符号。关于[噪音]，嗯，这只是1，因为根据我们的策略，我们采取各个行动的概率之和必须始终为1。因此，我们可以看到我们只是在对1进行微分。因此，我们试图对1进行微分，。

当然，这个常数被加在了基线上，这是很酷的。这意味着我们已经引入了这个常数。这是一个取决于状态的函数，我们并没有详细讨论计算差异的所有方法，或者具体说它是什么并不重要。无论添加什么，都是公平的。为了验证我们的理解，回到这个方程，如果我将 $S_t$ 的基线设置为处处的常数，梯度估计器是否仍然保持无偏。只需花一分钟时间与您的邻居交谈，根据我刚才说的，如果 $S_t$ 的基线是一个常数，那么就像对所有 $S_t$ 而言都是无偏估计器，只需花一秒或一分钟与您的邻居讨论一下。

了解，根据提供的信息，重述如下：

好的，让我们首先确认每个人都认为投票是公正的，对吗？是的，没问题。很好。现在关键是确保投票是无偏的，不是什么 s 的函数，而只是个常数。所以投票必须是 s 的函数，无论 s 取何值，它都是简单的函数。

因此仍然具有无偏性。需要注意的是，如果 s 是某个函数 fu - 如果，嗯，基线是状态和动作的函数，你认为这个论证是否成立？不，不，不，对。因为我们一开始要做的一件事就是将 S_t 的 b 全部移出。如果这个值也取决于动作，我们是无法这样做的。所以基线只是关于状态的函数。是的。函数 b 不能为您提供无偏估计状态。所以我不确定 b 有什么作用？b 只是关于状态的函数，它们都是无偏的。是的，所以它们始终是无偏的。可能确实 - 我的意思是，就像我刚刚提到的，嗯，你可能只需输入一个常数，它可能根本不会减少你的方差。因此，基线的定义当然是无用的，嗯，但它们确实是公正的。所以它们不会影响，啊，无论您的估计器是否无偏。如果它们的效果真的很糟糕，它们可能会使您的估计器变得更糟，嗯，或者[噪音]它们可能就会成为非常糟糕的估计器，嗯，

当然，他们不能通过选择好的来让它变得更好。因此，这最终使我们能够定义所谓的策略梯度算法。香草策略梯度的运作方式是，我们使用当前的策略收集一堆轨迹。然后，对于每个时间步，计算从该时间步到结束的回报，然后计算优势估计。接着，写出普通政策梯度。因此，普通策略梯度的工作原理如下：启动时，使用一些参数 theta 初始化策略，并且需要从一些基线估计开始。普通策略梯度的操作是在迭代i = 1, 2...中进行。使用当前策略收集一组轨迹，然后对于每个时间步进行操作。

在 t = 1 时，对于轨迹的长度 i，您要执行两个操作。首先计算回报，它等于截至目前所有回报的总和。然后计算优势估计 A-hat_i_t，它也等于这个回报。这里的 i 仅用于参数化，表示这是第 i 条轨迹。这里的[噪音]是指 S_t 的基线。请注意，这里的回报是直到轨迹结束时的奖励总和。这里的基线就是一个固定函数，可以是一个深度神经网络，也可以是一个表格查找。这个函数接受时间步 t 时的状态和轨迹 i 作为输入，输出一个标量。基线的作用就是这样。在常规策略梯度方法中，我们通常做的是……

接下来我们要重新调整基线。在这种情况下，基线将被定义为G中噪声的平均估计。因此，在一般政策梯度框架中，我们有普通政策梯度。接下来的步骤是总结迄今为止获得的所有轨迹。我们将对所有时间步长进行求和，然后基本上进行最小二乘拟合。请注意，这可以通过，这是一种监督学习。我们只有一些可以进行参数化的基线函数。我会确保在这里加上一个i。因此，基线函数可以使用完全不同的权重或参数进行参数化，然后我们得到了到目前为止我们见过的回报g，我们的目标是尝试最小化这个距离。因此，基线实际上代表了预期奖励的总和。请注意，从某些方面来看，这有点有趣，对吧？因为我们是在使用我们已经看到的所有数据。因此，这可以通过...

您可以利用全部历史数据来完成，或仅使用最近一轮数据。建立基线有多种选择。如果您使用全部历史数据，那么它就是基线。您可以对多种不同策略的数据进行平均，因为您获取了这些数据来自不同策略。如果您只在最近一轮执行此操作，那么您会得到大致V_Pi i-V_Pi i的估计，就像在迭代中一样。这将最终接近真值。如果您只是重新生成轨迹而不进行总结，那么对于当前轨迹的总结只会提供有限的信息。也许我之前的表达有点混乱。让我试着用更清晰的方式表达一下。假设我们有a个迭代，我们有d条轨迹。因此，如果我们这样做，它就等于V_Pi i。这里的i表示迭代次数，d表示我们在第i次迭代中收集的轨迹数量。这只是对数据的一种处理方式。

对于这个具体政策的平均值和轨迹，我说了很多，有点混乱。 有人对我们在这种情况下所做的事情有任何疑问吗？ 通常情况下，在这种情况下会有很多轮系列。 针对每个人，我们基本上会有一系列的循环。 然后，对于每项政策，我们都会有一组轨迹。 对于每条轨迹，我们都会有一组时间步长。 在这里所说的是当前政策的所有轨迹的平均值，并且适合于该基线。 一旦你得到了这个，你就有了基线。 然后我们会使用你的梯度更新策略。 这将包括这些与政策和你的优势函数有关的导数项的总和。

好的，因此您将使用在这里计算的价值函数。然后要将其乘以给定状态和动作的概率，再求其对数的导数。接着将这个梯度的估计插入到随机梯度下降、ADAM或其他优化算法中。这就是通常的政策梯度方法。在剩下的时间里，我们将看到一些稍微不同的基本模板变体。所以我会在一会儿告诉您，但我只是想强调，如果您——当您离开底层，就像我希望您理解的，从主要思想的政策梯度来看，本质上是董事会现在所做的决定。是的，我们正在运作的是我们正在运行，我们采用一种策略。

我们从数据中收集了大量信息，然后我们需要适应各种因素，比如优势，不同的计算方法也会涉及其中。最终，我们可以执行引导，进行类似于时序差估计或直接使用回报等操作。通常我们会使用随时间变化的基线来拟合。然后我们将更新策略，根据梯度来选择一些步长。这是非常重要的。换句话说，几乎所有策略梯度算法都遵循这个基本模板，可以在其中插入不同的元素，以及选择不同的步长采取方式。这将决定你看到的许多不同策略梯度算法。那么，我们该如何表示i的函数以便获得其梯度呢？这是一个很好的问题。所以，这里其实在问的是，我们如何定义并表示策略，以便计算其梯度。我们需要能够将其引入到这里。我们之前简短地讨论过这个问题，但也要注意，接近尾声时，高斯动作和Softmax函数都是可以进行微分的。通常我们会使用深度神经网络或者浅层深度神经网络来完成这个任务。

是的，我了解您的疑问。您在问的是关于神经网络基线和非平稳问题的相关内容，对吧？在使用神经网络基线像 b 这样的情况下，是否存在非平稳问题？根据经验，许多人都对这个问题感兴趣，特别是在估计梯度时。通常情况下，我们会针对于 D 轨迹运行策略，然后用这些数据来估计梯度。尽管我们可以使用其他数据，但可能会导致脱离当前政策，因为这样会将不同策略收集的数据混合在一起。根据经验，为了避免混淆不同策略下的数据，人们倾向于只使用当前正在运行的数据。因此，继续遵循当前政策可能是最有帮助的。您可以对旧数据进行重新加权处理，以实现更好的效果。

嗯，但这样做可能会引入方差。根据经验，通常这是最佳选择。我认为陪审团对此还没有达成共识，有关此事的研究正在进行中。我们已经进行了调研，Sergey Levine 的团队也做过类似的研究，但大多数情况下使用保单数据是有意义的。没错，还有什么其他问题吗？请先说出名字。我只是想做个确认，所以当您说要重新调整基线时，我们将基线设置为使误差最小化的值。太好了。是的，对于错误，如果我们这样做，仅对当前政策的数据点进行平均，那么本质上与我们进行蒙特卡罗政策评估时是相同的。因此，这几乎等同于蒙特卡罗策略评估。我们有一个固定的策略，然后我们使用参数化函数来表示它。嗯，然后我们只需调整这些参数，以便我们可以更好地使用蒙特卡罗来估计策略价值。好的。关于自动差分，您可以在幻灯片中找到相关信息。嗯嗯，接下来我们会经历的事情是……

让我们以更通俗易懂的方式对这段话进行重述：

让我们来思考一下，然后讨论这个话题。这部分讨论的重点是如何实现单调改进。梯度一旦确定，我们就需要弄清楚要朝哪个方向前进才能确保获得改进。这部分内容涉及更准确地估计梯度，最好能用更少的数据来减少方差。虽然它们都很重要，但它们的具体作用略有不同。让我们考虑一下如何提升这个问题吧。谢谢。让我们回到之前讨论过的话题，首先我们来谈谈基线。选择基线的标准是什么？讨论基线我们可以做的一些事情。

在这里提到的是在计算经验估计时使用的 V_Pi i。一般来说，我们通常会将 V_Pi i 作为基线。这意味着我们需要计算它。我们的估计方法可能来源于蒙特卡罗，也可能来源于TD方法。所以目前我们所看到的是，使用这些作为基线。我想要澄清的是，在执行蒙特卡洛返回和执行TD之间有几个不同的地方可以切换。其中一个是基线。我们有一个基线函数需要减去。我们还有一个G_t素数，对吧？因此，重新考虑我们的一般方程，这样我们得到的就是delta theta，v of theta。这是我们的参数，指定我们的策略参数。我们已经提到，这大约等于平均值为1/m的总和 i = 1 到 m 的一些奖励。我会将其包含在内。

在 t = 0 到 t - 1 的范围内进行求和，对，我改变主意。好的。我将这放在这里是因为最终它会成为一种我们可以以多种不同方式使用的函数。好的。这就是我们一直在研究的基本方程。我们已经提到，关于我们的策略参数值的导数大约等于取样自该策略的 m 个轨迹的总和，乘以我们在每条轨迹上获得的总奖励，再乘以所有时间步长的政策导数的总和，嗯，考虑我们在特定状态下采取的行动。好的。我们说这个方程很吵，嗯，但是无偏见的。现在我们可以考虑改变这一点作为目标。所以这就是对政策价值的无偏估计。现在我们可以考虑用其他方法替代。好的。

因此，我们可以在这里进行各种活动。我们可以尝试使用时间差异或蒙特卡洛方法。如果我们使用值函数来实现这一点，或者尝试显式计算值函数或状态-动作值函数，通常就被称为“评论家”。因此，评论家计算价值V或Q。因此，当我们谈论演员评论家方法时，我们有了策略的显式参数化表示，以及值或状态-动作值的显式通用参数化表示。有了这个，我们就可以设想利用它来改变我们的目标。我想要强调的是，演员评论家方法将这两者结合在一起，将策略与评论家结合起来。其中最流行的可能是A3C，这是由Mnich等人提出的。

在2016年的ICML会议上推出的A3C算法备受欢迎。这是深度神经网络的一种版本。尽管演员评论家方法已经存在很久，但A3C是深度神经网络中最流行的版本之一。如果要用价值函数来计算策略梯度公式，可以通过以下方式：计算导数得出与之前几乎相同的方程。因此，价值函数的导数等于通过对可能遇到的轨迹进行期望值计算，该轨迹中所有时间步骤的总和，再乘以策略参数的导数，最后再乘以$S_t$的Q值。

通过计算 w 减去 S_t 的 b，您可以插入对 Q 函数的估计，而无需进行蒙特卡洛估计。另一种表示方法是，如果我们将其视为价值的估计，这实际上就是我们的优势函数。但它可能指的是我们的——因此我们在这里有一个优势函数。您可以在这里定义这个优势函数，它是根据该集合的蒙特卡洛回报而确定的函数。这是一种不同的优势函数，它是 Q 函数减去一个由评论家维护的基线的结果，这个基线是一个值函数的估计。所以它们看起来非常相似，但您可以插入不同的选择。这些选择都会有不同的权衡。因此，蒙特卡洛回报估计是对当前政策价值的无偏估计。这种估计通常具有偏差，但方差较低。好的。

在讨论获得估计量时，批评者通常会计算不仅是基于 TD 估计或蒙特卡洛估计，而是可以在它们之间进行插值，这被称为 n 步返回，即对状态动作价值函数的估计。这意味着批评者可以使用一个估算器，比如从时间步 t 起的估计值，该值等于在时间步 i 上获得的实际值，其中 i, 1 表示在第 i 集的时间步 t 上获得的实际奖励。

从当前状态 \( S_t \) 到下一个状态 \( S_{t+1} \) 的折扣奖励和价值 \( V \)。因此，这基本上类似于 TD(0)。

但正如你在这里看到的，应该有，你知道，可能有某种方法可以在这两者之间进行插值。这些通常称为n步返回。例如，你可以这样做，你可以说，我将时间步骤i的奖励与时间步骤t+1获得的奖励相加，然后进行线性插值。这只是众多插值方法之一，落在两个极端之间，一个是单步奖励，另一个是总结所有奖励，你可以在这些方法间进行大量的插值。为什么要这样做呢？这样做通常会带有一些偏差，但方差较低。它将是无偏估计，但方差确实会增加。没有理由认为最佳解决方案一定是这两个极端之一。因此，您可以在TD估计值和蒙特卡罗估计值之间进行插值。所有这些只是构建回报的方法之一，然后您也可以减去基线。

在传统机器学习中，超参数的选择通常可以通过验证或交叉验证来确定，这是正确的做法。但是这种做法在深度强化学习中是否适用呢？考虑到计算量可能过大，我们可能需要做一些简化。在标准机器学习中，调整步长等超参数会被视为标准操作，但在强化学习中是否也是如此呢？或许这样的操作成本太高了吧。当然，你可以选择这样做。我认为这个问题很有趣，人们可能会采取一些技巧来解决，在强化学习中，也许会看到更多有意思的方法，例如使用TD(0)。

因此，这为我们提供了一种不同的方法，可以将这些目标插入到我们通常使用的策略梯度算法中。在进行梯度估计时，可以插入这些目标，以在偏差和方差之间找到平衡。实际上，这改变了我们的优化目标，以及计算梯度的方式。接下来我想谈一下的是，一旦我们获得了梯度，无论我们选择的获取方式是什么，我们都需要确定沿着这个梯度走多远。这为什么如此重要呢？这很重要，因为它仅代表了局部的估计值，即你正在给出该处的梯度估计。那么，你更新评论者的参数的频率有多快？这取决于具体情况。你可以根据需要异步更新这些参数。

因此，您可以为您的批评家和您的政策以及原则拥有不同的线程和网络。您可以持续地更新您的批评家，就像您知道的那样，您可以使用DQN来进行大量备份。一般来说，这取决于，我认为您可能会有一个时间表。是的，通常您可能会执行10或100次之类的操作，具体情况因应用程序而异，嗯。但是批评家没有理由只需要按照与您更新政策相同的时间表进行更新。异步执行通常是很有意义的。好的，因此，如果我们考虑一下这里发生的情况，这就是我们的参数化策略。这就是我们的价值函数。我们有一些疯狂的功能。好的，然后我们计算梯度。而且这个梯度，局部来说还是不错的。所以这里的东西就像线性的一样，看起来相当不错。但当然，当我们像这里这样走得更远时，情况就会变得很糟糕。就像如果我们尝试遵循梯度太远，我们将得到一个与真实值函数非常不同的估计值。

因此，在这种情况下选择步长时，考虑希望步长能达到多远是很重要的。我们需要搞清楚在梯度中应该走多远，这一点至关重要。可能会有人提出，这是显而易见的，不是吗？就像在任何监督学习问题中都需要小心地进行梯度下降或上升一样。在使用随机梯度下降时尤其如此。当然，不要走得太远，因为可能会导致过度调整，特别是当使用线性近似时。有人知道为什么强化学习情况可能更糟吗？为什么考虑步长更为重要？这与数据的来源有关。确实如此。因此，当你有一个不佳的策略…

当你收集数据时，可能会走向错误的方向。在监督学习中，数据是由独立同分布（IID）生成的，因此你为随机梯度下降选择的方式并不重要。而在强化学习中，这个选择决定了我们在迭代中使用的下一个策略来收集数据。如果我们采取了极差的策略，我们可能就无法获得关于函数的最佳值的实际数据。因此，更重要的是仔细思考我们的发展方向，希望能够持续改进。在强化学习中，这一点至关重要，因为我们需要考虑我们的步骤是如何执行的，因为这会影响我们收集的数据，以及我们的策略π。我的一个同事在谈到类似问题时也提到了这一点。

这段描述有点像在谈论一个人类从悬崖上跌落的场景，对吧？ 就像这些人一样，如果你所在的地方的政策真的非常糟糕。 你可能无法获得更多有用的数据。 这样一来，你就无法准确地计算梯度，然后就会陷入困境。 你可能会陷入一个非常非常糟糕的状况。 所以，我们需要认真考虑这方面。 有一种方法可以尝试，就是进行线性搜索。 我们正在讨论如何执行和确定步长的问题。 因此，我们可以尝试沿着梯度进行某种线性搜索。 在这方面，虽然方法简单但费用高昂。 通常会忽略线性近似效果较好的地方。

因此，我们希望比这做得更好。 当我们进行更新时，我们要确保每次都有进步，这意味着我们希望通过调整步长或梯度下降的幅度来保证单调改进。 我们的目标是确保在更新后，新的价值函数（V pi i+1）大于或等于之前的价值函数（V pi i）。我们希望通过调整步长大小来实现这一目标。
让我们再次思考我们的目标函数。我们有值函数和参数化值函数。因此，V(theta)等于在我们的策略下的预期价值，该价值由在t=0到无穷大的时间步长内，gamma的t次方乘以奖励和状态的乘积的和定义。
这就是我们要做的事情。

根据我们的政策得到的一系列状态是表达参数化策略价值的基本方程。我们的目标是寻求一个比旧政策更有价值的新政策。然而，我们只有旧政策的样本。因此，我们收集策略 pi i 的样本，然后尝试找出下一个策略 pi i + 1 应该是什么。这是一个根本性问题。我们可以通过从参数θ的 π 采样得到轨迹。我们希望预测 π 和 θ 的值。我们输入 π i 和 π i + 1，然后尝试通过更新这些值来找到最大值，以确定新值是什么。我们想要了解新参数的含义，因为这涉及到离政策的根本性问题，我们需要知道上一个...

我们想要通过政策数据了解下一个政策应该是什么。因此，我们需要首先强调我们新政策相对于旧政策的优势。我现在会转向一般政策梯度。所以我们有关于Theta的价值函数V。这意味着我们的新参数化政策值会等于我们旧政策的值再加上一些内容。当我们执行新政策时，会得到状态和操作的分布。尽管我们目前对此并不清楚，但让我们忽略这一点，关注于t=0到无穷大时的总和奖励，即政策优势π。通常情况下这是有效的。

嗯，并不一定与参数化有关。这只是在表明，由参数化为π的任何策略的价值等于另一策略的价值，再加上您在目标策略下达到的所有状态和操作的总和，以及采用此新策略相较于旧策略所获得的优势。所以，这表达了我们如何描述新策略的价值与旧策略的价值之间的关系。它与原始值函数完全一致，只是增加了您运行新策略后获得的优势。是吧？下标应该是关于优势的π符号吗？ 是的。因此，我们正在做的是——让我把这个问题记录下来。这确实是一个很好的问题，让我记录下来，以便于进一步研究。好的，我们已经得到了θ的V值加上所有状态的总和，我们将使用s的μπ符号。请记住这是平稳分布。嗯，我们用它来表示。

当我们执行参数化波动符的新策略时，我们将达到状态的稳态分布。 这是 θ 表示的波动符。 好的吗？ 然后乘以优势函数。 好的。 所以这意味着在我们期望的政策之下，S_t 和 a_t 都是合适的。 这里的优势是使用旧数据。 好的？ 这样我们就可以进行比较。 这样做有什么用处呢？ 这使我们能够比较 s_a (S_t, a_t) 减去我们旧政策的 S_t 值的 Q。 这让我们能够比较如果我们采取新的行动，效果会好多少。

好的，让我整理一下这段对话内容，并进行适当的翻译和澄清。原文中的一些音频不够清晰，我将尝试根据上下文理解并翻译完整内容。

对话内容整理：
对话者1：问题之一是我们并不知道这一点。您说的是从时间 t=0 到无穷大求和吗？谢谢您。回答是肯定的。
对话者2：又是[重叠]，所以问题是[听不清]？不，谢谢您让我澄清这一点。我们目前所做的是对所有时间步长进行期望，也就是说我们将实现的轨迹在新政策下。重新表述一下，我们在各个状态之间有一个固定的分布。我们要考虑达到这些状态的概率，并根据优势对其进行加权。这样，我们从时间平均转变为状态平均。这个步骤有意义吗？
对话者1：我们可以考虑我们的价值函数，并在时间步骤上对其进行平均，也可以认为我们是对所有状态以及每个状态进行平均。遵循新政策相较于遵循旧政策，可以获得多少相对价值？[听不清]。

根据上述整理内容，将其翻译为更易于理解的中文澄清如下：
对话者1：问题之一是我们并不确定这一点。您说的是从时间 t=0 不断累计的求和吗？谢谢您。回答是肯定的。
对话者2：又是[重叠]这个地方的问题是什么？不是。感谢您让我澄清一下。我们目前所做的是对所有时间步长做期望，也就是说我们将在新政策下所达到的轨迹。换言之，我们在不同状态之间有一个确定的分布。我们需要考虑到达这些状态的概率，并根据优势来加权。于是，我们从时间平均转向状态平均。这个方法合理吗？
对话者1：我们可以考虑我们的价值函数，对时间步骤上的价值进行平均，也可以认为我们是对所有状态以及每个状态的平均。相对于旧政策，遵循新政策可以获得多少相对价值？[听不清]。

抱歉给您带来困惑，让我们梳理一下内容。您说的是，我们需要研究各个州的情况，计算在新政策下达到某一州的概率，以及相对优势是什么？您提到的部分应该不包括错误。我们有一个波形符号，加上它，我们可以得到相对于所有状态的优势项。关于pi theta波浪线和波浪线pi之间的区别，我只是要澄清这一点。

这段对话是关于参数化策略的讨论。其中提到了使用theta参数化的策略，有关如何明确策略以及在值函数中使用theta来参数化的内容。对话中还强调了在这种情况下任何一种方式都是可以的。最后指出了使用pi符号时是指根据新参数化方法来定义策略。

有人对最后那个符号有什么疑问吗？是的？好的，请报个名。我有点困惑，主要是因为它与幻灯片上的符号有点不同。我想知道幻灯片应该是一样的，但是我正在尝试理解... [笑声] 我想知道在这种情况下我们是否要总结可能采取的行动，或者我们是否假设对每个状态使用董事会上的政策采取单一行动？对，你是对的。我忘记了这一点。我要走了。再重复一遍，好的，好的。假设V的Theta符号我将沿用相同的符号，因为幻灯片上V的Theta加上状态的Pi符号的稳定分布等于所得到的分布。那我们得到的就是目标政策下的折扣权重。

在 Pi 波形符下。好的。对 A 求和。好的。这就是加权事件，这是各州的加权分布。我们从时域转向考虑州时间的分布，着眼于我们在目标政策下可能采取的所有行动，以及每项行动相对于我们之前政策的相对优势，噪音，好吗？在某些方面，这看起来应该与模仿学习非常相似，对吧？所以我们再次思考，我们不是考虑随着时间的推移而获得的奖励，而是考虑在新政策下我们可能得到的平稳分布是什么，以及它与平稳分布相比如何根据我们的旧政策，我们会这么做。嗯，到目前为止我们正在研究的是，我们正在研究目标策略下的噪音平稳分布。

问题在于我们无法获取有关 pi 波浪线的样本数据，这使得我们无法计算相关数值。这只是一个表达方式。由于我们没有从 pi 波浪线那里得到样本数据，而只从 pi 那里得到了样本数据，所以我们无法做出相应计算。为什么我们要这样做呢？回顾过去，我们为什么要尝试这些事情呢？我们的目的是试图找到一种比我们之前策略更好的新策略，而进行普通策略梯度训练时会受到噪音的影响。在这里，我们试图估计当前策略的导数关于当前策略的 pol-，但一旦我们进行了这一步，我们对该值一无所知。所以，我们在这里尝试探究的是，在执行新策略之前，我们是否能够了解其价值是多少。我们的目标是通过将其与之前政策的值以及旧政策和新政策之间的某种距离联系起来，来尝试以某种方式了解其价值。理想情况下，这应该是根据我们可以利用当前可用样本数据实际评估的事物来计算的。

那么，我们的目标是要前往那个地方，对吧？是的。因此，我们有这个很好的表达式，但我们却无法计算它。因此，我们需要创建一个新的目标函数，对吧？我们将以一种反向的方式来做这件事，因为我们会对它进行调整，然后我们将展示为什么这样做是有益的。那么，我们想要做什么？我们希望利用这个。如果我们拥有这个，那么我们就可以将新政策的价值与旧政策的价值进行比较。问题在于，我们没有这个，因为我们没有关于新政策下固定分布的信息。因此，我们需要定义一个名为 L_Pi 的目标函数，如下所示。它是旧政策的价值加上所有州的总和，即旧政策的固定分布减去折扣分布。这就是不同的地方。因此这是旧政策，你们当前的政策，对吧？然后，表达式的其余部分看起来是一样的。现在，请注意，我们可以计算这个，对吧？

这意味着我们可以对当前事件的所有轨迹进行平均处理，根据当前数据估计我们的稳定分布，了解新政策的行动方式。因此，如果有人提出一个新的策略pi，我可以对其进行评估，并评估其优势，因为这个优势是根据我之前的策略Pi定义的。只要我有旧政策的国家行动价值函数表示，我就能够进行评估。因此，现在所有这些都是可以评估的。

可能会出现一些噪音，但我们可以计算处理。然后，我们将其转移到策略pi本身？是的。从符号的角度看，我可以互换使用pi的符号，它们只是一些用于计算的新参数。因此，我们的策略总是由一组参数来参数化。您可以将其视为新策略，也可以将其视为新参数，两者都是可以的。

当你遇到新的政策 π 并且尝试计算 μ，问题在于你缺乏关于 π 的数据。假设你已经运行了 M 次并收集了 M 条轨迹，这些轨迹是在旧政策 π 下收集的。由于你没有关于新政策 π 的数据，因此无法直接估计新的轨迹。通常情况下，如果新政策 π 与旧政策 π 不同，你将得到不同的轨迹，从而导致无法估计 μ。

如果回到一般的政策梯度框架，我们可以采取如下措施：我们有一个策略 π_i，我们运行它，并从中得到 D 条轨迹。我们可以利用这些数据来估计 μ。这将为我们提供所需的信息。

当我们关注政策 Pi_i 时，我们所经历的状态和采取的行动的数据是有噪音的，我们还没有关于 Pi_i + 1 的数据，因为我们还没有执行该政策。为了更清楚地估计旧政策的稳定分布，你基本上需要查看所有可用数据，如所有轨迹，然后看一下在每个状态下花费的时间比例是多少。这些数据将如何转化为 mu？你可以简单地统计，也就是说，你可以计算在某个状态下采取某个行动的次数，从而直接估计 mu。通常情况下，您可能需要一个高维参数函数来处理，但如果它不适用，您可以将其看作是自参数化的，并利用现有的策略数据进行适应。直观上看，这有效吗？因为我们假设状态分布在不同政策之间不会发生太大变化。

好的，我将上述内容重新整理一下：

直觉上，人们可能会问为什么某个方法会奏效。尽管我并没有解释为何这方法有效，只是提到了我们可以这样做并且这是可以计算的。

但是目前我还没有说明为什么这是有益的。不过我们将会证明，这样做将让我们能够达到一个下限，并且我们可以不断改进这个下限。

需要注意的一点是，如果你采用这种方法，如果你采用 π 的 Lπ，那么这就是目标函数，如果你在其中插入旧策略，这等于 V 的 θ，可以吗？换句话说，如果您在同一策略下评估这个函数，它将只提供给您价值，对吧？

所以保守地说，我只是简单介绍了一下，我们之后将继续深入讨论，但是我们可以使用它来实现所谓的保守策略迭代。

嗯，我们的直觉是，让我们首先从混合政策开始。想象一下您有一项新政策，它是旧政策和不同内容的混合体。你可以用一个公式表示为：新政策等于（1减去α）乘以旧政策加上α乘以一些新策略。这意味着您在现有政策基础上混合一些新内容。在这种情况下，如果您采用这个目标函数并评估新政策，您可以确保新政策的价值大于或等于旧政策的价值。这是因为您可以通过旧政策中的数据减去一些值来计算这个目标函数，最终可以降低新政策的价值。

您好，我只是想以另外两个观念做结尾，即再次强调，当alpha等于0时，这意味着pi new与pi old相等，因此变为0。这里的关键是，新政策必须优于或等同于旧政策。因为它们一样，政策也相同，这就很重要。所以，与我们预期的有些不同，这涉及到PDF技术挑战。因此，我要结束的是，下一步是要展示我们可以将其用于实质性地推导出新的价值函数下限。基本上，我们可以证明，如果我们改进超过下限，就可以保证实际价值函数是单调递增的。所以，我们将经历这个过程。

嗯，我还没有决定我们是否会在周一进行活动，因为这是中期审查的时间段，或者我们是否会等到下周中期审查之后再安排。嗯，作业要到期中之后才发布。因此我们有更多时间来完成。嗯，等我们完成这部分内容后，我会简要介绍政策梯度的要点。谢谢。

在我们开始之前，我想简单介绍一下中期的后勤安排。我们将分成两个房间，你将被指定到一个房间，根据你斯坦福身份证上的第一个字母。确认信将通过电子邮件发送。最后我们会在盖茨B1教室或卡伯利礼堂举行，具体位置将根据你斯坦福身份证上的第一个字母确定。请携带一页笔记，可以是打印本也可以手写，单面即可。有关期中考试的其他问题吗？如果有任何疑问，请通过Piazza与我们联系。今天我们将继续讨论政策梯度的相关内容，我们几乎完成了策略搜索的部分。期中考试将安排在周三，周一是假期，接着我们还将发布本周的最后一份作业，这次作业将涉及政策搜索。之后我们将着手进行项目，这将是剩下学期的主要任务。期中考试后。

我们将要开始快速探索和快速强化学习。 我想确保今天完成政策搜索， 因为您将在本周晚些时候发布作业。 所以，我们将用大约 20 到 25 分钟的时间进行政策搜索， 然后简要回顾一下期中考试材料。 有人有任何问题吗？ 另外提醒大家，每次提问时请报上你的名字，这样可以帮助我记住你，也让其他人知道你是谁。好的。在过去几堂课上，我们已经开始讨论基于策略的强化学习，我们试图找到一个参数化策略来学习如何在环境中做出正确的决策。对于我们的策略参数化，我们假设有一些参数向量，我们可以用softmax或深度神经网络之类的东西来表示这个策略。然后，我们希望能够计算这些策略的梯度，以便学习到具有高价值的策略。

因此，我们引入了一种常见的策略梯度算法，其思想是，在某种方式下初始化您的策略，并设定一个基线。在不同的迭代中，您会运行当前的策略。我们的目标是通过运行这些迭代来估计梯度。因此，我们要做的是估计当前策略的梯度，即相对于当前策略的 dV d theta。具体来说，您会完整执行当前策略的轨迹，即在环境中执行您的策略，获得状态、动作、奖励、下一个状态、下一个动作和下一个奖励。然后，您会计算回报和优势估计，将回报与基线进行比较，调整基线，然后更新策略。这是我们讨论过的一种常见的策略梯度算法。然后我们开始讨论在这个算法中做出的许多不同选择。

基于策略梯度的算法都会遵循一种特定的公式，用来估计某种回报或目标的决策。通常会选择一个基线，然后需要在计算梯度后做出一些决策，即我们应该沿着梯度走多远？这可以帮助我们确定我们在梯度上的移动距离。在第一部分中，我们讨论了如何估计当前位置的值，这个估计将帮助我们推测梯度。我们提到了一个简单的方法，即推出策略并观察回报，这与蒙特卡罗估计非常相似。因此，通过推出一个情节的策略，我们可以获得对值函数的估计。

然而，就像我们在蒙特卡罗中看到的那样，这是一个无偏估计，但方差很高。因此，我们正在讨论如何发挥各种作用——使用与过去相同的工具来尝试平衡偏差和方差。因此，我们特别讨论了如何通过引入引导和函数逼近来解决偏差的问题。就像我们在 TD MC 中看到的一样，讨论的是值函数逼近。因此，我们反复看到这些相同的想法，即我们试图了解特定政策的价值是什么。在我们对这个值进行估计时，我们可以在获得正确决策的无偏估计量与有偏估计量之间进行权衡，这可能使我们更快地传播信息，并更快地做出更好的决策。我们还讨论了一个事实：这些是 actor-critic 方法，既维护策略的显式参数化表示，又维护价值函数的参数化表示。不过，上次我们真正开始讨论的是：“好吧，”

大家都知道，我们可以利用各种现有技术来尝试估计目标和价值函数。然而，还有一个额外的问题：我们应该沿着梯度前进多远呢？因此，一旦我们估计了策略的梯度，我们需要确定在计算新策略时要沿着该梯度移动多远。我们认为在强化学习和监督学习中这一点尤为重要，因为不论我们采取什么行动，考虑哪种新策略，都将影响我们下一步获取的数据。因此，对我们来说，思考要在多大程度上努力发现新策略是至关重要的。我们讨论的一个理想特征是如何确保单调改进。我们真正追求的是确保单调改进，这是我们的目标。我们提到了追求单调改进，在DQN或其他许多算法中这是无法保证的。因为在许多高风险领域，...

在金融、客户和患者等领域，确保部署新策略前的预期效果至关重要。然而，在没有新政策数据的情况下，我们不希望随意尝试所有可能的政策，因为结果可能是灾难性的。因此，我们希望利用现有数据来指导我们采取下一步行动，并确定哪种新政策可能表现最佳。因此，我们的主要目标是通过策略梯度算法找到一组能够最大化价值函数的策略参数。挑战在于我们目前只能访问由当前政策生成的数据，被称为“旧 π”。这些数据通过一组参数θ来参数化，我们也用θ_old来表示。在整个策略过程中，我们会涉及到政策梯度的概念，在讨论政策时会涉及到θ的概念。

但请记住，π和θ之间存在直接映射。对于每个策略，都有一组精确定义的参数。因此，无论是在讨论策略还是参数时，它们都指的是完全相同的事情。因此，挑战在于我们拥有来自当前策略的数据，通过某些参数集定义，我们想要预测不同策略的价值。这就是离线学习面临的挑战。因此，我们前面讨论的是如何用我们已知的信息来表达策略的价值。我们讨论了如何将其表示为相对于当前策略的优势。因此，如果考虑一个新参数化值（具有一组新θ参数形式的新策略），这等于另一个策略由一组θ参数化的值加上估计的优势。因此，我们可以这样表达：在新策略下我们期望达到的状态分布。

乘以，如果我们遵循新政策，我们将获得在旧政策下的优势。 我们尝试这样做的原因是为了思考这里的主要目标是什么。我们试图找到一种方法来实现政策梯度，确保我们的新政策比旧政策更好。但我们想要在不实际尝试新政策的情况下做到这一点。因此，我们尝试通过重新表达新政策的价值来评估我们可以获得的数量。那么我们可以访问什么呢？我们可以利用当前政策的现有样本，希望利用这些样本和我们观察到的回报来在部署新政策之前估计其价值。这就是我们想要达到的目标。在这里需要注意的是，也许我们可以访问新策略的显式形式，就像我们考虑添加到神经网络的任何新参数一样。我们可以设想估计优势函数，

然而，我们并不清楚在新政策下状态的分布情况，因为这需要我们实际运行它。因此，我们正在讨论的是——让我们定义一个新的目标函数，这只是一个不同的目标函数。它可能是好的，也可能是坏的。我认为这是一个好迹象——尽管目前它只是我们可以优化的一个变量。因此，我们可以称之为新的目标函数来优化的变量，它将是先前数值的函数。请记住，这个函数始终代表着θ和π之间的[噪音]直接映射。因此，我们只是指出，这看起来类似于我们刚刚讨论的目标函数，这确实是新政策的价值，但我们不清楚各州在新政策下的平稳加权分布是什么。因此，我们现在将在当前政策下用固定分布来替代。总的来看，这不会——因此它并不等同于您的新政策分布。

通常情况下，只有当两个策略完全相同时，才能在这两个策略下获得相同的状态分布。有时候，虽然在两种不同的策略下可能会得到相同的状态分布，但这意味着它们具有相同的价值。因此，总的来说我们预计情况会有所不同，但现在我们会暂时忽略这点。我们只是想表明这是一个目标函数，是我们可以优化的内容。这么做的一个好处是我们有当前政策的样本数据。因此，我们可以想象仅利用这些样本来估计这个期望值。我还希望强调的是，这个新目标函数被标记为L。如果您在当前策略下评估目标函数L，那么当您将旧策略带入到目标函数中时，它将恰好等于您当前策略的值。因此，第二项变为0，回到原点，因为针对现有策略的优势是0。因此，如果您评估旧策略，那么这个目标函数将刚好等于旧策略的值。

另一种情况是新政策时会有所不同。这跟重要性抽样有什么相似之处？这是一个很好的问题。在重要性抽样中，我们通常会根据现有分布重新加权，以逼近我们想要的分布。而针对新政策，我们会在每个状态的级别上进行研究，尝试去摆脱对值函数的政策估算。在NeurIPS上刚刚发表了一篇非常棒的论文，就在一个月前，作者是Lihong Li和其他同事。他们研究了如何重新加权平滑分布，以尝试改进政策估计，尝试使用直接重新加权的方法，比如mu pi和mu pi waveforms。在这种情况下我们不会这样做，但有一些非常好的想法可以帮助解决这个问题。

在减少长期问题的方差方面真正起作用的是什么呢？嗯，在这种情况下，我们仅仅是进行替换，因此忽略了差异。我们没有应用重要性采样，只是假设我们获取的状态分布是完全相同的。实际情况并非如此，但我们将证明最终会实现我们想要的（真正优化的）有用下限。好的。你可能会问，如果使用这个目标函数，结果可能是好的，也可能是不好的。那么，如果我们对其进行优化，我们能否确保针对这种错误的目标函数进行优化时得到的新价值函数优于旧价值函数？因为请记住，这是我们想要达到的目标。我们并不太关心我们正在优化什么，我们关心的是我们得到的新价值函数实际上比旧价值函数更好。所以，就像之前提到的，如果您有一个混合策略，混合了当前策略和新策略，那么假设您有一个旧的策略 pi，还有一些其他策略，虽然我还没有提到如何获取这些策略，但我只是说有其他一些策略，这些策略定义了您的新策略。

因此，在概率为 1 - alpha 的情况下，您将继续执行先前的操作。当概率为 alpha 时，您会采取新的行动。这样，您就能确保新保单的价值不会低于特定下限。因此，新政策的价值大于或等于我们的目标函数减去某一固定数量。换句话说，通过针对这个奇怪的目标函数进行优化，您实际上可以限制新策略的表现。这看起来似乎是有希望的，但总的来说，我们不仅仅考虑混合策略。因此，这个定理所说的是，对于任何随机策略，而不仅仅是这种奇怪的混合策略，您都可以通过使用这个略显奇怪的目标函数来限制性能。在这方面，特别是，我们要定义总变差的距离。两个策略之间的变差（DTV）可以用一个点来表示，其中涉及许多不同的操作。

这里讨论的是政策表现的概率分布。这相当于两个策略中每个动作被执行的概率之间的差异的最大值。因此，它展示了一种最大的变化，即一种政策下采取某个行动与另一种政策下采取相同行动的概率有多大。然后我们可以通过对所有状态上这个值取最大值来得到总体变化的D max。因此，基本上说的是，在所有状态中，这两种政策在特定行动上的最大差异。那么它们之间最大的不同在哪里呢？这个定理表明，如果你拥有这个数值，通常我们无法对其进行评估。但它的意思是，如果你知道这个数值是多少，那么你就可以对其进行定义。如果你使用这个目标函数L，那么你的新策略值至少是你计算的目标函数减去这个数值，这个数值就是总变化距离。

在计算函数的过程中，我们需要关注最大距离和总变异。 这为我们提供了信心，如果我们优化目标函数L，我们可以限制价值函数的范围。 然而，处理这个距离——无论是最大距离还是总变异距离——并不容易。 因此，我们可以利用KL散度的上限来处理其平方，从而得到一个更易处理的新界限。 通过研究两种策略之间的KL差异，我们再次得到了类似的界限。 那么，这有什么用呢？ 这意味着我们有了一个新的目标函数。 如果我们使用这个新的目标函数，理论上我们可以得到新政策性能的下限。 那么，我们如何确保获得单调改进呢？ 我们的目标是实现单调改进。 我们希望V_ Pi i + 1 大于或等于V_ Pi i。 这就是我们的目标。 因此，i代表迭代次数。

我们希望新部署的策略实际上比之前的策略更好。那么我们应该怎么做呢？首先，我们有一个目标函数，即下界目标函数，这里我们要定义 pi i 的 Mi 等于 pi i pi 的 L。我要重申的是，这个方程实际上是上一张幻灯片中的复制内容，即 - 4 epsilon gamma 除以 pi i 的 1 - gamma 平方 DKL_ max。好的。所以这就是下限，就像我们之前在上一张幻灯片中定义的那样。对吗？因此，我们现在所谈论的是，我们新策略的价值 - 也就是我定义的 M 函数。因此，我们要说的是新值至少应该和这个下限一样好。所以我们可以说 V_i + 1 将等于 pi i + 1 的 Mi，它等于 L pi i + 1。

这里只是在描述一个概念。 换句话说，我们的目标是相信我们能够得到比之前的价值函数更好的结果。 现在，如果我们想确定当前政策的下限是什么，我们需要看的是 pi_i 的 Mi 。 所以这将等于 pi_i 的 L pi_i 。 将其代入方程中，得到 - 4 epsilon gamma - pi_i, pi_i 的伽马平方 DKL max。 这样做的好处是什么呢？ 嗯，原因在于相同策略之间的 KL 散度为0。 这意味着它们完全相同，KL 散度为0。 所以最终等于 pi_i 的 L pi_i。 我之前说过，如果我们回顾一些幻灯片来理解 pi_i 的定义，就能更好地理解。

在这里，Mi 的 L 是，如果按照当前策略评估它，它等于该策略的值。因此，当我们在当前政策下评估目标函数时，它与当前政策的值相同。现在，当我们往回走，它等于pi_i 的V。好的。这意味着什么呢？这意味着如果我想看一下我的第i+1次保单的值与旧保单的值相比如何，我们知道它大于或等于pi_i+1 的Mi。所以因为我们表示我们从这个定理中得到的V，即策略的新值大于或等于我们计算的下限。所以它大于或等于Mi of pi_i + 1 - Mi of pi_i。这意味着什么呢？这意味着如果您的新价值函数的下界比旧价值函数更好，则您将会获得单调改进。

因此，若该值大于0，则呈单调改进，意即对此下限进行优化，评估其数值，新下限高于旧下限，则值必须更好。这确保了单调改进。是的，为了澄清一下，对于这些值的比较，我们是否暗含将其视为无限范数，即更好？一般来说是的。我想说的是，他们可能会用到L2范数。但是，大部分情况下我们是否总是基于L∞范数来定义。当我们审视函数近似的L2范数时，会涉及到一些分析。大部分关注的是L∞范数，也就是考虑确保对于所有状态，这些状态至少与先前状态的值一样好。是的，所以我们的主张是，如果我们的下限提高了，

当您改进了两个下限时，例如对于现有保单和新保单获得了更高的下限，您一定会有所进步。这是因为您正在比较两个低点，确保新的下限高于其他下限会导致进步。如果您能够优化这些下限数值，同时确保当前策略的下限相等于该值，遵循这个政策就会带来进步。

基本上，这是在说，您确定了自己的较低价值，对现有价值进行了比较。因为您拥有一些东西，其下限优于现有价值，所以您知道新东西必须更好。因此，只要您按照该政策行动，就会有所进步。

Epsilon 似乎随着您的 Pi 而变化，并且它也是一个全局属性，绝对如此。 所以，这个讨论很有意义。 很少有人会问到这个问题。 所以请注意，您的下限是以 Epsilon 为单位的。 Epsilon 是对您有利的所有状态和行动的最大值。 在理论上，您可以评估这一点，尤其是在离散状态和动作空间中。 实际上，这是您不想要做的事情。 我认为这部分是比较正式的说法，如果您能评估这个下限。 我们现在要讨论的是一种更实用的算法，它试图采取一种保守的政策改进保证，并且实际上使其更加实用，更易于计算。 因为那是正确的。 是的，一般来说，评估这个 Epsilon 是很困难的。 现在您可以对其进行上限或下限估计。 但通常您并不知道这个 Epsilon 是多少。 请注意，正如共同指出的，这个 Epsilon 取决于策略。 所以，这意味着您可以进行有保证的改进。

这是最小最大化矿物形式之一。这是一个很好的主意，可以确保新的下限高于当前保单价值，实现渐进的政策改善。我想确保我们有足够的时间来进行中期审查。让我们简要讨论一下如何实施这一点，尤其是信任区域策略优化这种非常流行的策略梯度算法。这可能对你们有所帮助，因为你们中的一些人可能会在一些项目中使用它。虽然它可能不会成为强制性的家庭作业或期中考试的一部分，但熟悉它会是一个很有用的想法。再说一次，让我们看一下我们讨论的目标函数是什么。我们有一个L函数，通过减去一个我们可能难以计算的常数，将其转换为下界。在这种情况下，我们需要获取这个常数，并将其变为一个超参数，称之为常数C。

然而问题在于，即使能够计算出这个值，通常我们也无法确定其具体含义。即使可以计算或界定它，一般来说，我们会采取非常小的步长。从直观上来看，这是因为通常很难从当前政策推断出较远的情况。因此，要确保新值优于旧值，只需采取非常小的步长。从直观上来看，这是因为如果对政策做出的微小变化在某些平滑保证下，保单的价值不会发生太大变化。而且，如你所知，梯度通常是函数当前值的很好估计，非常接近。但我们需要尽快制定一个好政策，所以这经常是不切实际的。TRPO的主要思想之一是将其视为一种可信任的区域，并使用它来限制我们的步长。再强调一遍，如果回到这种常用的策略梯度算法模板，

我们需要为梯度选择一个步长。我们的想法是设定一个约束。我们会在这里定义我们的目标函数。我们不会明确地限制下界，而是会表达可以移动的范围。您可以调整梯度，但不能改变得太多。我们想要对KL散度施加一个约束，限制改变的幅度，告诉您在参数空间中可以改变政策的范围。好的，是的。所以下面我将简单阐述即如何实现它。所以主要的想法是，当我们考虑目标函数时，可能很容易评估，也可能不容易评估。即使在当前策略下，权重的访问可能受限，我们不能直接访问。我们只能访问根据当前策略推断出的样本。因此，一个想法是，我们不需要对状态空间进行显式求和，因为状态空间可能很复杂。

我们可以通过观察当前实际采样的状态并重新调整旧政策的权重来实现连续且无限的替换。 这就是我们的第一次替换。 接下来，我们希望将目标函数融入算法，以计算所需的数量，以确定新政策的最佳步长。 第二步涉及重要性抽样的问题。 我们有一个描述在新政策下采取行动概率的新量。 通过重要性采样，我们可以计算出在所有行动下的概率，尽管通常情况下这是一个连续的集合。 因此，我们不会处理连续集合，而是使用重要性采样来自旧政策的样本。

因此，根据当前政策，我们会检视我们采取行动的时间，并重新加权这些行动推动新政策的概率。这样，我们可以借助现有数据来近似期望值。接着，第三个替代是恢复对Q函数的优势，需要注意的是这三个替代并不会改变问题的优化解决方案。这些都是在考虑不同替代或方法来评估这些数值。因此，最终我们优化这个目标函数，并限制了我们对距离的限制范围。根据经验，通常只是抽样这种替代抽样分布Q，这就是你现有的旧策略。报告中还包含许多其他内容，这篇论文非常出色，涵盖了许多有趣的想法，但关于如何执行则略有省略。

这段内容涉及到一些复杂性，但简单来说，他们主要是在执行政策。他们在计算梯度，并考虑了一些约束，使用KL约束进行线性搜索。重要的是意识到他们受到保守政策改进的启发，并努力使其更实际和快速。他们已经将这种方法应用于许多不同问题，比如在运动控制器中，处理连续的动作和状态空间时非常有帮助，取得了不错的结果。总的来说，经验表明策略梯度是一个非常有用的工具。TRPO（Trust Region Policy Optimization）作为一种策略梯度方法，已在2015年的ICML上发表文章，并被引用数百次，成为主要的基准之一。

策略梯度算法模板通常如下所示，无论您是在查看现有算法还是正在尝试定义自己的算法。在每次迭代中，您都会执行策略，并收集数据轨迹。然后计算一个目标，可能是奖励或Q函数。在权衡偏差和方差之后，估计策略梯度，并希望沿着该梯度采取一步，以确保单调改进。

需要注意的是，您应该熟悉这些常用方法和强化学习，并理解这个通用模板，了解不同算法如何将其实现。不必推导或记住所有公式，将在作业3中有更多机会练习。在我们对迄今为止所做的工作进行简要总结之前，有人对此有任何问题吗？

好的。我们来换个话题。因此，现在我们将简短回顾一下我们迄今为止所做的工作。从教育心理学的角度看，有许多证据表明，思维的重复空间对学习非常有帮助，就像进行强制回忆一样，这也是参加考试的其他好处之一。因此，今天我们要做的只是快速回顾一些不同的主要观念。再强调一次，强化学习通常涉及优化、延迟奖励、泛化和探索，这些我们还没有深入讨论。这个话题实际上不会在期中考试中出现，但我们将在未来更多地涉及这方面内容。这是一个非常关键的话题，我认为这也是强化学习引人入胜的一个原因。不过，其他主题同样非常重要，而我们迄今为止花了一些时间在这些主题上。因此，就期中考试和课程思考而言，

在第一天，我列出了一系列学习目标。其中一些目标将在考试中明确评估，到课程结束时，包括考试，你应该非常熟悉强化学习的关键特征，使其与其他机器学习问题和人工智能问题有所区别。我们在第一天花了一些时间讨论这个问题，但事实上，智能体正在收集自己的数据，这些数据会影响它学习策略的方式。这就带来了数据审查的问题，因为智能体无法知道没有经历过的生活情况，这与监督学习有很大不同。另一个非常重要的事情是，当你遇到一个应用问题时，尝试确定为什么要将其描述为强化学习问题或者为什么不这样做。对于这种情况，你会怎么做取决于具体情况，因此问题并没有唯一答案。在这种情况下，最好思考如何定义状态空间、动作空间和环境动态。

奖励模型的设计有多种方法，建议在课堂上尝试使用的算法也有多种选择。在工业领域，可能会出现比特定算法更多的情况。重要的第三点是要了解如何评估强化学习算法的表现。评估可以采用不同的性能指标和评估标准，用来比较不同算法的优劣及各自的优缺点，包括偏差、方差、计算复杂性、样本效率等方面。目前我们讨论的重点是在已知世界运作方式的情况下进行规划、政策评估、无模型学习、价值函数逼近，再到模仿学习和政策搜索。强调了一个事实，就是在一般的强化学习中，可以尝试找到策略或模型的价值函数，使得该模型能够产生相应的行为。

一个价值函数能够生成政策，但并非必需，即使没有模型也可以获得政策。因此，很快我将完成这部分内容，相信你们在之前的课程中也已经接触过这些概念。迄今为止，我们讨论的内容几乎都基于世界是马尔可夫决策过程的假设。然而，我已经提到，现实世界往往不是一个马尔可夫决策过程。在MDP的情况下，我们假设只需要考虑当前状态即可，代表了所有之前历史的统计信息。因此，我们无需追踪全套状态、观察、行动和奖励，只需关注当前观察即可做出正确决策。对于这一点，了解马尔可夫性质的重要性以及可能违反的情况是必要的，同时理解模型、价值函数和策略等概念也是非常有用的。在规划中，我们假设您了解世界的工作方式，掌握动态模型。

您了解奖励模型，但仍然很难弄清楚如何采取行动。这就像了解围棋游戏一样，即使您知道所有的动态和奖励，试图找出围棋中采取的最佳决策仍然是非常非常计算密集和棘手的。在学习中，我们并不知道动态和奖励，因此我们仍然需要收集数据来学习一个好的策略，即具有高价值、高折扣预期奖励总和的策略。 我们提到了贝尔曼备份算子，这是一种缩写。如果您的折扣因子小于1，这意味着通过重复应用，您最终会收敛到一个固定点。我们讨论了值迭代和策略迭代。在值迭代中的第k次迭代中，您做出k次决策，始终计算最优值，然后使用它来备份，并得到第k+1次策略。在策略迭代中，如果您永远使用同一策略，那么您始终具有策略和该策略的值。然而，这可能并非是一个很好的策略，因此需要进行更新。就像我们所看到的那样，AI智能体可以通过大语言模型来学习复杂的决策过程。

这种方法与一种策略梯度算法密切相关，您可以在其中尝试估计该策略的梯度。因此，在策略迭代过程中，我们结合了评估和改进，这与我们在策略梯度方法中见到的类似。在这个过程中，我们首先计算政策的价值，然后使用它来采取行动并改善策略。在没有模型且没有额外模型的情况下，通常我们希望计算Q值，这样我们可以直接改善策略。这些概念有助于检验您的理解，并值得回顾。它们有些类似于小概念问题，可能会在考试中问到。因此，让我们花点时间来检查我们的理解，考虑具有有限状态和操作的MDP（以查找表表示），这意味着我们只有每个状态和操作的表条目。当γ小于1时，初始值函数的设置会影响最终计算值吗？为什么或为什么不？价值迭代和策略迭代是否总是产生相同的解决方案？

在有限状态和动作的马尔可夫决策过程（MDP）下，策略迭代所需的迭代次数是有界的。具体的迭代次数取决于状态空间和动作空间的大小，以及折扣因子的值。通常情况下，当状态空间和动作空间有限且折扣因子小于1时，策略迭代所需的迭代次数是有限的。在实际应用中，可以通过数学推导或实验来确定实际迭代次数。

我们要进行投票。 所以，我想问一下，谁认为值函数的 value- 的初始设定并不重要？好的，是的，没关系，也没关系，所以没有关系。为什么不呢？因为只有一个固定点。因为它就像-嗯，贝尔曼算子是一个收缩算子。只是单身。那又怎么样——谁认为价值迭代和策略迭代总是会产生相同的解决方案？是的？不，有人认为什么——嗯，为什么不是。给我一个他们可能考虑不到的例子。[噪音]是的。口头[听不清]。没错，是的。所以说它是正确的。你-你会得到相同的，嗯，价值函数。所以这取决于你想用哪种方式来回答这个问题。它们将具有相同的价值，但它们可能会有不同的策略。如果有多个策略具有相同的最佳[NOISE]值，则这是可能的。这种情况可能会出现，因为通常会有多种政策有助于实现最优值。

在大多数情况下，策略迭代所需的迭代次数是有限的。这是因为在表格MDP（马尔可夫决策过程）中，策略总数是有限的。通过类似于策略改进的方式在表格MDP中进行操作，您可以确保得到单调改进。这意味着您最多只需浏览每个策略一次，即可完成整个过程。因此，您可以得到保证的策略改进，因为这种情况下没有函数逼近，没有误差，您确切地知道当前值是多少，可以按照单调改进的步骤进行操作。

现在我们简要回顾一下无模型政策评估。无模型政策评估是一种被动的强化学习方式，我们只是想了解当前政策的表现有多好。在理想情况下，我们希望数据量不要太大。

在这里，我们探讨直接估计 Q 函数或策略的价值函数。在这种情况下，我们主要关注的是情景域。所谓情景域，指的是我们在世界中采取固定数量的步骤，或者处于一个已知会达到终止状态的环境中，因此我们知道情节会结束。在这种情况下，结束的概率是1，它们必然会结束。接着，我们会被重置到具有特定分布的起始状态。

在蒙特卡罗方法中，我们会直接对情景奖励进行平均，这很简单。我们采用当前策略，执行一定数量的步骤（记为H），或者直到情节结束。然后我们会重置并重复多次，最后取平均值。而在TD学习或Q学习中，我们使用目标来引导学习。虽然你们可能已经看过很多次了，但只是为了复习，我希望用这些图表来帮助大家理解这些区别。因此，在我们讨论动态规划时，我们考虑了已知转换模型和奖励模型的情况。因此，当我们思考策略的价值时，它恰好等于我们...

在实施这一策略时，我们会考虑到奖励值以及乘以伽马后的下一个状态的值。需要注意的是，在计算预期值时，我们考虑到了所有可能到达的状态。这意味着我们会综合考虑接下来可能遇到的所有状态，并在动态规划中明确处理这些情况。动态规划的过程中，我们会综合考虑所有可能到达的状态以及它们的价值。因此，当我们从一个状态开始，执行一个动作，然后转移到下一个新状态时，我们会重复这个过程，直到达到水平线或终端状态。我们关注的是对下一个状态的期望值，在动态规划中我们进行的是程序的引导。这里所谓的引导是指，我们并非构建整个树结构，而是对可能的状态进行明智的指导。

使用动态规划时，我们会跟踪所有状态的值，并利用它们来明确预期下一个状态的价值，并对未来状态值进行平均。在这种情况下，我们假设已知模型。虽然有一些方法可以扩展到未知模型的情况，但我们暂时不深入探讨这些方法。所以当提到动态规划时，指的是我们已知世界模型的情况。在此情况下，我们使用值函数V来进行更新，因为V是经验估计。这些值并不是对未来状态真实预期折扣奖励的完美估计，因为我们仍在计算它们。接着我们讨论了蒙特卡罗政策评估，它在许多方面与动态规划类似，不同之处在于我们沿着完整轨迹进行累加所有奖励，这是我们的目标。当提到政策评估时，蒙特卡罗评估是指对回报进行抽样。

我们的近似期望是对s素数概率的期望值进行估算。这意味着我们只考虑了一个s素数的情况，而不是考虑所有接下来的素数的期望值。尽管这种估算是无偏的，但我们称之为高方差。我们还讨论了如何将这些概念与时间差分方法结合起来，在这个过程中我们会进行引导和采样。我们进行采样是因为我们只关注单个未来状态，同时我们进行引导是因为我们引入了值函数的估算。与蒙特卡罗方法不同，我们对单个状态s进行采样，t+1，并进行引导，因为我们没有像蒙特卡罗那样一直进行模拟，蒙特卡罗方法中我们会利用当前值函数的估算。因此，对于每种情况，最好确定是否适用动态规划，这需要对模型有充分的了解。

嗯，关于蒙特卡洛或 TD 学习。那么当我们不了解当前领域的模型时，这些方法是否适用呢？它们是否适用于处理连续的非马尔可夫领域？能否处理非马尔可夫领域？嗯，让我简单解释一下我的意思。实际上，你可以将任何算法应用于任何情境，但结果可能并不理想。所以我的问题是，当我提到处理非马尔可夫领域时，这些方法是否能够取得好的效果，或者它们是否会默认按照马尔可夫假设进行操作？它们是否能够收敛到策略的真实值并且受到更新限制？现在，我们考虑的是一个表格案例。在这种情况下，价值函数是可以被完全表示的，那么这些方法是否能够给出价值的无偏估计？尽管这些估计可能是一致的，这意味着最终使用更新程序它们会收敛到正确的结果，但它们可能会在估计值上存在一些偏差。再次强调一遍，花一两分钟思考后，每个问题的答案并不是非黑即白。对于每个人来说，答案可能会有所不同。你可以随时与身边的人讨论这些问题。

不同质量的收敛到重叠的结果。但我们可以从背景开始说起。是的，这是一个很好的问题。政策迭代可以解决好问题。背景如下：我要请求大家再次投票。好的，如果答案是肯定的，请举手。所以，当没有模型可用于当前领域时，动态规划是否可行？不可以。蒙特卡罗方法可以吗？可以。时序差分可以吗？很好。那么，动态规划能否处理连续非情境领域？如果可以，请举手。正确，是的。所以，您可以使用动态规划。即使针对无限水平领域，您也可以使用贝尔曼算子和缩减。通常希望γ函数小于1，这样您的价值就不会爆炸。

嗯，没问题，你可以做到。对了。蒙特卡洛估计怎么样？不行。蒙特卡洛只在每一集结束时更新。TD估计呢？没错。很棒。嗯，动态规划（DP）能处理非马尔可夫域吗？不，蒙特卡洛呢？对的。对的。TD呢？不行。同样，你可以在任何你想要的地方运行所有这些，但是- 收敛到策略的真实值和DP更新的局限呢？对的。对的，蒙特卡洛呢？对的。对的。TD呢？对的。对的。关于值的无偏估计，DP有点不适用，因为我们并没有真实使用数据。这有点不同。嗯，蒙特卡洛是一个对值的公正估计。对的。对的，TD呢？不，太好了。好的。所以，嗯，如果我们在考试中问你这个问题，我们一定会澄清我们是在谈论表格设置还是函数逼近设置，其中一切可能都非常不同。对的？你能准确解释为什么TD不适用于非马尔可夫吗？对的。所以，对的，这是一件好事。

为什么时序差分不适用于马尔可夫决策过程呢？这是因为它基于马尔可夫性质进行了假设。这就是问题所在。因此，它在构建值函数时假定，当前状态的预期奖励贴现总和正好等于当前奖励加上每个下一个状态的未来奖励贴现总和，其仅基于下一个状态St + 1。所以这就是你假设马尔可夫性质的地方，因为你的假设是-如果，如果你有一个带别名的观察空间，它将忽略整个历史。而蒙特卡罗算法则从当前状态开始总结所有奖励。好问题。这是否假设马尔可夫过程？好问题，请提醒我你的名字。我们讨论的几乎都是TD(0)，其中我们只考虑奖励加上折扣因子γ乘以值函数，但我们也简要提及了N步骤。你可以计算r1 + r2 + γ乘以r2等等。

就 n 步的情况而言，你会得到以下内容：rt + gamma rt + 1 + gamma 平方 rt + 2 + gamma 立方 V st + 3。这就是 n 步的概念。这实际上是对马尔科夫假设提出了不同的观点。因为你可以有连续的状态，也可以有完全非马尔科夫的情况，或者类似 n 步马尔科夫的情况。这意味着，如果想要成功，就需要记录一定数量的历史信息。所以，举一个类似之前见过的例子，我们可以考虑类似随机游走的情形。假设我们有一个包含三个状态和两个终止状态的领域。我们总是从状态 B 开始，然后以 50% 的概率向左或向右移动。当到达终止状态时，过程终止。在那里，你可能会得到+1或者得到0。

这是一个等概率的随机游走，在到达最终状态之前不断进行。因此，我们可以尝试计算状态的真实值。在这种情况下，状态的真实值，需要考虑你将访问的状态的分布。例如，如果我们考虑状态的价值，我会这样做。对于状态C的值，它总是等于立即奖励加上乘以伽玛的下一个状态的值的总和，即状态S的值。让我们将其称为SD，将其视为S0。所以SD的值总是等于1。因此，SD的V等于1，因为你获得奖励然后游戏结束。

这意味着，当 γ 乘以 ½ 的概率时，您将得到 SB 的值，再加上 ½ 你得到 1。 最终，观察这个分布，您可以对不同的状态都执行相同的过程。通过执行此操作，您会发现在随机游走中以概率分布的形式停止在右侧或左侧，进而计算其值。在考试中，我们可能会简化这个过程，但是理解并完成这个例子非常有帮助，可以了解在价值函数方面会发生什么。接下来一个问题是，让我们想象有一条特定的轨迹，我们想比较在不同算法下会发生什么。所以，想象一下，我们有一条轨迹，从 B 到 C，C 终止后值加 1。这就是我们的情节。那么首次访问蒙特卡罗估计在 B 处是多少？

在蒙特卡洛中，我们通过将从状态获得的所有奖励加起来来估计该状态的值。如果我们第一次访问该状态时获得的奖励是1，那么该状态的估计值就是1。如果我们正在进行滑动平均以更新蒙特卡洛估计，我们需要知道初始值和滑动平均系数alpha。但在这种情况下，我们只需要考虑确切的回报，即从状态B开始到剧集结束的返回。接下来是关于TD学习的问题。给定数据BC0和CB0以及学习率A，TD学习的更新是什么呢？也许只需要一两分钟，然后执行一两个更新。

接着，想象一下如果我们以相同的学习率颠倒数据顺序会发生什么。这涉及到我们之前多次讨论过的一个点，即我们计算值时执行更新的顺序是否会影响结果。所以我认为我会以一种方式来处理这个问题。首先，我会假设数据顺序对于我们要计算的值是否重要，然后尝试计算其中一两个情况。让我们花一两分钟来决定这里的顺序对于结果值的重要性，然后再进行计算。我知道我没有给你们足够的时间来完成这里的所有计算。

这段内容讨论了在计算中顺序对于记住公式和值很重要的问题。作者强调，在某些情况下，顺序会影响计算的结果，但并不总是如此。在特定情况下，观察到一阶的值，然后根据公式计算得到更新的结果。在这种情况下，假设初始值为0是很重要的。未来将会进一步讨论这个话题。

更明智的探索，以及乐观经常确实非常有帮助的事实。 深度神经网络面临的一项挑战是如何设置事物以使它们保持乐观。 在这里，我们会假定一切都是0，这样B的值将是alpha的平方，C的值将是一个特定的表达式。 这是我将强化学习中的时间差分学习应用到这些案例中的基本原理。他们在第二行，接着下去。在哪里？应该是伽马的平方。是的，对的，最后的表达式。谢谢。所以就在第三行了。如果我们以相反的顺序进行，在第一次更新时，C的值将会是0，因为C转移到B，B的值为0。然后，当我们更新BC0时，C的值仍然是0，我们只更新最后一个C的值。

因此，这强调了顺序的重要性。 当我们进行函数逼近和情景重播时，也会遇到这种情况。 一般来说，在考虑策略评估算法时，了解偏差-方差权衡、数据效率和计算效率是很有益的。 在计算效率方面，TD学习通常很好，但在数据效率方面，就要看情况了。 有时，通过体验TD的重放，情况会有所改善。 因此，考虑到这些算法通常有多种变体是很有帮助的。 无论情况如何，都要保持准确性。 如果我们假设正在使用普通版本，或者如果愿意，通过额外的体验重播，可以改变情况。 现在，让我们考虑如何进行无模型学习以做出正确的决策。 我们已经讨论了很多关于Q学习的问题。 Q学习是一种基于马尔可夫假设的引导技术。 所谓的Q值是由奖励加上γ乘以状态的最大下一个Q函数来近似。 我们可以将其用作我们的目标，

接下来我们慢慢开始旋转。我们会使用更新的学习率，这个学习率会随着时间推移而逐渐减小，从一个样本到我们之前的估计中进行转变。通常我们逐渐减小 α，尝试将 Q 收敛到一个单一值。我们已经讨论了 Q 学习收敛的一些条件，这都是在可达性假设下进行的，也是我们目前正在讨论的表格设置。因此没有使用函数近似。如果你随机采取行动，Q 学习将在温和的可达性假设下收敛到 Q 星，这意味着，你不能像摧毁直升机那样，做出一个错误决定就导致世界灭亡，你不会获得更多的样本。因此，你必须能够无限次地重复访问所有状态，并尝试无限次所有操作。它有一个有趣的属性，当你使用一种策略收集数据时，你可以估计另一种策略的价值。

因此，这就是我们尝试估计最佳 Q 函数的地方，但是我们可以使用例如随机数据、随机样本、噪声或随机策略来尝试估计。原因是我们一直在努力找出下一步最佳行动是什么。因此，这是一个非常有趣的属性。如果我们考虑这种情况，那么有一些不同的情况，我们可以简要讨论一下。如果你有一个 Q 学习策略，它采用 e-greedy 方法，这里 e-greedy 的概率是 1 - epsilon。您会选择在当前 Q 函数下预期最佳的行动。在概率为 epsilon 的情况下，你会随机进行行动。因此，如果您在查找表中，您可以确保收敛到最优策略并且具有无限数据的限制。因此，会有一些轻微的噪声。对于第二个问题，我们是否可以使用蒙特卡洛估计以及具有大状态空间的马尔可夫决策过程(MDP)？

如果可以的话，我们来进行投票吧。无论如何，我会花一些时间和您的邻居交谈，然后我们再次进行投票。我不是在说这些人错了，我只是认为，由于大多数人没有投票，我假设大多数人只要思考一下就会受益。好的。

让我们重新进行投票吧。如果您认为在具有大状态空间的MDP中可以使用蒙特卡罗估计，请投票。是的。是的。实际上，并不受限于状态空间的大小，您可以使用蒙特卡罗估计。这个问题可能有多种答案，具体取决于情况。是的，这是一个很有意思的问题。在蒙特卡罗方法中，每个状态可能的数据点数量非常有限。如果您只有一个起始状态，那很好。但如果您有起始状态的分布，那可能会更加困难，或者需要使用值函数的近似方法。没有任何先验知识的情况下，您将无法应用这种方法。尝试在这种情况下使用它可能效果适得其反。也许我们需要开始考虑使用函数逼近方法。最后，这只是我放在那里的最后一点思考，我们还没有详细讨论，但我认为这是一个有趣的起点，可以开始连接我们之前讨论过的动态规划方面。

嗯，基于模型的强化学习并不一定总是比无模型更高效，尽管我们主要讨论的是无模型。所以我们还没有对此进行太多讨论，但是，在我们开始探索时，思考这一点是一件好事。之前简单提到过，文研究员Sun和微软研究院的一些同事发表了一篇不错的新论文，该论文表明在某些情况下，基于模型绝对比无模型更好。直觉是你可以紧凑地表示模型，但不能紧凑地表示价值函数。因此，你不需要很多参数来学习模型，然后可以用它进行规划，但如果尝试直接学习价值函数，就需要更多参数。所以，当我们在讨论中开始进入偶数时，最近很多重点都集中在价值函数的近似上。这部分内容也包含在作业2中。因此，我们讨论了如果您正在研究蒙特卡罗方法与TD学习，在政策评估中我们会有什么样的收敛保证？所以，强调这一点是很重要的。

因此，我们现在考虑评估单个政策的价值，并且讨论了如何考虑政策的固定分配。在定义一个单一的策略时，我们引入了马尔可夫奖励过程或马尔可夫链的概念，并且考虑到根据该政策进行访问的状态的平稳分布。我们讨论了收敛特性，特别是提到了蒙特卡洛方法, 无论使用何种函数逼近器，它都会试图最小化均方误差。因此，我们谈到了蒙特卡罗方法的这种技术风格，它的目标只是尝试最小化数据的均方误差。我们可以考虑线性值函数逼近器不应该采用这种方式，这也适用于其他值函数逼近器，因为它们也会最小化误差。在使用TD学习器学习的线性值函数逼近器的情况下，它将以常数因子收敛到最佳均方误差。在这种情况下，这意味着什么呢？

在这种情况下，我们可能会面临一个挑战，尤其是当使用诸如线性值函数逼近器之类的工具时，可能无法精确表示所有状态的值，因为我们只能选择一组参数来表示我们拥有的空间。因此，从根本上讲，以您拥有的空间表示的价值函数与真实价值函数之间可能会存在差距。我通常会用一个比喻来理解这一点：就好像有一幅美丽的照片，但是缺少了其中的配对人物。换句话说，您可以通过您拥有的参数集来展示您可以表示的价值函数，但您可能无法用一条线来准确表示所有真值函数。举个例子，如果您在二维空间中思考这个问题，您可能会想象对于某个状态，您的真实值函数可能呈现如何，但如果您使用的是线性逼近器，那么您可能无法精确表示它，因为它不会正好是一条直线。因此，蒙特卡洛方法的收敛可能会受到最佳均方误差的影响。

值函数逼近器的空间提供值，并且TD学习器会收敛到该乘以一个附加因子的倍数。明白了。我觉得不太可能喜欢这种情况。请注意这一点。好的。现在我要让它不再这样操作了。我们之前讨论过这样一个事实：当你在进行策略学习时，带有函数逼近的Q学习可能会发散，这意味着它甚至无法通过无限数据达到收敛。这也可能意味着，即使它最终收敛，收敛的结果也可能会有所不同。

对不起，这段内容比较混乱，我会对其进行整理：

他们在讨论梯度更新时指出，实际参数可能永远在变化。他们提到，是否可以以某种方式初始化函数逼近器中的参数来推动收敛，但不能保证它会奏效。讨论中提到参数的初始化是否有助于确定收敛或发散的可能性。他们认为这是一个有趣的问题，目前似乎没有正式的工作可以明确描述这一点。他们讨论是否能够避免梯度爆炸的情况，并认为这在很大程度上取决于具体问题。他们还提到可能存在一些病态的例子，构建这些例子可能是困难的，但也值得一试。他们建议观察参数估计值是否持续变化，以进一步了解问题的性质。

他们进行了许多关于深度学习和无模型的 Q 学习的讨论，研究了 Q 学习目标和 Q 网络，同时使用随机梯度下降和深度神经网络来逼近 Q 值。

整体来看，他们在讨论如何通过初始化参数、观察参数变化等方式，来解决梯度更新可能导致的问题。

在解决这些分歧时，我们可能会面临一些挑战，其中之一是本地更新的存在。一个状态的价值通常与其下一个后继状态的价值密切相关。然而，这些目标经常会发生变化，这可能导致不稳定性。因此，在过去五年中，对这个问题取得了许多进展，主要是通过修改方程的方式，以使其在梯度下降时更加稳定。

在深度 Q 网络（DQN）中，我们应该使用经验回放，即不要仅使用每个数据点一次，并且需要定期固定目标。这样我们可以通过固定值函数近似一段时间后的下一个状态，然后最小化均方误差来训练模型。经验回放在这方面特别有帮助，并且产生了相当大的效果。

尽管在这个领域中正在进行许多有趣的工作，但在收敛方面目前还没有很好的保证。人们对理解此类网络的形式属性非常感兴趣。我们还讨论了双 Q 网络，

在AI领域的讨论中，关于决斗以及优先重放等方法，可以帮助加速Q函数收敛到合理值。这也是我们今天要讨论的最后一个话题了。在有限状态空间中，特征可以表示真值函数。在给定足够数据的情况下，使用值函数近似的TD学习是否总能找到真实策略价值函数呢？这个问题主要是关于TD学习和政策评估。在这种情况下，我们能否保证在足够的数据下找到真值函数呢？让我们暂时中断一下，和大家交流一下，然后进行投票。我们应该假设这是关于政策的问题，或者至少是关于来自政策分布的数据。

谁愿意投票赞成我们确实找到了真值函数逼近器？ 这是正确的。好的，那么我们如何检查这一点呢？如果我们回到我在这里所说的，我所说的是，我们将收敛到最佳均方误差的常数因子。如果您可以精确地表示当前空间中的值，那么该均方误差始终为0。因此，附加的常量因子只是常量因子乘以0。所以在这种情况下，是的。因此我-因为我在这里说它-拥有可以代表真实价值函数的特征。所以我们已经说过，完全有可能在提供给您的特征中表示该策略的价值函数，因此有可能实现这一点。是的。对于非线性参数化来说是这样吗？用于TD学习？是的。因此，对于政策评估，如果你有一个非线性——如果你有特征——就像你有一个通用的表示，可以让你准确地

假设你正在研究政策学习，并且要进行TD学习，以估计价值函数。在这个过程中，你可能会遇到有限数据量的情况。即使你拥有足够的特征来表示价值函数，也可能找不到一个能够准确代表真实价值函数的函数空间。换句话说，即使我们有所有必要的特征，也不一定能够找到一个能够准确表示价值函数的函数空间。

这个问题的关键在于，即使我们拥有很多特征，我们也需要确认我们选择的参数化函数类能够精确表示价值函数。换句话说，我们需要一个算法来优化这个参数化函数类，使其能够很好地拟合真实的价值函数。这样，假设我们有一个能够很好地拟合真实价值函数的价值函数，我们就可以更好地进行政策学习。

一个预言机具有这些特征，那么参数向量将使得，嗯，那个零，那个TD学习可以找到。所以从本质上讲就类似于[听不清]，因为我们可以生成一个表格并保存它，这个表格不一定必须是实际的表格。因此，回顾一下，这种情况并不一定只适用于表格情况。这意味着，如果我们观察这里的一些内容，让我们设想这是您的状态空间，这是您的价值函数。因此，如果有人给出一条线、一个二次方程或者一个深度神经网络具有足够参数来准确表示这条线的情况，那么这个说法的意思是TD学习可以找到 - 可以准确适应这些参数。当我们开始进行Q学习时，情况就不同了。因此，在某些情况下，您可能会有最佳形式来表示价值函数，但您无法找到它，像是Q学习无法识别它。这就是我们试图在这里做的变化，即在TD学习中，如果存在这种情况并且您遵循政策，您就可以找到它，而Q学习可能无法。是的。接下来有一个问题，请告诉我您的名字，这样我就能澄清一下，值的逼近是线性还是非线性？是的。是的。所以我们正在尝试什么 - 对于通用表示来说，这是正确的。

如果你使用线性形式、表格形式或其他方式表示数值，通常会假设这些数值是准确的。所以，不管是线性形式还是其他形式，这一点都是成立的。当涉及到我们的值函数逼近器是否是一个契约算子时，这个问题与我们的值函数逼近器是否具有收缩性有关。当我们进行TD学习时，可以想象我们有两个步骤：一是近似我们的贝尔曼方程，二是关于我们的贝尔曼算子，如果我们能够完全准确地应用它，我们知道这是一个收缩算子，这意味着我们需要额外的步骤来逼近函数。如果我们能够完全拟合函数，那么就不会引入额外的误差。这就是其中一个优点。但是，一旦我们转移到不同的政策，情况就会变得更加复杂。在这种情况下，一切都取决于政策，因此更接近于监督学习设置。当你开始离开这个政策时，问题就变得更加复杂了。

好的，让我们简单回顾一下模仿学习、以及政策搜索。这将是一个比较基础的内容，但考试时你可能会遇到。你没有机会进行过练习，除了在讲座中听过之外。模仿学习的概念在于奖励函数的规范可能非常复杂。如果我们可以让人类展示程序，然后从中学习呢？行为克隆是我们进行监督学习的方法。因此，我们尝试学习动作和状态之间的映射，并将其视为监督学习问题。我们只需要找到专家的状态和动作对，然后尝试使用你最喜欢的监督机器学习方法（比如分类算法）来预测。在这种情况下，可能出现的问题是，你在试图模仿专家时会诱导出一个不同的状态分布，而不是专家的状态分布。这意味着最终可能会得到不同的状态分布，而不是专家的状态分布。

在这些新的情形下，由于缺乏相关数据，您应该考虑采取什么行动。在某些情况下，情况可能会变得非常糟糕。我们正在讨论模仿学习，其理念是重新获得示范的轨迹。现在的目标是直接学习奖励。这里一个值得重新思考的好处是有多少奖励函数与专家的示范兼容。我们之前也讨论过这个问题。如果您有任何疑问，请随时在课程结束时或在广场上与我联系。

我们正在探讨政策搜索，所以只是简单介绍一下。我希望您对这些问题有所了解。那么，为什么我们要使用随机参数化策略呢？将其纳入领域知识可能是一个不错的方法。这有助于我们处理非马尔可夫结构。我们也讨论了混合策略，并探讨了博弈论设置，其中确定性策略表现不佳，而随机策略表现良好。嗯，策略梯度方法并非策略搜索的唯一形式。我们还讨论了我的同事Steve Collins关于外骨骼优化的内容。

实际上，效果非常好。 但一般来说，我们将主要讨论梯度。 似然比策略梯度方法不需要我们拥有动态模型，这点非常重要，因为当我们没有动态模型时，减少策略梯度估计器方差的两个想法是使用时间结构。 在这里，涉及到这样一个事实：由于时间的结构，你现在在某个时间步获得的奖励不会受到你未来决策的影响。 第二个基线，这是我们在课堂上探讨的水平，但不涉及深入的程序知识。 所以只是总结一下。 建议是仔细阅读讲义，看看事情，比如检查你的理解情况。 如果您想查看现有的对话中的其他示例，注释可能会很有用。 特别是去年的期中考试的做法和两年前更加相似。 如果你看到一些在本课程中未涉及的主题，期中考试不会涵盖它，但是如果你有任何问题，请随时与我们联系。

今天我们要讨论的是快速强化学习。在我们班级中目前的进展是，我们刚刚完成了政策搜索的阶段，正在研究政策梯度，而现在正忙于作业。接下来将是最后一次作业，然后我们将有时间进行项目。现在我们要开始探讨快速强化学习，这是我们还没有深入讨论过的主题。迄今为止，我们已经讨论了很多与此相关的概念，比如优化、泛化和延迟奖励。接下来我们将讨论如何进行规划和马尔可夫决策过程，以及如何利用深度神经网络来扩展到非常庞大的状态空间。这种优化如何实现呢？对于那些有良好模拟器或成本低廉数据的情况来说，这种方法非常有效。在我的实验室里，我一直在思考如何教计算机为我们提供帮助，这自然而然地涉及到强化学习，因为我们正在教计算机如何做出对我们有利的决策。我相信在许多其他领域也有着广泛的应用。

我们期望计算机能够帮助我们，比如在教育、医疗保健或消费者营销等领域。在每种情况下，都可以将这看作是强化学习问题，因为我们有类似计算机这样的代理与人交互并做出决策，试图优化某些奖励，比如帮助学习、治疗病人或增加公司收入通过点击广告。在所有这些情况下，数据源都是人。这里至少存在两个主要挑战。首先，人是有限的。数据无法无限获取，而且尝试与人互动收集数据是昂贵的。因此，引起了样本效率的担忧。總的來說，

我们当然希望有高效且样本利用率高的强化学习算法。然而，迄今为止，我们研究的大多数技术，特别是 Q 学习类技术，其实都是受到对计算效率需求的启发。回想一下，我们最初讨论动态规划和 Q 学习时，在动态规划中，我们必须对所有后续状态进行求和，而在时序差异学习（TD 学习）中，我们对其进行采样。因此，在 TD 学习中，我们每次更新的成本是固定的，而动态规划的成本是状态数量乘以动作数量再乘以成本。所以做动态规划等工作的成本要比进行 TD 学习要高得多，基于每一步。因此，强化学习中许多技术的发展实际上都在考虑到这个计算效率的问题。在许多情况下，计算效率至关重要。就像，如果你想要从零开始进行规划，并且以每小时60英里的速度驾驶汽车，那么如果这需要你——。

当你以每小时60英里的速度驾驶汽车时，如果你的计算机需要一秒钟来做出决定，比如转动方向盘等行为，那么在这一秒钟内，你已经行驶了很多英尺的距离。因此，在许多情况下，你会受到实时计算的限制。特别是在一些机器人技术中，例如当我们使用模拟器时，我们需要高效的计算能力，因为我们需要快速完成任务。我们可以依靠模拟器，但它需要足够快，以确保我们的智能体可以及时学习。与此相比，样本效率则变得非常关键——收集样本可能很昂贵，而计算则相对不那么重要。因此，在经验成本高或样本难以获取时，计算效率就显得尤为重要。这尤其涉及到人的事务，比如学生、患者或客户。

就像我们的 AI 智能体了解世界的方式是通过做出决策，数据对于真实世界会产生影响。 因此，如果我们能够找到更好的治疗癌症方法，那么进行几天的计算是非常合理的。因为我们不希望在人身上进行随机实验，我们希望利用数据来实现真正的样本有效性。相比于 Atari 的情况，我们寻求真正的计算有效性，因为我们可以进行大量的模拟，而无需担心人身伤害。但最终，我们需要学会制作出一款优秀的游戏。因此，一个自然的问题可能是，我们现在关心的是样本有效性。 或许以前我们更关注计算效率，但也许我们已经讨论过的算法已经具有样本高效性。那么，是否有人还记得类似的数量级，或者在某个范围内，类似于DQN为Pong学习一个良好策略需要多少步？也许有人会提出多种答案，但我认为大致在2到10之间变化。

我估计的是，需要2到1000万个数据点。所以，学习打乒乓球需要大量的数据[笑]。就目前看到的技术而言，我认为它们无法解决这个问题。在我们找到一个好的解决方案之前，需要2到1000万个数据点是不合理的。在做出正确决定之前，需要首先确定如何定位广告或2到1000万个患者。因此，目前我们所见的技术在样本效率和经验上都有局限，需要新型技术的出现。当我们开始思考这个问题时，会涉及到通用性问题。好的算法意味着什么？我们讨论了计算效率，也提到了样本效率。总的来说，我们非常关心的是我们的强化学习算法的表现如何，我们将开始尝试根据样本效率来量化这一点。当然，拥有真正的样本有效算法可能会导致一种情况，即算法只使用前10个数据点，然后就不再更新其策略。因此，虽然只需少量数据即可找到策略，但这个策略可能是糟糕的。因此，讨论样本效率时要谨慎。

我们既想要零样本学习，又想要少样本学习，我们希望能够在不需要大量数据的情况下做出准确的决策。因此，我们仍然希望模型表现良好，但却不需要非常丰富的经验数据。当我们谈论一个优秀的算法时，可能意味着我们关注它的收敛性。这意味着在某个时间点，价值函数或策略是否稳定，比如渐近稳定，随着时间步数趋于无穷大。有时，我们讨论价值函数的近似，我们甚至可能会遇到振荡的情况。另一个更强大的方面是，随着时间趋于无穷大，我们是否会收敛到最优策略？在不同的假设下，有一些算法可以实现这一点。但我们还没有讨论过，到达最优策略需要多长时间。渐近性是一个漫长的过程。因此，如果我们有两种算法，其中一种能够在这个阶段获得最佳策略，

在比较这两种算法的性能时，一种是这样的，另一种是那样的，直观地讲，算法二比算法一更好，即使它们最终都能达到最优策略。因此，我们希望能够对它们中的任何一个进行解释，我们可以考虑诸如算法犯了多少错误，或者相对于最佳算法随着时间推移的性能表现等指标。因此，我们今天将开始讨论一些其他衡量算法优劣的指标。在接下来的几节课中，我们将尝试做一些不同的事情，讨论这些强化学习算法的表现如何，并考虑哪些算法在性能方面有待改进。我们要开始讨论表格设置，但今天我们只谈谈简单的情况。总的来说，今天以及下次的课程中，我们将探讨表格设置，并希望也能了解一些关于函数逼近和样本效率的内容。接下来，我们将开始讨论一种设置框架和方法。

所以，今天和下次我们要介绍的内容将会涉及强盗算法，也就是您们中的一些人——谁、谁、谁在进行默认项目？好的。许多人中已经开始从项目的视角思考这个问题。今天我们会介绍 bandits 算法，然后会继续讲解 MDP（马尔可夫决策过程）。接着，我们会介绍框架，用于正式评估强化学习算法的质量。这种工具能够评估许多不同的算法，要么符合这一框架，要么不符合，或者在不同框架下具有不同特性。然后，我们会讨论一些方法，这些方法是不同类别的算法，用于在不同设置下（MDP 设置或 Bandit 设置）实现这些不同评估标准的框架。很快我们会发现，有一些关于算法风格或方法的主要思想，它们被证明适用于强盗算法和马尔可夫决策过程，实际上还包括函数逼近。

而且还具有一些非常好的正式属性。关于我们如何进行快速强化学习，有一些重要的概念性想法。今天的计划是，我们首先从介绍多臂老虎机开始，然后将讨论数学形式上的遗憾的定义。接着，我们会谈谈在不确定性下的乐观方法，再然后，我们尽可能详细讨论贝叶斯遗憾以及概率匹配与汤普森采样。我很好奇，这里谁见过这些内容呢？有几个人看过，但大多数人没有。这和AI有关吗？我觉得不会，哦，好的，好的。所以对于你们中的一些人来说，这将是一篇评论，而对大多数人来说，这将是新的。因此，对于多臂老虎机，我们可以将它们看作是强化学习的一个子集。它通常被理解为一组手臂，有m个手臂，相当于我们在强化学习中提到的动作。

在我们的实验中，我们正在考虑有m种不同的行动。通常我们将这些行动称为"手臂"。对于每个手臂，都会有对应的奖励。在讨论中并没有涉及奖励的不确定性，我们只关注预期回报。对于多臂老虎机这一概念，今天我们要明确的一点是，奖励可能是从随机分布中抽样的。这意味着我们并不知道每个手臂的奖励分布。根据手臂的不同，你会获得不同的奖励。举例来说，对于第1个手臂，奖励分布可能是这样的；这是奖励的概率及相应的奖励值。而对于第2个手臂，则可能是另一种分布。在这个特定案例中，第1组的平均奖励将高于第2组的平均奖励，并且它们的方差也会不同。

在多臂老虎机问题中，奖励并不一定遵循高斯分布，可能有多种不同的分布。在这个问题中，每次选择一个动作（也称为拉动手臂），即使重复拉同一个手臂两次，结果也可能不同。因此，在这种情况下，每个动作都可以被看作是一个状态，类似于马尔可夫决策过程（MDP），但没有转移函数。因此，可以将其视为只有一个状态。每次拉动手臂都会保持相同的状态，可以选择执行m个操作中的一个，并且每一步都会观察从与该手臂相关的未知概率分布中采样得到的奖励。类似于强化学习，事先并不知道这些奖励的分布，目标是最大化累积奖励。 如果有人事先告诉您这些分布是什么，

在这里，我们将讨论强化学习案例中的一些概念，通过使用符号来更清晰地描述。我们将行动价值定义为特定行动的平均奖励，这被称为 Q 值，对于代理来说，某些 Q 值是未知的。最优值 V* 将等于最佳动作的 Q 值。在这种情况下，如果您选择的不是最佳动作，那么就会错失一步的机会，我们称之为“后悔”。后悔量描述了在采取次优行动后可能损失的预期。

如果你选择了最佳的动作，那么你在该时间步的遗憾就是零，但选择其他动作时都会有一定的损失。通过将智能体在所有时间步上的行动进行相加，并将每次行动的预期奖励与最优行动的预期奖励进行比较，就可以得到总遗憾，即错失最佳行动所带来的机会损失。需要澄清的是，这是未知的，代理人不知道这一点。这也是未知的。为何代理不知道第二个问题？因为你不知道奖励的概率分布。对。所以，你不知道... 对，正确。 所以，你无法了解奖励分布是什么，因此也无法准确了解奖励。你可以观察并从中获取样本。因此，你可以观察奖励。你可以获得一个奖励，这是从采取特定动作后的奖励概率分布中采样得到的。

[噪音] 但是由于我们无法观察到最佳手臂的真实期望值，也无法观察到我们选择手臂的真实期望值，所以除非我们处于模拟环境中，否则这通常是无法评估的。但我们将讨论限制这一点的方法，并考虑采用算法或尝试最大程度减少遗憾的方式。因此，如果我们考虑量化这一点的方法，另一种考虑方法是考虑您采取特定行动的次数。我们可以称之为动作次数Nt。这就像我们选择动作1、动作2等的次数。然后，我们可以定义一个间隔，即[噪音]最佳手臂值与我们选择的手臂值之间的差异，这就是间隔。这代表我们由于选择不同于最佳手臂的手臂而产生的损失。因此，这种差距等于，如果选择最佳手臂则不会有任何损失。

对于每种手臂，只有正数才是相关的。因此，等效后悔的另一种方式是考虑选择每个手臂的预期次数乘以它们的差距。那么，相对于最佳手臂，选择该手臂会造成多大损失呢？我们需要的是一种算法，它能够调整选择拉动差距较大手臂的次数。如果有一个非常糟糕的手臂，就像执行一个很糟糕的动作会得到很低的奖励，你就不希望频繁选择它，而是倾向于选择能接近最佳手臂的动作。在之前我们接触过的算法中，贪婪算法就是一种简单的选择方式。在强盗问题中，贪婪算法非常简单。我们只是对每个手臂的奖励进行平均。因此，只需要观察每次选择手臂的时间，记录每个时间戳获得的奖励，并计算平均值。这只是给出了我们对Q值的估计。

贪婪算法会选择具有最高奖励值的动作，并永远执行该动作。在某些情况下，由于奖励是从随机分布中抽样的，如果不幸抽样结果不具有代表性，可能会导致选择错误的动作而陷入困境。举个例子，假设有两个选择（a1和a2），奖励分别为1和0。如果从a1中采样，可能得到一个非零的样本概率为0.2，而从a2中采样可能是0.5。因此，a2的真实平均值会低于a1。这说明仅仅从其中一个动作中采样一次可能不能准确估计其真实价值。

如果你贪婪地行事，就会一直走向错误的方向。这是我们的政策会影响未来决策的观念，还是存在一些不受贪婪政策影响的样本集？因为看起来，如果有非零奖励，你就会一直追求这个奖励。嗯，这是一个很有意义的问题。对于很多算法而言，通常假设如果你有一组有限的“手臂”，那么所有算法都会通过至少选择每个“手臂”一次来运行。同样地，你可以说，如果没有任何数据，你会以平等的方式对待所有情况，或者，但基本上，大多数人会认为，直到获取了所有“手臂”的数据，我们才会像圆形罗宾一样，对所有东西进行采样。在这之后，你可以选择贪婪行事，或者尝试其他方法。所以，必须有一个预先设定的空间。这是一个很好的问题。现在我们也假设以相同的概率进行分裂。

因此，如果两个臂具有相同的概率，并且它们都具有最大的action值，那么你会在这两个臂之间分配时间，直到这个值发生变化。 换句话说，在这个例子中，如果我们首先从a1中采样一次，然后从a2中采样一次，由于这些样本有非零的概率使得动作a1的平均值低于动作a2，那么你可能会永远选择错误的动作锁定。现在，e-greedy算法，就像我们之前在课堂上学过的那样，做的事情很类似，除了以概率1-epsilon选择贪婪动作，否则以epsilon的概率在所有其他动作或臂上进行选择。因此，在这些情况下，我们拥有更强的鲁棒性。 因此，在这种情况下，我们会继续采样其他动作，但我们总是会做出次优的决定，至少在epsilon的时间内。它比随机选择的概率少一些，约为epsilon。

我理解了您的意思，稍作调整如下：
嗯，实际上，这里的选择空间稍微小了一点，因为，嗯，如果按照均匀分布同时在所有臂上进行手指数，并且我们会选择最佳动作。[杂音]好的。所以，在我们进一步讨论更好算法之前，先看看这些实例吧。嗯，比方说，我们正在尝试找出治疗脚趾骨折的方法，但这并非实际的医学案例。不过，我们可以想象有三位不同的外科医生——也就是三种不同选择。嗯，其一是手术，另一种是用另一根脚趾作为支撑来包扎受损的脚趾，这可能是您在互联网上找到的建议。最后一种选择就是什么也不做。结果的度量标准将是一个二元变量，即六周后通过 X 光来评估您的脚趾是否愈合。好的。所以，我们可以把这个问题建模成一个有三个臂的多臂老虎机，其中每个臂对应于，嗯，好吧，我稍后再问您它代表着什么。另外，这里还有一个未知参数。每个臂都有一个未知参数，即奖励结果。所以，让我们花点时间来熟悉实例。

在这种情况下，我们要考虑的是手臂的拉力对应的情况以及为什么我们选择将其建模为一个强盗问题而不是马尔可夫决策过程（MDP）。通常情况下，MDP会考虑到一个代理人在一个包含多种状态的环境中行动的情况，而在这个问题中，我们只有一个状态，即脚趾折断的情况。因此，将这个问题建模为强盗问题是合理的，因为我们只需要考虑在这个特定状态下采取的不同动作的影响和结果。因此，在这种情况下，拉动手臂或采取行动会产生什么样的影响，是我们关注的重点。

这段对话涉及的概念对应于医疗领域中的临床试验。在临床试验中，通过对新患者的护理做出决定来评估不同治疗方法的有效性。每个患者相当于一个独立的实例，接受不同治疗方法的效果不会影响下一个患者的治疗选择。目标是确定哪种治疗方法的平均效果最佳。在这个情景中，将各种治疗方法视为伯努利随机变量，即患者要么被治愈，要么在六周后没有治愈。

嗯，实际经验证明，在这个特殊的案例中，手术是最佳选择。如果进行手术，治愈的概率为95%，而静观其变的治愈概率为90%，不采取任何行动的治愈概率为0.1%。如果采用类似贪心算法的方法会发生什么呢？哦耶。抱歉，是否可以将其他因素考虑进去，比如手术的副作用？例如，与只听不处理相比，像[听不清]这种手术可能更经济，有没有办法将这些因素考虑在内呢？是的，这是一个很好的问题。因此问题在于，我们能否了解手术的侵入性以及其他可能的副作用等。您可以想象几种不同的方式来融入这些信息。其中之一是，您可以调整奖励结果。您可以说，也许手术更有效，但成本更高，我必须以某种方式将结果与成本结合。嗯，在这种情况下，人们可能会对另一个因素感兴趣，即您可能有不同结果的分布。那么在这种情况下，它们都具有相同的分布，都是伯努利分布。但在某些情况下，您的奖励结果可能并非是一个简单的函数，对吧？

有时候，对于某些人来说，手术可能会很糟糕，但对大多数人来说，它效果很好。但是也有一些人由于副作用很严重，会觉得糟糕，所以情况并非都是理想的。有时会有很高的风险，有些人可能对麻醉或药物反应不良。因此，在这些情况下，我们可能需要关注风险敏感性，而不只是预期结果。事实上，我们小组正在研究安全强化学习，包括安全性考虑，以及如何优化风险敏感标准。除此之外，也需要考虑患者的个体差异。可能需要结合患者的背景特征来决定手术方案，而不是套用统一标准。希望那些正在进行默认项目的人也会考虑这一点。我们可能会在未来的讲座中讨论这个话题。总的来说，我们经常会处于复杂的环境中，这些会对结果产生影响。在这种情况下，让我们设想一下可以采取的......

在这种情境下，我们正在讨论三种潜在干预措施，并且使用的是贪婪算法。如前所述，我们对每个臂进行一次抽样，然后计算经验平均值。假设我们对动作A1进行抽样，得到的奖励是1，那么A1的预期奖励经验平均值现在是1。接着我们执行A2，同样得到1，因此更新了平均值。然后我们尝试A3，结果得到0。如果我们假设平均值是随机分配的，那么贪婪算法选择每个臂的概率是多少呢？

在这里，你提到了"二加二，然后 epsilon 对一，加上或减去一点"。这是对于ε贪婪算法的情形可行的描述。在这种情况下，贪婪选择每个臂的概率就是50-50。看来你已经开始谈论ε贪婪算法了。

因此，A1和A2的概率是相等的，都正好是一半。让我们想象进行了几个时间步。在初始阶段，我们每个动作都会选择一次，然后总是与最佳动作的奖励进行比较。这样就可以得到遗憾，即A星的Q值减去所采取行动的Q值。一开始的遗憾是完全相等的，所以是最优的。当采取最佳行动时，遗憾为零。第二种情况下，遗憾为0.05（0.95 - 0.9）。第三种情况下，遗憾为0.85（0.95 - 0.1）。而在第四种情况下，遗憾为0。

因此，A1和A2的概率是相等的，都正好是一半。让我们想象进行了几个时间步。在初始阶段，我们每个动作都会选择一次，然后总是与最佳动作的奖励进行比较。这样就可以得到遗憾，即A星的Q值减去所采取行动的Q值。一开始的遗憾是完全相等的，所以是最优的。当采取最佳行动时，遗憾为零。第二种情况下，遗憾为0.05（0.95 - 0.9）。第三种情况下，遗憾为0.85（0.95 - 0.1）。而在第四种情况下，遗憾为0。

在这种情况下，我们会选择贪婪吗？考虑到我们目前已知的值，我们还会再次选择 A3 吗？不，根据目前的情况，我们不会再选择 A3。有人可能会问，为什么不选择呢？在目前对 A3 的预期奖励是多少呢？是的，我们的预期奖励是多少？我想说的是，虽然我没有具体列出来，但这是实际情况。这些数字代表我们得到的奖励。所以，分别是1, 1, 0。因此，我们目前对 A3 的预期奖励是0。我们知道我们的奖励在0和1之间。我们的估计永远不会低于0。考虑到我们之前两次行动都得到了正1的奖励，这意味着它们的平均值永远不会达到0。因此，我们将永远不会再选择 A3。

这实际上并不像这里描述的那么糟糕。在这种情况下，A3只是一个较差的选择，因为它的预期奖励较低。在其他情况下，可能是因为我们运气不佳选择了A3。这意味着我们可能永远不应该选择最佳行动。我认为用B星来表示行动的奖励是一个好方法。在这个情境下，它和这个概念是相同的。

所以，这和B星表示的意思是一样的。我可能会在符号之间进行切换，但是您可以随时问我。所以在这种情况下，我们可能永远不会再次选择A3。请注意，在贪婪的情况下，有时候如果我稍微调整了参数，您可能会再次选择A3。这是因为如果情况稍有不同，比如没有伯努利奖励，而是高斯奖励，其他臂的奖励可能会下降到比另一个臂更低的水平，这时您可能会做出切换。因此，并不总是明智地坚持最初看起来最好的选择。但在这种情况下，根据这些结果，

我们不会再选择A3。好的，让我们开始e-Greedy算法。在这种情况下，我们假设前几个结果是完全相同的。然后，我们要采用二分之一减去epsilon的方式，当结果超过2时重新随机选择。因此，根据概率epsilon，当epsilon大于3时，我们将选择A1、A2或A3。当1减去epsilon大于2时，我们将选择A1或A2。有趣，好的。所以在这种情况下，几乎是一样的，只是我们仍然有一定的概率选择A3。我们可以进行类似的计算。在这种情况下，我们假设所有结果都是完全相同的。所以，这就是e-Greedy算法的操作方式。

我会再次选择 A3。是的。如果ε已修复，则不会进行更新，是的。如果ε固定，它会选择a_3多少次？主要问题是它是有限次还是无限次。也许你可以和你的邻居谈谈，然后确定ε是否固定，a_3是否会被选择有限次或无限次，这对结果意味着什么？好的。我将让大家投票。

因此，如果你认为它会被无限次选中，请举手。好的。那么，遗憾地是，这意味着什么呢？是好是坏？情况可能会很糟糕。好的。不好的是，不幸的是，在这些情况下，遗憾会是无限的。[笑声]所以我们会永远不幸地有无限的遗憾，但增长速度可以更慢一些，具体取决于你所使用的算法。[噪音] 所以，特别是，是的，我们也可以在这种情况下考虑。所以，如果你有一个像我们在这里为a_3做的那样的大差距，并且我们选择该选择无数次，那么ε-greedy也会有很大的后悔。我喜欢这个图，这个图来自David Silver的幻灯片。如果你永远在探索，就像，如果你只是随机进行，我们没有讨论，但你也可以这样做，那么你会有线性总的后悔，这意味着时间步t下，你的后悔会随着t线性增长。本质上，你的后悔是无限增长的，而且是线性增长的，这基本上是等比例的——我的意思是，它前面通常会有一个常数，但是，

嗯，它实际上是一个常数乘以您在每个时间步所能做的最糟糕的事情。因为如果您始终选择每个时间步的最差手臂，您的遗憾也会线性增长，这是非常糟糕的情况。如果您从不探索，即采取贪婪策略，那么遗憾也会是线性增长的。因此，从根本上说，这意味着到目前为止我们使用的所有这些算法在糟糕情况下可能表现得非常糟糕。因此，关键问题是它的表现是否比可行的更好。我们是否可以达到通常所说的次线性遗憾。因此，我们希望在算法在性能和样本效率方面被认为是良好时，其遗憾能够呈次线性增长。嗯，当我们考虑这个问题时，通常会考虑我们设定的界限是问题相关还是与问题无关。对于大多数情况，这取决于一个关键点。对于马尔可夫决策过程，我们可以获得的大部分界限都与问题无关。对于多臂老虎机问题，有许多与问题相关的界限。

在强盗问题中，问题相关的边界意味着我们获得的遗憾数量将取决于这些差距，这应该是很直观的。 比如，想象一下只有两个手臂，分别是a1和a2，以及平均值，还有预期奖励。 所以，如果预期奖励分别是1和0.001，那么直观上我们应该更容易看出第一个手臂比第二个更好，比如一个是0.53，另一个是0.525。 因为在一个情况下，两个手臂的平均值之间的差异非常大，而在另一种情况下，这个差异非常小。 所以直观上来说，如果差距很大，我们学习就应该更容易，而如果差距很小，学习就应该更困难。 对于某些行为而言，最佳奖励是确定性的，也就是说我们的遗憾为零。

在这个对话中，谈到了一个关于最佳奖励和贪婪策略的问题。最佳奖励是确定性的，直到后悔为零。如果我们知道每个手臂的奖励都是确定的，那么我们只需尝试每个手臂一次，然后就能做出最优决策。即使我们知道奖励是确定的，还是有其他限制。如果在贪婪策略下奖励是确定的，我们可以一次性尝试所有手臂，避免犯错误并减少后悔。在这种情况下，初始的遗憾可能与时间无关，因为后悔会趋于零。讨论中谈到了手臂方差与奖励的关系，问题是除了奖励的均值，方差是否也很重要。

嗯，是的，除了讨论问题的范围之外，您当然可以考虑参数。比如，如果您掌握一些有关奖励分布的参数知识，您可以利用这些知识。如果您了解它是高斯分布或类似的分布，那就更好了。一般来说，如果您了解这些信息，或者了解这些时刻的信息，您应该可以利用这些知识。我看到大部分的信息都是关于查看均值和方差。在很多情况下，我们常常假设回报是有限的，这对我们所做证明非常重要，即使没有进行其他参数假设。但是，换一个角度来看，与问题无关的版本只是说[噪音]，无论您所在的领域如何，无论间隔如何，我们是否能够证明，无论领域的结构如何，我们是否仍然可以保证后悔是线性增长的，这是我们今天要重点关注的问题。因此，我认为下限、理论下限有助于理解问题的难度。所以我认为，某些情况可能是次线性的，之前的研究已经对此进行了探讨。

遗憾必须增长多少呢？在这种情况下，我们主要考虑的是遗憾，通过使用mu's，记录下这些遗憾。 在这个情况下，根据差距和KL散度的分布相似性，可以展示遗憾增长的下限。因此，无限增长的遗憾是不可避免的。如果在奖励分配上没有其他假设，一般而言，你的遗憾将会无限制增长，尤其是在差距和KL散度这方面。但它仍然是次线性的，这是好的。因此，它与时间步长t呈对数增长。令人鼓舞的是，我们的下限显示还有空间上的余地，对吧？我们应该能够增长得更慢，至少目前还没有正式的结果表明它必须是线性增长，而更可能是可以增长得更慢。

那么我们应该如何做呢？为什么要这样做？我们可能会怎么做呢？现在我们讨论了一个特定的框架，即遗憾，我们讨论了一个背景，即强盗，现在我们要讨论一种方法，那就是面对不确定性时保持乐观态度。这个想法就是选择可能具有高价值的行动。为什么要这样做呢？我有一个问题 - 在上一张幻灯片中，那么这在每个 t 都是正确的，还是仅在其中一个，因为它实际上不只是说它大于无穷大？好的，这是个好问题。问题是 i- 是否成立。我认为这在每个时间步上都成立，我必须检查确切的方式，比如他们写这个的方式，还有常量，但我认为这应该在每个时间步的基础上成立。我们确实是在说，随着时间的推移，这应该是真的[重叠]是的。当 t 变大时的极限，这是我必须回顾原始论文的地方，可能还有一些临时的常数项，所以这可能是 t 变大时的主导项。

在许多情况下，特别是在我们的马尔可夫决策过程（MDP）案例中，我们经常会遇到与时间步长无关的临时项目，但在早期仍然很重要。这是我的猜想。这是一个很好的问题。面对不确定性，乐观意味着我们应该选择可能具有高价值的行动。为什么呢？这里有两种可能的结果。如果我们选择一些我们认为可能不错的东西，其中一种可能就是它是好的。因此，如果选择的行动是好的，或者更精确地说，在我们的例子中，假设我们选择了动作a₁，而且a₁确实具有高奖励，那么就很好了。如果我们采取了一项行动，因为我们认为它可能会带来高回报，然后实际上它确实带来了高回报，那么我们会感到略微遗憾。这也是一个好结果。另一种可能的结果是什么呢？如果a₁没有高奖励，并且当我们对其进行采样时，我们得到了较低的奖励。【噪音】

好的，如果我们从某种行为中获得了低回报，我们可以从中学到一些东西。比如，当我们去尝试一家餐厅，第一次体验很棒，但第二次却很糟糕，这时我们意识到这家餐厅并不太好，更新了对它的评价。这意味着我们认为之前的行动并没有那么有价值了。因此，从本质上讲，要么这个世界真的很美好，如果是这样，我们就不会后悔；或者这个世界并不那么完美，我们从中学到了一些东西。所以，这为我们提供了信息。

因此，从积极的一面去采取行动不仅能为我们提供有关奖励的信息，也有可能让我们获得更高的奖励。这一原则被证明是非常有效的，至少从1993年Leslie Kaelbling开始就已经存在。引入了区间估计的概念，然后开始在不确定性技术下对这些类型的乐观进行大量分析。那么，我们如何可以更正式地实践这一原则，或者说，我们如何能更准确地知道一个行动具有高价值意味着什么呢？

假设我们对每个动作的值估计设定一个置信上限，这样实际值很可能小于或等于这个上限。一般来说，这些上限取决于我们选择该动作的次数。我们希望当多次选择一个动作时，其置信上限应该接近其Q值。如果我们很少选择某个动作，可能会过于乐观。然后我们可以通过选择具有最高置信上限的动作来进行置信上限算法的拆分。因此，对于每个动作，我们都维护一个置信上限，然后选择具有最大置信上限的动作，之后更新选择的动作的置信上限。因此，在UCB算法中，在时间点t=1，2，3...时，我们首先有一个初始化阶段，在这个阶段我们为每个动作拉一次臂，然后计算所有动作的UCB。

然后，对于给定的一组点，我们会选择最大的那个作为arg max，然后从该臂的真实奖励分布中进行奖励采样。接着我们会更新该动作以及所有其他手臂的价值估计。实际上，通常我们不仅需要更新我们所选择的动作，还需要更新所有其他动作，尽管这并非必须，但从理论上讲，通常需要这样做以涵盖高概率区间。很快我们将看到更多相关内容。因此，每次获得奖励后，您都会更新所有手臂的置信上限，然后选择下一个动作并反复执行这个过程。好的。那么，我们如何定义“多伦多大学”呢？

因此，我们来看一下霍夫丁不等式。在霍夫丁不等式中，我们可以将其应用于一组独立同分布的随机变量。假设这些变量都介于0和1之间，我们将这些变量的平均值定义为a。霍夫丁不等式表明，样本均值a的概率会接近真实的期望值，也就是真正的平均值。我们通过经验得出的平均值可以看作是一个ε（epsilon）的常数。真实的平均值大于经验平均值再加上一个常数U，小于或等于指数减去2ν的平方的概率，其中ν代表样本量。因此，我们也可以这样说：

如果您希望某件事情以一定的概率实现，您可以选择一个mu值，使得Xn加上mu至少与真实平均值一样大。假设我们的目标是，希望经验平均值加上mu（小于实际平均值的概率）等于delta除以T的平方。尽管我们很快就会明白为什么要选择特定的概率，但让我们先假设这是我们需要的概率，这样我们就可以确定mu的取值。因此，指数-2mu的平方必须等于T的平方乘以delta。然后我们的任务就是解出mu的取值。这个解告诉了我们什么？就是在这种情况下，mu将等于T的平方除以delta的2n对数的平方根。所以，我只是解了这个方程。这对我们意味着什么呢？

这意味着，如果我们这样做，假设我们定义了我们的“奖励”概念，或者在这个案例中我们定义的"X"，我将使用与Hoeffding不等式相同的符号。因此，如果我们有Xn加上μ，并且选择了特定的μ值，那么它通常会大于或等于X的真实期望值，概率大于或等于1 - t的平方的增量。因此，Hoeffding不等式为我们提供了一种定义上限的方法。因为现在这些不再是X，你可以想象它们只是从我们的手臂上拔下来的"奖励"。也就是说，如果你取到目前为止你的单词的经验平均值，然后加上这个上限，这取决于我们拉动手臂和t的次数。因此，这里要注意的是我们拉动任何手臂的时间步总数，n是我们拉动该手臂的次数。因此，它们并不是同一回事。

所以，在我们目前的能力范围内，我们有一个能力范围，它会根据我们拉动特定手臂的次数减少，然后我们有一个对数项，根据我们拉动任何手臂的次数而减少。这就是为什么在每个时间步之后，我们需要更新所有手臂的上限置信度。因此，这两种竞争效应正在发生。当你拉动某个手臂的次数更多时，你会更好地估计奖励，因此上限会缩小。但是，你也有一个增长较慢的项目，即对数项目，它会随着时间步数的增加而增加。因此，这是我们定义置信上限的一种方法。因此，我们可以将其用于奖励。因此，我们可能想要说，某个臂的置信上限等于该臂的经验平均值加一再除以2，乘以我们拉动该臂的次数，再乘以 t 的平方对数，最后除以 delta。这是我们定义置信上限的一种方法。

好的，接下来的问题是如何证明乐观的遗憾是次线性的，这一概念已经被讨论。下一步是进行一个快速的调查，询问大家想要调查写在屏幕上还是黑板上。如果你想要在黑板上展示，请举手；如果你想要在屏幕上展示，请举手。我们将在黑板上继续下一部分。关于 t 推导的问题，刚开始我们似乎是固定的，但后来提到 t 是在每个时间步更新的。这个过程如何实现呢？确实，这是一个很好的问题。在确定对付强盗的时间范围后，可以将 t 设置为最大值。如果你知道你在 t 步内要采取行动，你可以将其代入然后进行操作。

在大语言模型中，有一个称为"置信界限"的概念，对数项基本上是固定的。在线设置中，如果你不清楚，可以通过不断用时间步更新来解决。关于Delta是如何决定的以及它的含义是什么这是一个很好的问题。Delta是怎么样的？Delta是什么？好的，好问题。在这里，问题是Delta是什么。在这个例子中，它告诉我们这个不等式成立的概率是多少，稍后我们将提供一个高概率的后悔界限。我们可以说有一个后悔界限，就像概率是1减去Delta一样，你的后悔将是次线性的，就是这样。你也可以得到一个预期的遗憾界限，UCB的论文中提供了预期的界限，但我认为这有点，这个界限在课堂上更容易理解。所以我倾向于使用高概率界限。是的，所以在我们谈论后悔之前，我还没有完全理解您如何使用。

我后悔了，更新您对行动价值的估计这一决定。嗯，关于您的问题，我们是否应该使用后悔边界，以及如何使用它来更新我们对行动的估计，实际上我们并没有这样做。后悔只是分析我们算法的一种工具。这样澄清了吗？因此，后悔是一种通过分析遗憾增长速度来评估算法表现的方法，但它并不直接应用于算法本身。算法不会计算后悔，也不会用于更新。打扰一下，我会暂时把这部分内容留在这里，以便深入探讨。好的，让我们继续。我们现在要尝试证明一些关于之前遗憾以及对于置信上限算法增长速度的内容。在我开始证明这点之前，我想向您证明一下，我们将探讨的是这些遗憾边界以及置信边界失败的概率。因此我在这里所说的是，我们将……（信号中断）。

根据该臂的平均体验，结合我们拉动次数而定的术语，我们定义这些置信上限。现在，我想说服你的是，在任何给定步骤里，置信度不成立的概率是多少。为什么这是个问题呢？好吧，我们希望在任何步骤中都能限制——对不起，当我们运行算法时，我们的置信区间无法保持的概率。为什么呢？因为如果它们都成立，我们便能确保获得一些，嗯，不错的特性。所以请注意，如果每一步上的所有置信度都成立，我们能确保以下内容。

因此，如果所有的可信度都成立，界限也成立，那么在时间 t，我们选择的动作 a_t 将会优于其他动作 q，也就是最优动作的真实价值。为什么会这样呢？其实有两种情况，要么 a_t 等于最优动作，要么 a_t 不等于最优动作。因此，如果我们花时间来思考一下，就能意识到如果我们的可信度界限成立，那么实际上存在这样的情况，即这个界限将大于或等于该动作的平均值。

如果这个方程成立，我们就不会失败。这意味着我们知道Qt将大于该臂的真实期望值。如果这是正确的，那么在每个时间步都会成立。如果不清楚应该问什么或如何思考这个问题，请随意举手或找一位邻居。因此，有两种情况：我们选择的手臂是星形或不是星形，在这两种情况下，如果置信界限是正确的，那么这个方程会成立。所以，也许让我们花点时间考虑这个问题，如果不知道如何开始，请随意举手。我想在这里澄清一下，如果置信界限是正确的，那么...

故对于所选择的臂，其置信上限将始终高于预期回报的最佳臂的真实值。这是说，无论你选择哪个臂，被选择的臂的置信上限都会高于任何其他臂的真实值。这就是这个方程所揭示的内容。虽然在所有时间步骤上这种情况可能不会持续，但我们可以认为它是如此，因为这些仅仅是高概率的边界。然而，如果这种情况在所有时间步骤上持续，那么无论选择哪个臂，其置信上限都会高于实际臂的价值。

好的，我理解您的意思是您要强调保持信心，将其放在首位。因此，拥有置信上限意味着以此方式定义的上限值超过了该手臂的真实值。让我们来解决这个问题。假设存在两种情况。如果 A T 等于 A 星，这意味着一个星级的 Ut 是否大于一个星级的 Q？这是否符合置信上限的定义？是的，根据定义。因此，如果一个行动有效，那么置信上限减去该行动的置信上限必须大于该行动的平均值。如果这个置信上限有效，好的，这是有效的。所以如果我们选择了最佳操作，

我们定义了置信上限，因此它们确实优于该臂的平均值，这一点是成立的。 另一种情况是当at不等于星号。这意味着什么呢？这表示at的Ut大于星号的Ut。因为否则的话，我们会选择星号。这意味着其他一些手臂的置信上限高于最佳动作，并且我们知道这些置信上限值大于星号的Q值。明白吗？因此，如果置信界限成立，我们就知道在每个时间步长，我们选择的手臂的置信上限都优于最佳手臂的真实平均值。是的。在使用epsilon贪婪策略时也是如此吗？你是在问在使用什么策略来选择手臂的问题吗？

是的。它将采取一些最大化的行动，对吗？不，事实并非如此。这仅在我们选择 arg max 时才成立，您可能看不到这一点，但可以看到 Ut 的 arg max A。因此，第一个不等式可能适用于其他算法 t，但它特别适用于置信上限。太好了，这是个好问题。所以这说明我们是否能得到它，我们很快就会明白为什么这很重要，但有点直观。这表示，如果置信界限成立，那么我们就知道我们选择的臂的单独置信界限将优于最佳臂。我们想要这样做的原因是稍后当我们进行后悔边界时，我们不想处理我们没有观察到的属性，即最佳臂的值。因为我们不知道恒星的 Q 值是多少。我们无法沟通，我们不知道那是哪只手臂。因此，当我们现在查看遗憾界限时，遗憾界限是指明星的 Q 值。我们不知道这个数值是多少。

因此，我们需要找到一种方法来消除这个数量，并最终基于这些上限进行决策，但我们需要确保所选择的臂的上限比Q星要好。如果我们的置信上限成立，这种情况发生的概率是多少？也就是说，如果我们想要在所有时间步骤上关于事件的并集，表示为ut等于1到t的所有事件的并集，其中星号Q的概率减去我们采取的操作的置信上限。我们关心的不是那种情况的概率，而是失败的概率。因此，如果所有的置信上限都表明情况是好的，那么失败的概率是多少？你所选择的那支手臂实际上并没有提供更好的结果。


-----


在讨论无限地平线的情况时要考虑的因素又是什么呢？这是一个很好的问题。确实，我们正在探讨无限地平线的情况。我们需要考虑时间步长 t 方面的情况，并找出我们的遗憾所在。那么，为什么这一点很重要呢？你们想让我保留这一点，还是我们现在就可以继续讨论？大家都提到了，谁来做呢？好的。那么让我们看看，我们是否可以前进？让我来看一下，或者不看也行。好的。那么，为什么这一点很重要呢？好的。我们现在需要明确定义我们的遗憾。因此，董事会这一部分只是在说我们已经在做这方面的工作，所以这些置信上限很有可能是正确的。现在我们要尝试阐明我们的遗憾是什么，哦，太好了。好的。谢谢。好的。那么，我们的遗憾是什么？

UCB 算法在经过 T 个时间步之后的遗憾，正好等于所有这些时间步 t = 1 到 t 的一个公式的总和。请注意，在这个过程中我们一无所知。对于我们所选择的每个手臂的真实均值，以及最佳手臂的真实均值，我们一无所知。因此，我们需要转化为我们所了解的东西。这些都是未知的。所以我们要采取的一个方法是强化学习中常用的技巧，即添加和减去相同的量。因此，我们对 t 求和的结果是：-t = 1 到 Ut 的 t，其中上限为 a_t - Q(a_t) + Q(A^*) - U_t a_t。我只是添加和减去相同的量，它代表了我们在每个时间点选择的手臂的置信上限。因此，重点是，

我们这里展示的是，如果我们所有的置信区间成立，那么我们选择的臂的置信上限将大于星型臂的 Q 值。这就是我们展示的内容。因此，这意味着它必须小于或等于 0。因为我们证明了无论我们选择的哪个臂，其置信上限都高于最佳臂的实际平均值。因此，这个方程的第二部分小于或等于 0，这表示我们可以设定遗憾的上限。因此，我们可以放弃第二项了。这很好，对吧？因为现在我们没有了未知数。我们只需要检查每个时间步采取的实际行动，并将该时间步的置信上限与真实平均值进行比较。但请记住我们定义置信上限的方式，并将其应用在这里。我们定义臂的 Ut 置信上限方式完全等同于，嗯，

在这里我们讨论了经验平均值，即at，加上1除以2的平方根，再除以t的平方，再除以delta的对数。这将是Q（at）与Q hat（at）之间与Hoeffding不同之处。 我们将这个概率记为U，即大于U的概率很小。 假设所有的置信区间都成立，意味着我们知道臂数的真实经验平均值和真实平均值之间的差异不会超过U的范围。 关于底部面板有两个问题，首先，第二个问题是关于i=1（臂数）的并集在哪里。

我明白您在讨论的是增量和求和过程，具体到 t 平方的增量。在这种情况下，我们要确保每个时间步长上，所有的置信上限都成立。这个过程是为了在所有武器上获得额外的金额，保守地确保在每个时间步上我们都不知道选定的手臂是哪个。这样做是为了表明对于星形 Q 值大于所选臂的上限而言，确保置信上限达到足够的程度是必要的。

对于每个时间点，对于每个单臂，您的置信上限都必须保持不变，这意味着您的置信上限在所有时间点上都是正确的概率。在这种情况下，我们说在特定时间步上，置信上限的概率是t的平方乘以δ。这就是我们定义U项的方式，根据Hoeffding不等式，它在t的平方上以最小概率δ成立。当你将t从1加到无穷大时，得到的值小于π的6次方，小于2。这是一个有趣的事实。

当你代入这个值，将得到一个2，然后将所有手臂的总数m乘以这个数，就得到了一个δ。这让我们可以进行无限求和。需要注意的是，

在无限视野成立之前，这可能会受到质疑。因为当我们进行求和时，本质上，我们是在确保我们的置信界限永远成立。所以，我们可以这样说，我们基本上是在假设我们的置信区间成立的前提下进行这一部分操作。我们的置信界限意味着，对于同一臂的预期均值和真实均值之间的差异在μ内，以高概率在U内，这里的U就是定义中的U。这就是我们可以根据Hoeffding不等式做出的声明。现在我们可以拿出这个值，就是我们的U，并将其代入到这个方程中。因此，这就是我们置信上限和Q之间的差异。换句话说，这完全等于从t = 1到U的t的总和。对符号可能有些混淆，所以我会插入精确的表达式，即1除以2与a的t、t log t平方除以delta。

在这种情况下，经验均值和真实均值之间的差异受到U的限制。因此，我们可以将其分成我们所操作的不同手臂。这是所有时间步长的总和。请注意，如果我们将其上限设置为大T，则这等于小于或等于平方根对数T的平方除以增量，然后我们将得到所有时间步长的总和。根据我们选择的手臂将其分开。这与我们查看每支手臂相同，我们拉动了多少次？因此n的总和等于1除以n、t、i、1除以n的平方根。因此，我们只需将其分开，就像对于每个手臂一样。

我们选择了其中一些步骤。 好，现在我要计数我们选择的手臂了。 所以，nt代表我们选择手臂i的总次数。 然后我们来总结一下，如果我们注意到这个事实：将1到t的平方根与n相加，结果小于或等于2的平方根t。你使用了一个完整的论证来说明这一点，我很乐意在线下详细讨论。是的，在后面。你说了1比2的情况是什么[听不清]。谢谢，我们有一个2，我们可以在这里放一个2。 谢谢。对于常数，我可能会放宽一些限制，但我绝对会抓住它们。因为这些边界大多数最终都会成为...

这取决于它是次线性还是线性的。更精确一点说。好的。所以我们有这个值，嗯，这个值何时达到最大？如果我们同样频率地尝试所有不同的选择，这个值就会达到最大。为什么呢？因为当 n 增加时，1/n 会减小。因此，最优的情况是，将你的资源平均分配到所有不同的选择上。所以如果我们回到这里，我将其称为 a。所以 a 小于或等于，抱歉，根号下的 2*log(t)*delta 乘以 i 的总和，i 从1到m，被 n 的总和除以 m 所等于的 t 的总和。这就好像我们将所有资源平均分配给所有不同的选择。1 除以 n 的平方根，好的，然后我们可以运用这个表达式。

好的，所以这里涉及的是一个数学计算问题。当我们将小于或等于1的数除以2时，我们接近目标。然后，我们计算t除以delta的平方，将得到i的总和，该总和在1到m范围内等于2乘以t除以m的平方根。接着，当对m进行求和时，我们得到小于或等于2乘以log t的平方除以m的delta的平方根。当我们考虑t和m时，我们完成了计算。那么这些计算结果意味着什么呢？它表明，如果我们使用置信上限，我们的损失增长速度是次线性的，即乘以对数。所以这里涉及的是时间步数，当我们使用置信上限时，损失的增长速度也会受到影响。

为了做出决定，遗憾的增长速度会慢得多。 这种情况并不影响问题的界限，也与间隙无关。存在更好、更紧密的选择，这取决于间隙。但这说明，在面对困境时保持乐观从根本上来说是明智的，因为这使我们能够更多地成长，相比在电子贪婪的情况下，我们在后悔方面表现更好。是的，你能否再次在白板上展示一下最后一个问题，关于从 t1 求和到大 T，然后你刚刚提到的 t 平方大 T，t 从 1 加到大 T - 1 会发生什么？很好的问题。是的，这里的对数项 t 范围从 t = 1 到 t，在 t 达到大 T 时达到最大值。因此，我们对这个对数项进行上限处理，将其变为一个常数，然后我们可以将其提出。这里最酷的一点是它是次线性的。这就是要点。好的，我会讲一个例子，下次我们会谈更多类似的情况。

好的，让我来整理一下你提到的内容。接下来我们将讨论一个关于脚趾骨折的玩具领域的例子，探讨在这种情况下置信上限是如何确定的，以及算法在这些场景中的应用。这将是我们接下来要研究的内容。随后，我们将介绍一种技术，即基于不确定性方法的乐观主义，也就是根据经验奖励的组合来研究价值问题。迄今为止，我们已经了解到了置信上限，并开始应用它做出决策。下一步，我们将讨论贝叶斯理论在我们对世界的认知方面的作用，即维护一个先验，不断更新它，然后利用它来指导我们的行动。所以，下周我们会深入探讨这个问题。期待再见。

好的，现在我们将继续并开始，与本节的主题一致，我们将在不确定性下保持乐观，希望这样的态度能够奏效，但我们会拭目以待。在我们深入内容之前，有人对后勤或其他课程方面有任何疑问吗？是的，我想确认一下，执行默认项目的人员是否必须提交里程碑或者只能提交其他内容，而不是执行默认项目？对，问题是在询问执行默认项目的人员是否需要为里程碑做任何事情。不需要，没问题。还有其他问题吗？好的，提醒一下，我们目前在课程进行到了上次我们开始谈论强盗和遗憾的地方，今天我们将进行一个简短的回顾，然后继续讨论快速学习。今天我们将从贝叶斯强盗转向马尔可夫决策过程，然后周三我们将更深入地讨论快速学习，接着我们会尝试讨论加快学习和探索的话题。

只是想要提醒我们所有人，为什么我们需要做这些事情。如果我们希望将强化学习运用到现实世界中，就需要认真考虑我们现有的数据，如何收集这些数据以及如何最有效地利用它们，这样我们的智能体就不需要大量数据就可以做出正确的决策。我最初对整个主题感兴趣是因为我开始思考智能体学习做出正确决策意味着什么，以及智能体需要多少信息才能做出正确决策的信息理论限制是什么。这能够帮助我们做出有据可查的最佳决策。因此，我们一直在思考一些不同的关键问题。我们正在讨论一些设置。上次我们讨论了老虎机，今天我们还将讨论老虎机和马尔可夫决策过程。我们谈到的是框架，这是我们正式评估算法优劣的方法。这些可能是一些经验成功的框架。上次我们讨论了遗憾的数学框架，今天我们将探讨其他一些框架，用于全面评估强化学习算法的优劣。接下来，我们还将讨论到。

帮助我们理解这些不同框架的方法和风格。 上次我们讨论了在不确定性下的乐观态度。 现在简要回顾一下强盗问题。 强盗问题基本上是马尔可夫决策过程的简化版本，在最简单的设置中没有状态，只有一组动作。 在这种设置下，我们要考虑奖励是通过某些随机分布给出的，而且这些奖励遵循某种未知的概率分布。 每个时间步，您可以选择一个动作，并观察到相应的奖励。 您的目标是通过时间选择动作以最大化奖励。 与监督学习问题的不同之处在于，您只能观察到您选择的动作的奖励，这是一种审查数据。 您无法预测如果采取其他动作会发生什么。 因此，我们必须根据已观察到的审查数据做出正确的决策。 在这里我们讨论的遗憾指的是从正式数学意义上来比较，…（接下来的内容不全无法翻译，请提供完整文本）。

我们所关心的是采取行动后的预期奖励，即最佳行动的预期奖励。需要注意的是所有这些都是随机的，并不一定遵循特定的参数分布，但可以用高斯分布来想象。假设我们有两个高斯分布，分别代表动作1和动作2。在这种情况下，动作1的平均值大于动作2的平均值，所以动作1有更好的预期奖励。然而需要注意的是，在某些特定试验中，有时您可能会发现实际奖励超过了预期奖励，即您可能在次优动作上获得比最佳动作更高的实际奖励，这点很重要。因此，假设您已经选取了动作A2并且得到了一个特定的样本，您可以根据特定的样本数据。

与选择最佳策略相比，我们对预期奖励的表现更出色。尽管已经多次考虑过这一点，但我们仍然专注于期望。因此，我们会问：“就平均而言，哪个手臂是最佳的选择？如果选择次优手臂，平均损失会有多少？”我们的目标是最大限度地减少总体后悔，这等同于随时间推移最大化累积奖励。因此，我们随后引入了在不确定环境下的乐观主义想法。简而言之，我们尝试估计每个手臂潜在预期奖励的置信上限。因此，我们试图对每个手臂说：“我们认为其期望值的上限是多少？”然后在采取行动时选择具有最高置信上限的手臂。这会导致两种可能的结果。具体而言，可能会发生两种情况。要么... 要么...at等于A星...

我们的遗憾是什么？是当我们选择最佳行动时，遗憾为0。这很理想。在每个时间步，遗憾要么是0，要么不是。如果不是0，那么置信上限会怎样呢？是的，是的，是的，这个答案是正确的，所以我们要减少遗憾。如果我们选择了一个非最佳的臂，那意味着它的实际平均值低于我们的平均置信上限，至少很有可能是如此。因此，通常我们对于该臂的真实含义会有更多信息，然后我们要减少它。如果我们减少得足够多，随着时间的推移，我们应该会发现最佳臂的置信上限更高。那么，如果我们采用下限会发生什么呢？

嗯，这就是为什么置信上限是一个非常好的概念之一。我谈到的这些想法已经存在了相当长的时间，大约有20到25-30年了。我记得最早提到这个概念的是1993年Leslie Kaelbling的《Kaelbling》。她当时在麻省理工学院。我不确定这是否是她的博士论文，还是之后的研究成果。她讨论了估计潜在回报的区间估计的概念。虽然她没有正式证明这个想法是有效的，但她在马尔可夫决策过程上进行了尝试，结果显示，就经验表现而言，这是一个非常好的概念。随后很多人开始研究并进行理论分析，证明了这个方法的有效性。我认为，不管是基于经验还是理论，哪个领域最终实现了进步都很有趣。因此，在处理强盗案例中的不确定性时，我们可以乐观地依赖于追踪我们观察到的奖励，并且我们已经证明可以使用类似霍夫丁不等式的工具来计算这些置信上限。要记住，我们从每个手臂得到的样本都是独立同分布的。

因为这些样本都来自相同的参数分布，我们无法确定。上次我们发现，通过使用置信上限算法，可能会有对数后悔。为什么这很重要呢？因为我们正在研究贪婪算法，并且发现它们可能具有线性后悔。这是线性的吗？是指采取行动的时间步数。那么为什么线性后悔不好呢？如果是线性的，意味着你可能在每个时间点都做出最糟糕的决定，这是非常不利的。因此，我们希望学习过程更加谨慎，确保我们的算法正在逐步学会做出正确的决策。需要注意的是，在这种情况下，我们的结论与之前的研究略有不同。这与“间隙”有关，即δ a 等于指数族的 Q，a 的 - q。

采取具体行动而不是最佳行动要糟糕得多。这与先前的边界有一些不同。先前，我们证明了一个与间隔无关的边界，因此问题中的间隔并不重要。现在，这是一个基于间隔的边界。当然，现在你不知道实际上存在哪些差距。如果你不知道这些差距，那么你也就不知道要证明的最佳手臂是什么，但知道是否存在这种知识在分析或结果中始终至关重要。这就是说，你不需要知道具体差距是什么，但是如果你使用这个算法，你的遗憾增长的程度取决于领域的属性。你不必了解领域的属性，但这将决定你的遗憾程度。所以这就是说，对于置信上限，如果差距的大小不同，你将获得不同程度的遗憾。你的算法将根据置信上限继续运作。

在这种情况下，AI系统不需要了解这些细微差距，只会根据这些差距表现出更好或更差的性能。 总的来说，我们对此非常满意。 我们希望我们的算法可以适应问题领域，这就是所谓的问题相关边界，在这种情况下，您会喜欢这些。 您希望有一个不可知的算法，也就是说，提供问题领域相关的平衡，如果问题相对简单，它们会表现更佳。好的。 让我们看看我们的玩具示例，我们正在讨论它作为了解这些不同算法如何运作的一种方式，我们正在研究治疗脚趾骨折的方法，我们在手术、施加石膏或不做任何治疗之间进行考量，我们想象我们有一个伯努利变量来判断这些治疗是否有效。所以期待手术是最有效的，成功率为0.95，施加石膏为0.9，不做任何治疗为0。

那么，置信上限算法是如何工作的呢？首先，我们没有任何数据，因此我们对所有手臂进行一次采样，从伯努利分布中采样。假设我们有手臂a1、a2和a3。我们的经验估计是对于每个手臂，我们从中看到的所有奖励的平均值。接着，我们对每只手臂拉一次，相当于每个动作各拉一次，我们得到了1-1-0的奖励序列。接下来，我们需要计算每个手臂的置信上限，以便确定下一步该做什么。在这种情况下，我们的置信上限由经验平均值加上2*log t的平方根定义，其中t是拉动手臂的次数，Nt(a)是对特定手臂的采样次数。这个过程被称为全臂拉动。

这里描述的是多臂赌博机问题中的一种情形。在这个问题中，存在多个手臂（arms），每个手臂都有一个未知的奖励值，玩家的目标是最大化累积的奖励。

假设有三个手臂：a1，a2和a3。通过使用置信上限（Upper Confidence Bound，UCB）算法来决定选择哪个手臂来拉，以平衡探索和利用的权衡。

在这种情况下，每个手臂的UCB值需要根据当前的情况进行计算。对于a1和a2手臂来说，由于它们的奖励值相同，它们的UCB值也相同，可以根据算法计算得出。而对于a3手臂来说，由于其当前奖励值较低，它的UCB值会有所不同。

通过计算每个手臂的UCB值，玩家可以在下一步选择拉动哪个手臂，以达到最大化奖励的目的。

因此，我们有50%的几率选择其中一个，然后另一个50%的几率可以选择另一个。好的，让我们来比较一下。所以，如果我们使用UCB，我刚才提到过，我会在这里重新定义一下，这样人们可以记住，UCB的公式为a2等于1，再加上平方根2乘以log3除以1。a3的UCB等于平方根2乘以log3除以1。好的，为什么不花点时间来定义一下，如果您使用ε-greedy且ε=0.1，选择每个臂的概率是多少？如果您使用UCB呢？如往常一样，请随时与附近的任何人交谈。[噪音]好的，让我们进行投票吧[噪音]。

我可以帮您整理出更清晰易懂的内容。如果有一个问题，不嗯，如果两个手臂存在且有非零概率，或者说有三个手臂存在且有非零概率。这样的话，如果您使用UCB [NOISE]，那么两个手臂的概率是非零的吗？三个手臂的概率是非零的吗？有人知道UCB的人，您认为只有两个手臂概率是非零的原因是什么？是的，因为一旦您选择了最大动作，您只会选择a1或a2。对，没错。所以，如果我们在这里选择最大动作，那么我们只会选择a1或a2，我们第三只手臂的概率就会是零。好的，让我们来投票支持一下，嗯，ε-贪婪法。对于ε-贪婪法，对于两个手臂来说，有非零概率吗？三个手臂呢？对，选择a3的概率是多少？[噪音]。是的，[听不清][噪音]嗯，0。

在这段对话中，讨论了多臂老虎机算法中的一些概念。UCB（Upper Confidence Bound）和ε-greedy是两种不同的算法，它们在选择动作时有着不同的策略。UCB会尽量平衡探索未知和利用已知，而ε-greedy则更倾向于以相同概率对所有动作进行探索。

在对比UCB和ε-greedy时，指出了UCB在对手臂进行探索时更具战略性，尤其是对于避免选择认为不好的手臂更为有效。相比之下，ε-greedy则可能会无差别地对所有动作施加相同的概率权重，缺乏在探索与利用之间的平衡。

通过以上讨论，最终指向了对于在多臂老虎机场景下如何进行动作选择这个问题，提出了对于不同算法在权衡探索与利用方面的战略性思考。

让我们简单回顾一下。首先，我们要选择拉动作a1。现在，让我们设想我们选择的是哪个动作。好的，所以我们选择了a1。现在，让我更详细地说明一下在这种情况下的置信上限。假设我们选择了a1，现在，我们需要重新定义置信上限，实际上我们需要重新定义所有臂的置信上限，因为该置信上限的分母取决于t，即总次数t或动作a被选择的次数如果我们用这种形式的话，我们可以拉到当前为止。这样，我们会得到UCB。假设我们选择了动作a1，假设我们得到了1，对吧？所以a1的UCB将具有同样的平均值，即1，加上平方根2乘以log4，因为我们已经拉了四次，但这个动作只被拉了两次。所以，我们需要除以2。动作a2的UCB将具有和之前一样的平均值，因为我们还没有选择它。然后，它也会有2乘以log4，但我们只拉了一次。

在这种情况下，给定动作a3的UCB（Upper Confidence Bound）仍然平均值为0，并且也会有2log4除以1的噪音。因此，我们需要权衡对a1和a2仍然具有相同的经验平均值。但现在我们选择的a2比a1少，所以我们需要做出反转决定，现在我们选择a2。这就是为什么即使我们在上一轮选择了a1，无论我们得到的是1还是0，我们最终会选择a2。这种检查理解的方式确实很好，无论我们上次选择a1时得到的是1还是0，结果都会是这样。即使我们得到了a1的1，在下一轮我们仍会选择a2，因为它的置信上限会下降，即使均值是相同的。所以对比我们一直淘汰的最佳动作a1，我们的遗憾是多少呢？我们的遗憾是0，而这里是0.05。

当我们考虑数值 0.95 减去 0.9，结果正好是 0.05。而当我们再从0.95减去0.1时，结果是0.85。这种变化展示了我们处理遗憾的方式。尽管在模拟世界中我们可以推理出这个过程，但在真实世界中，由于我们无法获得完整信息，我们无法事先知道这些结果。UCB算法的操作方式还有其他问题吗？除此之外，需要注意到置信上限是高概率边界，也就是说有可能会出现低于实际平均值的情况。因此，我们必须考虑不同的情形，包括置信上限可能不准确的情况。有时候，简单地选择具有最高上限的臂并不一定是最佳选择，而可能需要考虑具有最高下限的臂。

我们可以选择具有最佳下界的臂。 如果我们有两个臂 a1 和 a2，我们对臂 a 的 Q 进行估计。 在这种情况下，a1 的上限较高，但a2 的下限较高。 为什么这会导致线性遗憾？双臂案例是一个容易考虑的情形。 如果您愿意头脑风暴，随时与周围的人交流。 这实际上是乐观的原因，至少在强化学习中如此。 大家对此有何看法？

这段对话涉及到了关于线性遗憾的讨论。所谓的线性遗憾是指当人们面临选择时，由于信息的不完整或者偏见导致无法获得真正的最佳选择而产生的遗憾。在这个对话中，讨论了选择a1和a2两种不同选项的影响。

当选择了a2这个选项后，会出现逐渐缩小的结果，类似于确认账单。如果选择了a2，并且在接下来的决策中也坚持选择a2，那么就会趋向于得到平均值的情况，而实际平均值可能高于选择a1的下限。这导致确认偏差的发生，即使a1是更优选择，由于持续选择a2而无法得知。因此，我们将无法获得能够反驳我们初始假设的信息，也就无法确定真正的最佳平均值是多少，这就是线性遗憾的产生原因。

因此，要解决线性遗憾问题，可以考虑采用一种乐观的方法，即设定置信上限。虽然这个上限可能会随着时间的推移而变化，但一个更简单的做法是在初始化时确保达到较高的价值。

举个例子吧，假设你已经观察到每个手臂的一个槽，而且它真的非常好。因此，只需将所有值初始化为一百万或类似的值即可。那么，当你进行经验平均时，你其实是在从这个奇怪的假池中取得平均值。你可以想象一下，一旦你得到了一百万、一万亿，你就假设你拉动了每个槽，然后对所有经验奖励进行平均。因此，在许多情况下，这种方法实际上可以很好地发挥作用，挑战在于确定你对这个虚拟槽有多乐观。因此，要将这种方法与其他方法进行比较，需要记住，贪婪会给您带来线性的总遗憾。持续的ε-贪婪策略也会带来线性的总体后悔。如果你衰减 ε-贪婪，实际上可以获得亚线性的总遗憾，但通常需要对未知的差距有所了解。所以这有点嗯，通常是不可能真正实现的。但原则上，你知道，如果你有一个神谕，你就可以……

当处理腐烂事物时，如果初始设定过于乐观，可能会导致亚线性后悔的情况，然而确定乐观的程度非常微妙。在马尔可夫决策过程中，需要比所想象的更加乐观才能使过程顺利进行。接下来我们将讨论贝叶斯老虎机和贝叶斯遗憾。迄今为止，我们对奖励分配几乎没有做出任何假设，只是假设奖励是有界的，通常在0到1之间或者0到某个有限值（如0到R max）。尽管我们了解奖励是有限的，但我们并未对奖励的分配做出具体的参数假设。虽然Hoeffding并不要求奖励是有界的，但我们仍然需要确保奖励的分配不属于高斯分布或伯努利分布等特定分布。

因此，另一种方法是假设我们实际上确实拥有关于奖励参数分布的信息，并且将利用这些信息。现在我们要讨论贝叶斯强盗，即在给定历史情况下显式计算奖励的后验概率。考虑到先前的行动、拉动的手臂以及观察到的奖励，根据后验概率，我们可以使用它来指导探索。如果您拥有准确的先验知识，这将对您有所帮助。频率派和贝叶斯派之间存在一定程度的争议或世界观分歧。在这门课程中，并没有对此展开深入讨论，但我们认为这是集成先验知识的一种很好的方法。如若您有关于环境特定奖励结构的先验知识，可以将其融入到探索中，有助于您的决策。现在从贝叶斯的角度快速回顾一下贝叶斯推理。对于许多人来说，这可能是一个复习，而对一些人来说则可能是新知识。我们假设我们对未知参数有先验知识。在我们的例子中，这些未知参数可能是……

我们想要确定每个臂的奖励概率分布的参数。我们的方法是通过观察或数据来更新未知参数的不确定性，例如在拉动手臂时观察奖励情况。让我们来看一个具体的例子，假设臂 i 的奖励取决于某个未知参数 phi i 的概率分布，这个参数是不确定的。我们会对 phi i 设定一些初始先验，即在拉动手臂之前对该参数的不确定性进行设定。然后，当我们拉动手臂 i 并观察到特定奖励 ri1 时，我们可以利用这个数据来更新我们对参数分布的估计，这些参数决定了该臂的奖励概率分布。因此，我们使用贝叶斯规则来进行这一步。具体来说，我们计算参数 phi i 的后验概率。

假设我们观察到奖励等于先验概率乘以给定数据或证据的概率，除以观察到奖励的概率，这就是贝叶斯规则。关键在于如何计算这些内容，以及如何更新参数的后验分布。通常，这种更新可能非常困难，特别是当没有关于先验和数据可能性结构的参数形式时。这涉及先验和数据可能性两个方面，如果没有明确的结构，那么其中之一可能是使用深度神经网络，另一个选择可能是一些其他随机方法。

在不确定参数分布的情况下，可能很难得到后验分布的闭式表示。但总的来说，通过特定形式的先验和数据可能性，我们可以通过分析来实现这一点。有些人熟悉共轭，但并非所有人。共轭是非常有用的概念，例如，指数族就是共轭分布。如果先验和后验的参数表示相同，我们称之为共轭。举例来说，如果先验和模型都是高斯分布，我们称之为共轭。这意味着我们可以通过分析或封闭形式进行后验更新，这非常方便。

在同一个参数族中，随着我们收集到有关这些隐藏参数的更多证据，例如这个[噪音]后面我会举例说明。在许多不同的参数族中都存在共轭先验的情况。这意味着如果在该参数分布中存在初始不确定性，当您观察到一些数据后，您可以更新参数，而且仍然属于同一个参数族。这种性质非常优雅，在统计数据中经常出现。[噪音]好的。因此，我们以伯努利的例子来说明这一点。考虑到一个强盗问题，其中每次拉动手臂只会得到二元结果。这是从具有参数 theta 的伯努利样本中进行抽样的。这种情况经常出现，比如广告点击率、患者治疗成功或失败等。因此，在许多情况下，当我们拉动手臂或采取行动时，我们只会得到二元奖励，要么是0，要么是1。[噪音]事实证明，贝塔分布，即贝塔(alpha, beta)，是伯努利分布的共轭分布。因此，这意味着我们可以在给定 alpha 和 beta 的情况下写出伯努利参数的先验分布，如下所示。

这是 theta 与 alpha - 1 以及 1 - theta 与 beta - 1 相乘再乘以 gamma 的比率。 Gamma 与阶乘以及阶乘分布相关。 因此，所有这些都可以通过分析计算得出。 一个好的思考这个问题的方法是，我们可以把 α 和 β 视为之前拉动手臂的结果。 因此，我们也可以利用它们来编码有关这个问题的先验信息。 我简要向您展示如何更新这些内容的示例。 [噪音] 但如果您假设 Theta 的先验是 Beta 分布，会有什么情况发生呢？ 嗯，如果情况是这样的，这就是先验。 然后，如果您观察到我们的奖励只有在 0 和 1 之间，因为这是我们的奖励，每次抽样都会分配奖励， 那么经过更新后的 theta 会呈现出非常好的形式。 这实际上就是相同的 beta 分布，只是在 alpha 中加了 1，或者在 beta 中加了 1。 因此，基本上，如果观察到 r = 1，那么您会得到 alpha + 1 和 beta 的 beta 分布。 如果观察到 r = 0，则将其添加到 Beta 项中。

在这个问题中，alpha代表你获得奖励为1的次数，beta代表你获得奖励为0的次数。您能解释一下theta如何影响这个结果吗？描述为什么这不仅仅是对beta分布的描述？为什么首先提到theta是伯努利很重要？我对这个问题提出了这个很好的疑问。因为beta分布是伯努利分布的共轭先验。因此，我们的思路是，如果考虑一个有二元结果的臂，那么我们可以将这个臂的平均值看作由伯努利分布的参数表示。所以，如果平均得分为0.7，意味着平均0.7次我们得到奖励1，然后剩下的0.3次得到奖励0。

我们接受 3 次零奖励。因此，嗯，该臂的平均值为 0.7。因此，我们在考虑一个描述其平均值的伯努利分布参数的臂。我们可以将其想象为一个平均值为 theta 的臂，这个 theta 就是伯努利分布的均值。现在，我们想要进行贝叶斯分析，根据迄今为止观察到的数据来估计该参数的概率。如果我们想要更新对 theta 的估计，而不是对奖励的估计，那么我们会使用 beta 分布来表示对可能的 theta 的分布。随着我们看到更多证据，我们会不断更新这一分布。很快我会向您展示这些 beta 分布的形状，这样我们就可以想象在获得更多证据后，theta 的概率分布是如何变化的。比如，如果您看到一系列奖励为 1, 1, 1, 1, 1, 1, 1，那么您的 beta 分布将……

表明 theta 很可能较高。 如果你得到 0, 0, 0, 0, 0 这样的结果，你的 beta 将会有一个不同的后移，这意味着你的 theta 可能非常低，接近于 0。很好，我们很快就会看到一个例子。在这种情况下，一个很大的好处是，对于伯努利分布来说，这是一个我们经常要考虑的常见分布，我们可以写出该参数的先验，我们可以仅使用计数来分析并更新它。因此，我们只需记录观察到的结果为 1 的次数和为 0 的次数，然后用这些信息来更新我们的后验。那么，现在我们如何在贝叶斯环境中评估性能呢？在常规的遗憾中，我们没有考虑参数的分布。我们只是考虑有些参数，比如，你知道那只手臂的意义是什么。然后我们定义了对最佳手臂的遗憾。贝叶斯后悔假设参数具有先验分布。因此，贝叶斯后悔告诉我们，通过...

考虑我们先验的可能参数，以及预期的后悔。如果我们获得特定的theta，对性能的期望又会是什么。因此，我们以一种略有不同的方式看待这个问题。尽管我们并不打算深入探讨这个问题的哲学方面。但是，贝叶斯遗憾告诉我们，我们无法确定这些臂的分布是什么。在不同的世界里，会有不同的价值观，以及我们在这些不同世界中的平均表现如何。那么，对于贝叶斯强盗，我们如何做出正确的决策呢？一种可能性是，假设我们对每个臂的奖励都有一个参数分布。在贝叶斯情境下，我们当然可以做到这一点。概率匹配的概念大约在1929年左右就已经存在了，已经存在了接近100年。我们想要基于最优概率选择一个动作a，这个想法看起来像是我们试图选择可能更优的武器一样直观。

我们想要选择的武器不太可能是最佳的。面对不确定性，乐观地说，一般来说，不确定的行动更可能是最佳行动。因此，不确定的行动意味着我们对它们的回报了解较少。问题在于，虽然根据最佳概率选择手臂听起来很不错，但我们并不清楚如何计算这个概率。因此，这里的意思是，我们想要对过去的手臂进行采样。这里的“历史”指的是之前的拉动和奖励结果。所以我们想要根据某个手臂相对于过去历史上其他所有手臂更好的概率来选择拉动哪个手臂。尽管这在理论上很有吸引力，但我们却不知道如何准确计算这个概率。所以这有些神秘，一个非常简单的方法居然可以实现概率匹配。

这个想法被称为汤普森采样，你知道，在大约20年前，这个想法再次流行了起来。一个真正有趣的地方是它在强盗问题上曾经消失了很长一段时间。当然，是在人工智能（AI）社区和计算机科学（CS）社区。然后大约八年前，人们重新对理解这些感兴趣了。部分原因是我一个同事发表的一篇论文，你很快就会看到结果表明它可能是非常有效的。那么，汤普森采样是如何工作的呢？我们要为每个手臂初始化一个先验。通常我们希望它是共轭的，但并不一定。如果是共轭的，那就太好了。我们为每个臂都有一个概率。请记住，这是决定分布的参数的概率。所以，如果我们有伯努利臂，它的概率可能是θ。在这里，i代表第i个手臂。例如，它可能是beta分布的参数。

我们可以这样说，第i个臂具有θ伯努利参数的概率等于从Beta1,1中采样。因此，我们会选择一个特定的参数族来代表我们对每个臂的奖励分布的先验分布。然后，在每一轮中，我们首先从后验中采样奖励分布。稍后我们将再次介绍一个具体示例。这就像选择一个特定的θ，假设臂e的平均值或伯努利参数为0.7，并为该参数选择一个特定值。一旦你得到这个值，你就可以通过取平均值来计算动作价值函数。因此，请注意，如果θ是0.9，我们假设对第1臂进行采样。我们刚好采样到了第1臂。

当 θ 参数设为 0.9 时，第一个臂的 Q 值将等于伯努利参数 θ₁ 的期望值，即为 0.9。因为伯努利随机变量的期望值就是 p，即θ。因此，我们需要计算每个动作的价值函数。举个例子，对于高斯分布，它就是其均值。因此，我们只需要计算在我们采样的特定奖励分布下，每个臂的平均预期奖励是多少，然后选择最适合这些采样参数的动作。然后我们执行这个动作并观察奖励，之后利用贝叶斯定理更新我们的后验概率。因此，我们只需考虑参数。我们会对一组特定的参数进行采样，尽管这些可能是完全错误的。这些只是我们为每个臂设定的实际参数。然后我们会假装这个世界是最优的，获得一些数据，然后重复这个过程。

这是一个比较简单的问题。我们需要看看如何进行采样。稍后我会用一个伯努利实例向您展示。但我们并没有尝试明确计算哪个臂的后验概率最优。我们只是进行一些采样，然后稍微贪心地处理这些样本。所以汤普森采样实现了概率匹配，这非常棒。从直觉上来说，这就是概率匹配。概率匹配意味着我们根据历史记录中的最佳概率来选择一个动作。基于概率，臂确实是最佳的臂。我们可以将其视为在给定历史的情况下选择奖励的期望值，并且臂等于给定历史Q&A的argmax。这就是汤普森采样所计算的内容。那么，让我们看看脚趾骨折的例子实际上是怎么样的。

在这种情况下，为了让描述更加具体，我们需要考虑三个不同的部门，它们都致力于研究如何更好地治疗人们的骨折脚趾，以及治疗的成功或失败。手术的伯努利参数为0.95，而胶带的参数为0.9，（即失败的概率为0.1）。在这种环境下，如果我们要进行汤普森采样，需要牢记模型不知道这些实际参数。我们会选择参数为beta 1,1。这意味着我们会认为theta1的概率等同于beta。那么，beta 1,1是什么？看起来像均匀分布，值范围在0到1之间，这就是theta的概率。因此，如果有人提到beta 1,1分布，意味着你需要从中选择一个伯努利参数，这个值可能是0，可能是1，也可能是0.5。

这段内容讨论了关于汤普森采样算法中参数先验分布的设定和采样过程。在开始时，参数 theta 对于每个"臂"都是未知的，因此假设采用了一个均匀的无信息先验分布，即 beta 1, 1分布。汤普森采样算法的核心思想是从参数的先验分布中采样参数值。在这种情况下，参数的值在 0 和 1 之间均匀分布，因此会选择一个介于 0 和 1 之间的值。虽然目前还没有收到奖励，但通过汤普森采样，可以从先验分布中为每个“臂”选择一个参数值，同时保持对每个“臂”一视同仁的假设，即没有理由认为哪个“臂”比其他的更好。

我们做的事情就是刚才提到的，我的 θ 参数可能是从均匀分布中采样得出的。在这种情况下，我们像进行采样一样，从均匀分布中多次得到了不同的值。这些是我们固定的参数。现在，让我们假设这些是真实的值。比如，我假设我的手术 θ 是 0.3，我的录音 θ 是 0.5，我的空载 θ 是 0.6。那么，在这个虚拟世界中，如果我们要选择哪个手臂呢？是第三只手臂。确切地说，因为第三只手臂的 θ 值是 0.6，所以在这个世界里，第三只手臂是最好的选择。因此，我们将选择 θ 为 0.6。另外，使用无信息的先验是有必要的，因为可能有很多不同的 θ 值，比如 beta 2, 2 或者 5, 5。在这种情况下，对于 Thompson 抽样或其他方法来说，这样做具有明显的优势吗？

是的，这个观点确实很有道理。她提到了我们目前使用零信息先验，与使用零信息先验相比，这种做法有利有弊。所以你可能会得到 Beta 版本 3 或 4。嗯，β 值为3或4，或任何不是1或1的值，都会导致你的分布有偏差，改变你对样本的抽样形式。如果你有真正有用的好信息，它就像有了一个真池一样。嗯，它可以指导你对样本的排序。缺点是，如果信息不正确，一段时间内你可能会被误导。因此，我们经常讨论我们对错误先验的鲁棒性。嗯，使用零信息先验意味着你不会从先验知识中获得很多好处，但也不会处于劣势。好的。因此，在这种情况下，我们选择手臂三，因为这正是我们在样本中预期平均值最佳的手臂。嗯，但实际上第三个手臂并不太好。

我们知道这一点，因为第三臂实际上只有 0.1 的奖励。 因此，在采样并获得患者结果时，我们会在这种情况下得到 0。 因为真实的第三臂奖励是0.1。 因此，我们从0.1的伯努利样本中采样，大多数情况下我们会得到0。 现在我们要做的是更新第三臂的后验。 假设奖励是0，我们需要更新第三组的 θ 概率。 好的。所以我们讨论的是beta分布是伯努利先验的共轭分布。 如果我们观察到1，我们会更新第一个参数，也会更新第二个参数，但我们只观察到0。 所以我们的新 beta 分布是1、2。 因为我们刚刚观察到0，所以我们进行更新。 这就是我们的新参数。 这是我们第三臂的新后验分布，如下所示。 所以这仍然是θ，值始终需要在0和1之间，因为这是伯努利参数。

现在的情况是这样的。请注意，情况已经发生了变化。过去它是平坦的，但现在它发生了变化。我们观察到奖励为0。因此，现在我有一个更小的概率θ。因此，如果我要进行采样，与之前不同的是，相比较较高的值，我更有可能得到较低的值。这就是我们的新情况。另一只手臂的情况又是怎样的呢？这个是为了θ3。对于其他两个手臂，它们看起来仍然一样，因为它们仍然是β1,1。对于β1,1来说，对于另外两个手臂θ1和θ2，这就是θ的概率。另外两个仍然是一样的，因为我们还没有对它们进行任何干预。我们还没有任何结果。所以它们看起来仍然像是从均匀分布中抽取的。但是在θ3上的概率看起来更偏向0。因此，现在在Thompson采样中，我们再次从这些不同的值中进行采样一个值。

在这三个版本中的每一个中，现在想象一下我们得到了0.7、0.5和0.3，对吧？向后翻一张幻灯片，应该说Q a3的p，而不是a1的Q，因为你不是说a3-谢谢。是的，等等，那里有一些错误，是的。感谢您指出这一点。还有其他问题吗？好的。所以我们更新了第三臂的后侧。我们第一臂和第二臂的后侧与前侧相同，因为我们没有——我们没有拉它们。好的。现在我们将从这三个版本中进行采样。汤普森采样现在所说的是，给定所有臂的后验，让我们为每个臂选择一个实际参数。这次我们将得到0.7、0.5和0.3。那么这次我们要选择哪个臂呢？臂一。臂一。正确的？所以现在最大的将是第一臂。好的。所以现在我们将...

我们得到一个看起来像 Beta(2,1) 的后验分布，因为我们更新了先验 Beta 的参数。请记住，我们可以将其视为 R 的数量等于 1s + 1，因为我们从 Beta(1,1) 开始，这是 R = 0s 加 1 的数量。所以现在我们的新后验分布看起来像这样。这里是 Theta(1)，这是 1，Theta(1) 的 0 概率。好的。现在，正如我们所期待的，我们看到 Arm 1 取得了良好结果，因此伯努利参数高于0.5的概率正在上升，因为我们看到了一些积极的结果。好的。现在我们有了，那么我们的新后验是什么样子的呢？我们有一个 Beta(2,1)，我们有一个 Beta(1,1)，因为我们还没有选择第二组，然后我们有一个 Beta(1,2)。好吧。那么接下来会发生什么呢？嗯，让我再次对伯努利参数进行采样。假设我们得到了 0.71、0.65 和 0.1。

这意味着我们将再次选择第一臂。 我们观察到手术效果相当好。 现在我们的后验概率是3比1。 所以现在是0到1之间的概率值，代表theta1的概率。 看起来现在情况更加有利。 你对我们可能选择下一个尝试的手臂有什么猜测？ 请记住，我们目前针对第二臂和第三臂各有一个分布，对于第三臂的分布看起来是这样。 因为theta a2和theta a3是在0到1之间的值。 谁会认为，嗯，theta1会再次被选中并且看起来比其他选择更好呢？ 这是正确的，因为它的伯努利参数有一个后验分布，该参数越来越接近1，概率曲线也变得越来越陡峭。 至于theta2，我们仍然没有采取过动作a2，

但大语言模型只给出一个统一的概率。 因此，我们从中采样得到的值不太可能比我们从第一个选项中采样的值更好。 所以在这种情况下，我们可以想象再次采样，这次得到了0.75、0.45、0.4的结果。 我们再次选择动作a1，现在我们有了beta 4,1，它看起来更尖锐。 请注意，这与UCB算法完全不同。 UCB算法首先在动作a1和a2之间进行时间分配，因为它们都依赖于经验值，但后来动作a2的时间减少了。 在这种情况下，我们还没有采取动作a2。 这可能使我们犹豫不决。 然而，在这种情况下，实际上还可以，因为theta 1实际上是最佳选项。 但可能还存在一些权衡。 是的，我们唯一需要更新Beta分布的方法吗？ 是，是否有一定规则要求我们应该将其增加一些或者进行噪音处理？ 当然，我们有不同类型的奖励。 这里的奖励是0和1，对吗？ 那么你是否考虑过一些可能是测试版本、测试版本分配的奖励呢？ 很好的问题。 例如，

在这里我们正在讨论的是奖励的处理方式。如果奖励不是二元的，我们将如何处理呢？在这种情况下，我们不能简单地使用伯努利分布了。如果你的奖励不是二元的，伯努利分布和贝叶斯共轭就不再适用。如果你的奖励是连续值，你可能会选择使用高斯分布。在这种情况下，你会有一个高斯先验，取决于你对参数theta的了解程度。一般来说，就像对于多项式一样，你可以使用狄利克雷分布。你需要根据奖励的分布情况选择适合的共轭先验。通过这种方式，你可以对许多不同的参数分布系列进行更新。另外，我们正在讨论的另一件事是关于乐观算法的。乐观初始化就像从均匀分布开始。那么是否更乐观地对待问题更好呢？这是个很棒的问题。这里我们只考虑了一种确定性的先验。使用乐观的方法是否更好则取决于我的经验和具体情况。我想先问一个问题，请提醒我你的名字。

因此，让我们看一下它们实际上是什么样子的。 在这种情况下，如果我们保持乐观，首先对所有的操作进行采样，然后我们会得到a1或a2的结果。 在汤普森抽样中，我们选择了a3，接着我们又选择了a1，a1，a1，a1，a1，a1，a1，a1。 在这种情况下，a1是最佳选择。 因此，通过使用统一的先验，在某种程度上一旦你看到一些看起来比0.5更好的选择，你就会倾向于更频繁地对其进行采样。 因此，你的利用速度在某种程度上更快了。 你可以将这个先验放在那里，但通常问题是要放多少以及它是否真的有帮助。 因此，汤普森抽样一个很酷的地方是，在贝叶斯遗憾边界的情况下，它们和置信上限一样好，但根据经验来看，更快地探索通常是有帮助的。 所以你可以保持乐观，但实际上这可能会损害性能，因为这会迫使你选择，就像在这种情况下，r1实际上是最佳选择。 现在，你可以想象我们可能只是幸运地得到了a2。 在这种情况下，您需要一些东西来帮助您最终选择a1。

请注意，即使在使用统一的先验的情况下，也仍然存在可能在某个时刻选择 a2 的情况，因为在该先验下，可能性为0.999。因此，即使遵循这些先验，您仍可以确保尝试其他操作。但这是一个很好的问题，需要考虑要包含什么信息，这点很有趣。是的，对。嗯。请告诉我您的名字。[听不清楚]。它困难地被打断，其他动作很难跟上。所以，嗯，在这方面，失败的机会更重要吗？然后[听不清楚]。这是一个很好的思路，所以如果一只胳膊真的很棒，那么其他胳膊就很难赶上。所以在这种情况下，情况确实是，因为 theta 1 实际上是0.95。让我们设想一个稍微[有些噪音]不同的情况，比如0.

当Beta分布的参数值接近1时，在一段时间内进行采样，Beta分布很可能会逐渐收敛到真实参数值附近。因此，如果你持续对theta 1进行采样，最终会收敛到真实值。如果真实值与1相差较大，那么你可能需要更多的样本来进行采样。这就好比穿制服，离1越远，采样到更高值的概率不为零。因此，初始阶段很关键，但随着时间的推移，你会逐渐接近真实值，就像我们处理经验分布的方式一样。

如果我们观察常客遗憾的生成，这与贝叶斯遗憾不同，因为需要对参数进行平均。在这种情况下，汤普森采样的效果会更好。因此，在这种情况下，结果将趋近于0。

在这段对话中，谈论了汤普森采样在算法性能上的表现。 汤普森采样是一种在统计学中广泛使用的采样方法。对比于传统的置信上限方法，汤普森采样被认为是更加乐观的。尽管两种方法在下限方面相似，但乐观性上限更好。可以考虑将两种方法结合，从置信上限开始，然后使用汤普森采样的信息来更新结果。这样可以充分利用两种方法的优势。

[噪音] 嗯，我觉得我们可以尝试将它们结合起来可以吗？或许你可以。我认为你可能可以。我不确定。所以我认为其中一个方法是从一个置信上限开始，然后尝试使用汤普森采样。对我来说，汤普森采样的一个好处是它不像置信上限那样过于乐观。置信上限在很久以后往往过于乐观。就像，“哦，我试错了 30,000 次，但下一次也许会成功。”就像你的机器人或者其他类似的东西。所以，嗯，它通常会考虑到一些极端事件。但是，如果你知道这个世界实际上是一个高斯分布，就像你的机器人再次撞墙的概率依然很高，即使它只撞了 10 次。因此也许你应该考虑不同的方法。因此，我想通常你可能想从汤普森采样开始，但你也希望保持对之前观察到的数据的稳健性。一些尝试将这些想法结合在一起的工作被称为 PAC-Bayesian，你尝试在这个过程中得到，我将很快定义一下 PAC 是什么。但是，嗯，你试图得到的是一种折中，你试图保持一种类似于优信界限的概念，但同时也能两全其美。所以，如果你的先验知识很好，你可能更倾向于贝叶斯方法。

如果你喜欢某种常客，但发现事实证明你的先验是错误的，那么你可能会喜欢 PAC。 是的，所以我认为有篇论文改变了很多人对贝叶斯、强盗和汤普森采样的看法，这篇论文是由我的同事 Lihong Li 和 Chapelle 完成的。他们研究了上下文强盗，希望我们周三可以稍微讨论一下这个问题。

上下文强盗的概念是，你有一个状态和一个动作，这使它与我们迄今为止看到的强盗有所不同。与马尔可夫决策过程（MDP）不同，你的操作会影响下一个状态。因此，举例来说，对于那些执行默认操作的人来说，可能会出现这样的情况：你当前治疗一个患者的方式不会影响下一个患者，但患者的特征可能会影响哪只手臂更适合使用。

因此，上下文强盗是一个非常流行和强大的框架。在这种情况下，他们在研究新闻文章推荐时发现，汤普森采样的置信上限要比许多其他算法更好。当结果存在延迟情况时，它也可能更加稳健。

在实际案例中，这种情况经常发生。 想象一下，你正在治疗一个病人，但在接下来的六周里你无法确定脚趾手术是否有效。 与此同时，其他病人也需要治疗他们的脚趾。 如果你使用置信上限算法，通常结果很确定。 然后，你可以继续对每个病人采取相同的方法，直到得到第一个结果。相比之下，汤普森抽样是随机的，所以你会尝试多种方法。 这就是为什么汤普森抽样在实践中非常有用的另一个原因。虽然我今天不会详细证明，但建议如下：观察汤普森采样的贝叶斯遗憾，它会产生类似于置信上限的结果。因此，它本质上关注的范围与 UCB 相同。 尽管其中有一些微妙细节，但粗略地证明，如果你的先验是正确的，这些方法也有很好的贝叶斯遗憾边界。这样便是一个相当良好的合理性检查，在这种情况下你将获得对数后悔的增长。

好的，我会尝试翻译和重写这段内容。

刚刚提到的另一个框架可能大致是正确的。理论上的后悔界限指定了您的后悔随时间增长的方式。很难确定是犯了许多小错误还是几次大错误。由于后悔是累积的，它无法区分这两种情况。在患者治疗的情况下，这可能至关重要。有些病人可能会出现非常严重的副作用，就像每个人都可能会有头痛一样。因此，累积的遗憾无法区分这两者，因为如果几个人有非常严重的副作用，那么平均起来与许多人只有头痛一样严重。一个想法是，也许我们只需限制出现严重错误的数量，例如，我们希望限制出现严重副作用的人数。因此，“可能近似正确”在监督学习中越来越重要。在决策的背景下，我们经常用以下方式定义“可能近似正确”。

有一个可能近似最优的算法，可以在多项式时间内选择一个动作，使得该动作与最优动作的差距在epsilon以内，概率高达1-delta。尽管该算法不能保证每次都选择最优动作，但有很高的置信度会选择接近最优的动作。这种可能的近似性来源于我们只保证epsilon-optimality。重要的是，唯一的潜在错误是可能会远远大于epsilon，因此可能发生的重大错误的情况是个多项式函数。这里的多项式函数是关于问题参数的函数，例如可供选择的动作数量、epsilon和delta。您应该能够提前计算这一点，以便知道可能会犯多少错误。令人兴奋的是，许多PAC算法都拥有这样的性质。

好的，我来重新整理下你提供的内容。

PAC算法基于乐观或汤普森采样。相对于我们在MDP 时使用的方法，PAC方法在处理强盗问题时较为不常见。在强盗问题中，我们常常感到后悔。然而，在研究马尔可夫决策过程时，PAC更受欢迎。我们稍后可能会看到其中一个原因，或者如果今天没有涉及到，欢迎随时提问。

在我们之前的小例子中，PAC会是什么样子呢？让我们用O表示乐观，TS表示汤普森采样，在ε内表示我们选择的操作在最佳操作的ε范围内。因此，ε的值接近最优操作。这里我们所面临的情况是-因为我们是乐观的，我们选择的第一个动作是a1，所以他在ε范围内，是的，因为我们接近最佳动作a2，它的平均值为0.9。

因此，在0.95的95%置信水平下，这是确定的。动作a3的概率是0.1，因此它不在最佳动作的epsilon范围内，所以是“错误”的，依此类推。这意味着算法基本上可以根据epsilon的定义选择a1或a2动作。因为我不在乎你选择a1还是a2，两个动作都很好；它们之间的区别在0.05之内，我的意思是，这很好。但是，我们不希望你选择动作3，因为那是更糟糕的选择。所以在这种情况下，人们可能会说，这不在epsilon范围内，因为我们选择的第一个动作不好，但其余的都是好的。PAC方法的作用是计算所有这些——计算错误。所以，我们刚刚讨论了强盗问题，以及不同类型的框架和标准。

我们讨论了遗憾、贝叶斯遗憾和PAC。然后我们探讨了两种方法：乐观法和汤普森采样。现在我们可以看到，马尔可夫决策过程中有许多类似的想法适用，但也更具挑战性。因此，我们现在要特别讨论的是表格式马尔可夫决策过程。事实证明，即使使用表格式马尔可夫决策过程，情况也变得更加微妙。那么，它是如何运作的呢？在遗憾-PAC中，贝叶斯遗憾是适用的，乐观法也是如此，概率匹配同样适用。所以，让我们首先来考虑面对不确定性时的乐观主义。首先，让我们想象一下只进行乐观初始化会怎样。在这种情况下，设想我们将所有状态-动作对初始化为某个值。考虑到这一点，假设我们将它们初始化为rmax除以1-gamma，其中rmax是您在任何状态-动作对中能获得的最高奖励。我们花点时间思考一下，为什么这个值一定是乐观的呢？

有人可能会问为什么一定要如此乐观？答案是，这种乐观确实是正确的。因为正如我们已经多次证明的那样，将 rmax 划分成小部分一定会产生最大的价值，虽然这种价值可能会有点过于乐观。我们已经验证过，对于折扣马尔可夫决策过程，您可以获得的最大价值是 rmax 除以 1 - gamma。如果每个状态都有这个值将是最好的，但有可能某些状态没有，因此这个值一定是一个乐观的估计。所以，如果您将所有状态的操作值初始化为 rmax 除以 1 - gamma，然后使用蒙特卡洛、Q-learning或者Sarsa等方法进行增量更新，这将非常有帮助。这可以鼓励系统对状态和动作进行探索，因为本质上你是在假设一切都很完美，直到实际证明并非如此。不过，这种方法的缺点是，

很遗憾的是，按照通常做法无法保证性能，尽管根据经验通常会更好。因此，虽然这确实是一个上限，但这是乐观的。一个关键问题是你从这些乐观值的更新速度有多快。因此，在这个案例的早期结果中，Even-Dar和Mansour在2002年证明，如果您在Q-learning中使用学习率，则应该是α-i。因此，如果您以特定的α速率在每个时间步i上运行Q-learning，并且初始化一个状态的值，那么初始时刻，rmax除以1-γ乘以这些学习率的乘积，t是学习最佳Q所需的样本数量，那么仅贪婪Q学习就是具有该初始化的PAC。我只想强调一下这一部分。请注意，这是因为这是所有学习率的乘积，比rmax大于1-γ的值要大得多。

好的，让我重新整理一下这段内容，让它更通顺易懂一些。

所以，假设所有时间步长上的α值都是0.1。这意味着在这种情况下，得到的就是t乘以1除以0.1，结果大约是10t的[NOISE]。因此，需要做决策的时间步数呈指数增长，这看起来非常乐观。事实证明，这足以满足可能近似正确（PAC）的标准，但并不完美。这个数字非常非常大，明白吗？Chi Jin和其他伯克利的研究人员做了一些很棒的工作，表明如果你使用不那么乐观的初始化，这与置信上限密切相关。你需要非常小心你的学习率，并做出相应的调整。然而，如果你谨慎处理学习率，他们证明无模型Q学习也能符合可能近似正确（PAC）的标准。最近这是一大进展，因为

所有进行的工作都是在基于模型的环境中进行的。在NeurIPS大约两个月前发布了一项关于悔恨上界的研究成果。虽然并非最佳，但表现出了良好的悔恨上界。这表明零模型算法也可以做得很好。基于模型的MDP方法目前是我们拥有最佳边界的方法。因此，我们可以采用几个主要想法或程序。一种方法是对所有估计都非常乐观，直到您确信对动态和奖励模型的经验估计接近真实动态参数。这意味着算法会假设所有状态动作对的奖励都很高，就好像是rmax除以1-gamma。直到收集足够的数据后再进行最大似然估计时，才会对这些参数进行全面估计。

它们将接近真实参数。 所以你可以说，在我获得足够的数据之前，我会非常乐观。 然后，当我获得足够的数据时，我认为我可以获得一个接近真实估计的良好经验估计，然后我将使用这些数据。 所以这几乎有点像一个切换点。 你会一直假装一切都非常好，直到获得足够的数据，然后再转向经验估计。 所以这些是最早的一些，表明马尔科夫决策过程（MDP）可能是偏执逼近(compassionate approximation)。 这是2002年的事情。但从经验上看，它们通常也不是那么好，因为，你假装事情真的非常棒，尽管你可能有很多证据证明状态-行动对是并不理想的。因此，另一种方法是根据您所掌握的信息保持乐观。那么我这是什么意思呢？我的意思是，当你的AI智能体四处走动并收集对其所获得的行为和奖励的观察结果时，它会利用这些数据来尝试估计，给定这些数据后世界可以有多好。因此，一种方法是计算动态和奖励模型的置信区间。因此，我们已经在强化学习中看到了这一点。

我们已经计算了信心上限和下限，也可以计算奖励的信心上限和下限。实践证明，我们还能够计算动态模型的信心集。或者我们可以基于经验或数据添加奖赏奖金。在我们结束之前，今天我想讨论一下，至少谈谈第二点。我之所以要讨论这种特殊方法，是因为当我们开始考虑在函数逼近的场景中执行此操作时，如果您的动态模型是由深度神经网络表示的话，那么确定其不确定性就会变得复杂。在强化学习深度神经网络方面的许多进展都集中在无模型方法上。如果我们有探索奖励，那么我们可以轻松地将其扩展到无模型的情况。根据经验来看，如果我们使用明确的信心集，这些方法通常表现得相当好。所以我想简要解释基于模型的信心度、基于模型的区间估计以及探索奖励是如何运作的。假设我们有一个 ε-δ 和一些常数 m。好的。

接下来需要初始化一些计数。这是用来记录我们观察到的状态-动作对的次数。我们将对所有的状态 s 和所有的动作 a 进行这个操作。我们还将记录观察到的动作-状态-动作、下一个状态对的次数。0 表示所有的状态 s，a 表示所有的动作，s' 表示所有的下一个状态。我们还会记录从任意状态和动作对获得的总奖励数量。因此，我们可以说对于所有的状态 s，a，它的记录为 rc = 0。基本上，我们将记录我们处于任何状态、采取任何行动并进入任何下一个状态的次数，以及在执行这些操作时获得的奖励总和。然后我们会定义一个参数 beta。好的，我会仔细检查一下，我理解了—— 是的。

好的。所以我们将使用贝塔来定义奖励的参数。这个参数是根据公式计算得到的：1除以（1-gamma）乘以2取log状态数和动作数的乘积，再乘以m，其中m是输入参数除以delta得到的。接着，当t=0时，我们初始化状态。首先，我们可以直接将s的Qt,a设为1除以（1-gamma）。这里假设所有奖励的取值范围在0和1之间，即有界奖励。因此，当我们将所有账户初始化为0时，我们可以说我们还没有观察到任何奖励，就好像世界很美好。

我们的Q值可能是所有状态-动作对中最高的。在这种情况下，r-max等于1，因为我们的奖励范围在0和1之间。接下来，我们要做的是在当前状态下采取行动，基于Q函数的随机策略。这样做会打破之前建立的关系。然后我们观察奖励和下一个状态。接着我们只需更新我们的计数器，即更新特定状态-动作对的计数。我们要更新s、a、s'、s、a、s'的计数，即在特定状态下采取特定操作并转移到特定下一个状态的次数。然后我们更新状态-动作对的奖励，即之前的奖励加上当前的奖励。最后，我们将利用这些经验计数来定义经验转移模型和经验奖励模型。

因此，我们的奖励模型将是最大似然估计（MLE）奖励模型，它只是状态-动作对的奖励值除以我们遇到该状态-动作对的次数。这反映了该状态-动作对的平均奖励。接着，我们的转移模型也将是动作a和状态s的频次除以该状态-动作对出现的次数。我们将建立经验转移模型和经验奖励模型。初始值的设定并不重要，可以将其看作是任意值。好，我们将为所有情况执行这些步骤。然后，我们将计算一些新的Q函数。在执行这些步骤时，我们将计算出新的Q函数。

在使用经验模型的基础上，我们引入了一个奖励奖金因素，该因素取决于β值以及尝试特定状态下动作的次数。我们可以进行价值迭代，这是我在这里正在做的事情。你可以按照自己的方式来解决这个问题，但主要思想是通过对奖励模型和转换模型的经验估计，通过平均我们的计数或从状态-动作对中获得的平均奖励来使用这个奖励奖金。然后我们将这个奖励奖金添加进去。需要注意的是，奖励奖金可以是任何值，甚至是无穷大，所以你可以根据需要初始化任何Q s,a值，因为如果我们没有进行计数，我们可以将所有s, a初始化为nsa=0，你可以将其设置为Q-max。因此，如果你尚未对状态-动作对进行采样，请执行此操作。这意味着你需要对其进行处理。

任何未曾尝试过的事物都具有臻美之感。其他一切都是根据平均参数和奖励奖金的组合来确定的。随着获得更多数据，奖励奖金会逐渐减少。因此，我会将其整理如下，更为简洁。对，这就是奖励奖金的概念。您可以看到，随着时间推移，这个数字将会逐渐减小。随着时间的流逝，您将逐渐接近使用特定状态-动作对的经验估计。对于您尚未尝试过的状态-动作对，奖励将会更加丰厚。因此，最酷的事情在于，我们可以考虑这是否符合可能近乎最优的概念。让我再耗费一分钟时间，这与强化学习相关，如果在除去前N个时间步之外的所有时间步上，所选择的动作接近于最优动作，那么该算法就是可能近乎最优的算法，其中N代表着状态数...

动作数、gamma、epsilon和delta，并非所有的算法都会涉及这些要素。贪婪算法不属于PAC学习。贪婪算法的复杂度可能是指数级的。嗯，也许我们周三可以讨论这个问题。所以不行。令人高兴的是，我刚刚向您展示的MBIE-EB算法是符合PAC学习的。那么PAC学习的作用是什么？这意味着，除了特定的时间步数，我只是将其进行了圈出。所以这是一个非常复杂的表达式，但是它是状态和动作数量的多项式。它还是折扣因子和epsilon的函数。一般来说，要更接近最佳解，您需要更多的数据来确保您接近最佳解。因此它与epsilon成反比，并且这个多项式依赖于状态和动作。这意味着，在特定时间步之后，您的算法将会执行接近最佳的操作。这很棒。它表明只需要使用这些平均估计值加上额外项，然后计算Q函数。

除了面对多项式时间步的问题外，您实际上可以在其他所有时间步上表现得非常出色。接下来，我放的是一些理论，只是简单提到，在某种难以构建的、简单的玩具领域。这些类型的算法甚至在比一些已被证明有效的算法表现更好的情况下。它们可以比贪心算法等更有效。因此，像MBIE-EB这样的算法，MBIE是使用置信集的一种相关算法，它可以做得比其他算法更好，更乐观直到有信心。这些通常比贪心算法更优秀。因此，这些种乐观算法在实践中表现更好，且证明更胜一筹。在周三，我们将开始讨论如何将它们与泛化结合起来。谢谢。

好的，让我们开始吧。我想在开课时讨论一些后勤事项，以及解决一些在 Piazza 上关于期中考试成绩的问题，有些人担心这可能会影响他们最终的成绩。回顾一下去年期中考试的平均分和分布非常有趣，几乎相同。去年的平均分约为69%，今年约为71%。你会看到非常相似的分布。上面这张是我们的——哦不，下面这张是我们的。所以你可以看到这是2019年，这是2018年。这两个分布看起来非常相似。我们没有为班级设定官方曲线。如果有人得分超过90%，我总是认为，即使每个人都得分超过90%，这意味着这些人都非常理解材料，值得得到 A。如果我们的分布确实异常，有时我们会偏低一些。但只是为了让你了解，去年，

大约42%的学生在班级中获得了A。这对于那些关心期中考试表现、期末成绩以及是否能在课堂上取得好成绩的人来说是很重要的。有人对期中考试有疑问吗？我们知道有一些重新评分的请求，我们正在尽快处理。是否有任何其他问题我应该知道的？我很好奇是否每个问题都有可用的分数分布，因为我觉得在最后一个问题上我缺少了时间，想知道是否真的是这样。是的，我们有这些信息。我会再次与助教确认，我们没有理由不公布它，我认为毫无顾虑。我们使用Gradescope获取了所有问题的完整分布，所以我们可以发布。很多人在最后一道题上没有时间，那肯定是最困难的题目。这正是我们看到最大差异的地方，所以我们会特别小心对待这个问题。我们非常努力确保，如果在整个考试过程中有代数错误，那就没有任何价值，我们关注的是概念理解。有关期中考试还有其他问题吗？那么我将记录每个问题的详细分布情况。

一般来说，当我们处理这个问题时，我们会尝试查看任何具有很高方差的问题，然后再次检查该安排以确保我们是公平的。另外，我想提一件事 - 我们将继续接受到周五中期重新评分的申请。之后，这个请求将会被关闭。这就是关于中期的安排。希望这对至少班上的一些人平息一些担忧。现在我想谈谈另一个问题，即测验。测验将在大约两周之后进行，不到两周时间。这次测验的格式有些特殊。我们有自己的理由这样安排。在一个有大型期末项目的班级中，最大的压力之一是需要完成一个大型期末项目和一次大型期末考试。我认为，对学生来说，两者都需要付出很多努力。否则，为什么在期中考试之后还要上课呢？我的意思是，为什么需要学习课程后半部分的材料呢？我们确实认为这些材料很有价值，特别是最近涉及更重要话题的部分，但并不打算设置一个真正高风险的大型考试。因此，我和一些教学人员进行了交流。

在这里的教学中心被称为VPTL。我们提出的想法是进行低风险的测验，这个想法非常有趣。从很多人那里我了解到这个想法实际上是可行的，这就是设计初衷。具体设计是这样的，这个测验将分为两部分，都是多项选择题，并围绕某种高级概念性问题展开。我们将发布去年的版本作为示例供大家参考。整个流程将分为两个阶段，首先是个人完成单独部分，大约需要45分钟，然后将随机分配到一个小组进行合作部分。你的成绩将由个人部分和团体部分共同组成，但最终成绩取决于团体部分的表现。如果你所在的小组表现较差，那么你的成绩将与个人部分一致。我们设计这个模式的原因在于，对于团队来说，这是一个互相挑战的考验。因此，当你与团队决定答案时，你选择的答案将被标记为最终答案，关键在于你能够清楚地阐述为什么你相信某些答案是正确或错误的，并说服你的同学。

在这个过程中，这种做法可能非常有用，可以帮助我们更深入地理解材料，并倾听他人观点。最终的测验是以这种方式进行的。去年出现了一些噪音，之前人们在广场上对此有些担忧。可能会涉及一些博弈论方面的问题。我们精心设计了这个测验，让它只占成绩的一小部分。再次强调，只有在团队中表现优异才能取得更好的成绩，因此它是经过精心构思的。根据经验，当我们实施这种做法时，引发了很多欢笑，很多人似乎真的很喜欢这个方向。我们已经在考虑是否应该在课堂的多个部分中采用这种方式。但与以往不同的是，几乎没有人曾经进行过这种考试。有人对此有疑问吗？这将占你成绩的大约5%。是的。我记得你之前在课程中提到过，你们已经做出了关于团队的决定，或者将要做出决定，是这样吗？是的。我们还没有做出决定，问题在于团队如何分配，如果我们已经做出了决定，我们将通过随机分配来进行。这还取决于哪些SCPD学生将在校园参加考试。

几天前发布的安排是这样的：你会被随机分配到一个团队，与该团队一起参加考试。有人对此还有其他问题吗？我们将发布样题，覆盖所有课程内容。自期中考试以来发生的事情将更加重要，但整个课程内容都将涵盖在内。参加考试的想法是，那些一直在听讲座或在线观看讲座的人需要学习几个小时，充分准备考试。你可以带上备忘单，就像期中考试时那样。还有其他问题吗？是的，关于测验的个人部分还是与其他人一起的问题。我们会先单独做，然后再与其他人一起，大约20分钟后。所以，对于个人和团队的安排是这样的：每个人都会首先参加考试，大约需要45分钟。

好的，完成后你就提交答案，然后当大多数人都提前完成这部分课程时，取决于每个人，然后你们作为一个小组接受新的考试。 你们会进行与之前完全相同的考试，但只需达成共识的答案即可。 然后你擦掉你的答案，这样我们就能看到有多少次你需要擦掉，直到得到正确答案。 但基本上，我们只能看到你第一次是否就得到了答案，或者是否需要多次尝试。 你们对测验还有其他问题吗？ 如果对此有任何疑问，请在 Piazza 上联系我们。 我想简单谈一下去年出现的问题之一。 我们关心的是博弈论。 因此，你的得分总是个人得分和小组得分的最大值。 所以人们说的很对，你应该在个人部分尽你所能回答，因为那部分值得最多分数，然后在团体部分，如果你左右为难，应该找人同意第二个答案以对冲风险。 如果你想要[笑]或尝试这样做，你就可以，你或你的团队成员可能会超越自己。

去年集团部分大约占0.5%，所以非常少。存在这样一种可能性——只有在你真的在两个选项之间左右为难时，才会考虑这样做。再重申一次，只有一个正确答案，因此在实践中我们几乎不需要进行博弈论的分析。我们总是欢迎听到人们对事情的解释。现在有关测验或物流的其他问题吗？大多数人已经提交了项目的里程碑，接下来几天会给那些没有执行默认设置的人提供反馈。之前我已经发布了这条信息，是为那些之前没有看到的人准备的，等级分布基本相同。今天我们要做最后一部分关于快速学习的讨论，这是一个非常庞大的话题，需要做很多工作。今天我们会花更多时间进行讨论，接下来周一，切尔西·芬恩刚刚在伯克利完成了她的博士学位。

她即将在夏天加入这里的教师团队，并将讨论元学习，这个领域非常令人兴奋。她将主要探讨强化学习中的元学习，其中元学习与多任务学习或迁移学习密切相关。为了重新审视我们在快速学习方面的讨论内容，我们需要考虑数据的重要性，尤其在医疗保健、教育和客户领域。我收到了在 Pinterest 上发表演讲的邀请，他们显然也对这些想法感兴趣。我们一直在讨论两种不同的情境：Bandits（赌博机制）和马尔可夫决策过程，以及用于评估算法性能以及所需数据量的各种框架。需要指出的是，在课程的这一部分中，我们并没有深入探讨计算复杂性，但一些框架具有较高的灵活性，可以轻松扩展到多项式样本复杂性。通常来说，这些框架可以满足计算复杂性的要求。接下来，让我们继续讨论马尔可夫决策过程。

我们之前讨论了在强盗领域建立专业知识的过程，涉及评估强盗算法优劣的方法以及实现方法。我们详细探讨了数学上的遗憾，即我们的行动与强盗表现之间的差异，强调强盗领域对遗憾的重视。我们还介绍了两种降低遗憾的技术：不确定性下的乐观和汤普森采样。借助贝叶斯模型，我们可以明确表示在拉动手臂或采取行动时可能发生的情况，并利用这些信息。
在上次开始讨论马尔可夫决策过程时，我们发现其中很多想法都与强盗问题相关，但在许多方面更具挑战性。我们正在探讨一些可能正确的概念，特别是基于模型的区间估计。

我提到的可能是一个大致正确的算法。那么让我们回想一下，PAC 是什么意思？有些人可能在机器学习中已经听说过。因此，给定输入、epsilon 和 delta 的强化学习算法可能是近似正确的。这里，epsilon 表示我们希望达到的最佳程度，而 delta 表示我们希望这种情况发生的概率。除了少数步骤之外，所有步骤都受到 epsilon 和 delta 的影响。我们的算法将选择一个动作，该动作的 Q 值...

真正的最佳 Q 值大于或等于V，即将其表示为V。该状态的最佳可能值，减去 epsilon [噪音] 的概率至少为1 - delta。今天我对常数有些松散，有时这将是 1 - 2 delta，有时前面可能有一个小常数，也可能需要保持一定量。我提到这一点是为了让您记住这一点，可能会有一些小的常数，可能是两个或四个。但重要的是，您已经接近 - 非常接近最优解，除了可能有一个常数因子之外，其中N是S大小的多项式函数。您的状态空间大小是您的动作空间的大小，再加上伽马，epsilon和delta。是的，1 超过 epsilon [听不清]。

超过epsilon，是的，很好。问题是，这些是否取决于epsilon或delta，或者1除以epsilon或1除以delta？是的，在所有的表达式中，它们最终将成为1超过epsilon和1超过delta。所以你也可以这样写。因为本质上N会更大，如果你想更准确的话。因此，随着epsilon变小，您将需要更多的数据才能更加准确。如果您想更加确定，您就需要扩大这个增量。好的，我只是想在我们继续之前简要地将其与遗憾进行对比，因为在强盗场景中我们主要考虑的是遗憾。但很高兴思考一下PAC和遗憾之间的区别，特别是在在线学习情景下。这意味着我们的算法在MDP中进行在线学习，并且它将永远学习。这就是遗憾告诉你的。所以遗憾在说，如果这个数足够大的话，你能看到遗憾吗？因此，遗憾的意思是，假设我们从状态S_0开始。

遗憾的是，如果从一开始你就做了最好的选择会怎样呢？你的生活会变得多么美好。如果你赢了，比如第一次参加填色比赛，那将会让你进入哈佛并最终进入最高法院，这会是多么棒啊。但如果你没参加那次填色比赛，你会走另一条路。所以本来你可能会得到+10，但现在你却只得到了0。你去了另一个州，根据马尔可夫决策过程来看，你现在已经不再是赢得填色比赛的那个人了。然后，你知道，[噪音]你的人生轨迹就被不可逆转地破坏了。在这种情况下，[笑声]你不仅是会根据你的行动被判定，还会被判定基于最优策略下的状态分配。因此，你会被一直审视，如果我总是从开始就做出最佳选择，我会取得什么成就呢？我总是参加填色比赛。我会一直去哈佛。我不会错过那次斯坦福大学的机会，并且我会起诉最高法院。但是，一旦你做出不同的选择，你将面临...

AI智能体可能会进入不同的州。 但你会看到这些差异。 因此，评判将基于你最终所处的状态分布和在那里获得的奖励，以及最佳政策下的状态分布和相应奖励来进行。 因此，是一个相当苛刻的标准。它指出你总是需要接受评判，比如，如果你永远做出最佳决定。PAC在某些方面更为合理。 PAC根据你的算法得出的状态分布来评判你。它将采取对你所在州来说接近最佳的行动。PAC会说，好的，你从这里开始。你没有参加填色比赛，结果去了那里。这太糟糕了。考虑到当时的情况，你可以选择参加下一场填色比赛，或者不参加。我将根据当地的差距来进行评判。我永远只会根据我算法所达到的状态分布的最佳程度来判断。

因此，PAC相比于后悔会给予的遗憾要小得多。与遗憾相比，差异更微小且稍显消极。想象一下你置身于一个非常严格的马尔可夫决策过程（MDP），你必须做出第一个正确的动作才能抵达某个美好的土地，你会发现自己在这个可怕的网格世界里辛苦奋斗。在这种情况下，后悔更容易与你的表现进行比较——即使你最初做出了错误的选择，PAC也会认为你一直在尽力做出对当前位置最有利的事情。考虑到最终你可能处于极差的状态，你已经接近最佳状态了。从某种程度上来看，你可以将PAC视为充分利用当前环境，而后悔则总是关注你是否从一开始就做出了正确的决定。

谈到情景式MDP？这个问题很有意思。问题是情景式MDP是否也适用于偶发性MDP？在情景式MDP中，我们进行了h个步骤或有限步骤后进行重置。在偶发性MDP中，遗憾和PAC更接近，因为通常情况下，...（接下文缺失）

PAC（Probably Approximately Correct）保证在这种情况下是可以实现的。今天我不会过多讨论这些，我们要讨论的是初始状态。你需要观察像S0的V星和S0的Q星这样的指标，例如你所采取的行动或者所遵循的策略。在这些情况下，它们会更为接近，因为你总是从初始状态进行评估，你可以进行重置。但在在线学习中，比如强化学习，它们可能有很大的不同，因为状态分布可能会有很大差异。是的。关于C1和C2的来源，您能否详细解释一下？因为我觉得它们不是根据任何类似的给定参数而设置的。哦是的，我刚刚提到了C1和C2的问题。我将非常放松地遵守常量。大多数此类后悔保证通常是数量级的，所以N是S到6的函数还是S到4的函数，我们通常不会过于担心常数。因此，我提到这些是为了说明，一些不同的理论界限会有不同的常数。

今天我们将忽略这些细节，只是为了让您知道那里可能存在一些固定值。例如，这可能是1-2的增量，而不是1-增量。嗯，这就是后悔和PAC之间的区别之一。现在让我们回到这个算法。我想强调一下，所以今天晚些时候我们将讨论一些关于泛化的问题。但我想了解一下，我们如何开始思考一个算法是否符合PAC。我之前告诉过你这个算法符合PAC。但现在我想谈谈为什么它符合PAC，以及一个算法成为PAC意味着什么，以及我们是否可以使用通用模板来展示一个算法符合PAC。我现在要讨论的所有内容都涉及表格设置，我们可以在其中将值函数写成表格。稍后我们将讨论这些想法如何扩展。我们特别选择了一种算法，即奖励奖金算法。所以这里有一个不错的小奖励。因为这会更容易扩展到无模型的情况。现在，当我们查看MBIE-EB时，我想强调的一点是，

当我们回顾并更新我们对这个过程的记忆时，我们实际上在进行最大似然估计，也就是计算每个状态-动作对的转换模型和奖励模型的经验估计值，并进行计数和除法操作。因此，我们观察我们在某个状态-动作对中的频次，然后推断接下来可能发生的状态，并利用这些信息构建经验模型。对于奖励结构，我们也采取类似的方法。接着我们需要决定如何采取行动，我们使用这些经验模型，可以将其视为在我们的奖励模型中稍作调整。因此，我们将其视为将经验奖励加上一个奖励项。您也可以将其看作是 "R hat" 加上这个奖励项，即 SA 的 R hat 加上这个奖励项。这样，我们定义了一个新的马尔科夫决策过程（MDP）。在这个新的MDP中，转换模型是 T hat，奖励模型是 R hat prime，这是经验奖励加上这个奖励项的结果。

虽然它不是一个真正的马尔可夫决策过程（MDP），但这是一个我们可以解决并尝试计算其最优值的MDP。这就是我们在这里所做的，我们构建这种乐观的MDP，其中我们使用经验过渡模型。然后我们使用奖励模型，在我们不常访问的地方提供非常大的奖励。受噪音干扰。关键问题之一是要保持多么乐观。我们做了很多工作，试图使事情变得或多或少乐观。如果有时间，我会向您展示一些其他幻灯片，介绍该领域的一些最新进展。

之前我们讨论过MBIE-EB PAC。现在，让我们探讨制作PAC的充分条件，以及MBIE-EB是如何满足这些条件的。这里我要谈的条件基本上都是从这篇论文中推导出来的，稍作修改，不过还是需要这篇文章的基础来阐述MBIE-EB。

因此，事情肯定会有所不同。但从 30,000 英尺的高度来看，这基本上是以为 MBIE 是 a-MBIE-EB 是 PAC 算法的合理方式。好的，让我们深入探讨一下。让我们将这个概念表达清晰，也许我可以将其写在黑板上，因为一次性看到所有这些对理解有帮助。好的。那么，构成 PAC 的充分条件是什么呢？我清楚记得当我在博士阶段开始研究 PAC 证明时，这篇论文对我非常有帮助。那么，PAC 的充分条件是什么？这个理论相当优美。尽管对理论不感兴趣的人来说，我认为理解这个概念也是有益的，因为它可以直观地让人们了解算法需要具备哪些属性才能高效，以及需要什么类型的属性。

属性足以让你的算法高效吗？好的。首先要乐观。需要强调的是，这并不是成为PAC（概率近似正确）条件的唯一要素，但这里有一组条件。所以要保持乐观态度。在这里，乐观是指你所使用的计算值。因此，这是因为这是 s_t，这是 a_t。因此，这就是我们计算的实际值，就像我们从MBIE-EB计算出的乐观值一样。这就是算法的计算值。必须大于或等于状态-动作对的真实最优值减去所有时间步长上的 epsilon。因此，它指出，每当我们进行MBIE-EB计算时，当我们采用经验模型并添加奖励奖金时，我们必须选择一个奖励奖金，使得无论我们为结果状态-动作对计算出什么，减去一些 epsilon 在所有时间步上都是乐观的。

所有这些只需要以高概率成立，但我会像这样写出来。所以这是第一个条件。第二个条件有点微妙，但我稍后会详细介绍具体情况。所以第二件事就是所谓的准确性。我会先写下准确性。所以第一件事就是，你需要对所有时间步保持乐观。第二件事是，你需要准确。这意味着V_t。这又是算法计算的内容。你的算法会计算这个。因此，这就是MBI使用您的乐观模型计算的结果 - 一个奇怪的MDP的V pi t。我稍后会告诉你关于那个奇怪的MDP的事情。不是我刚才说的MDP，而是美好乐观的MDP。这不是真正的MDP。它是一个介于两者之间的MDP。强化学习中的这种类型的技巧在我们构建的地方经常出现，你可能已经在几个证明中看到了。

我们会在两个不同的马尔可夫决策过程之间添加和减去相同的项。类似地，我们将构建一个既乐观又类似真实MDP的MDP，虽然在现实世界中这样的情况并不存在，但我们将其作为分析工具。因此，我很快就会揭示这个方法。 [噪音] 有不同的方式来定义它，但它指的是与你乐观的[噪音] MDP和你真正关心的MDP[噪音]密切相关的内容。所以，这里的π(t)代表您在时刻t实际执行的策略。您计算的值必须在epsilon范围内接近这种奇怪的混合MDP[噪音]，即必须非常接近另一个MDP。这么做的原因在于，我们将能够借助它来试图衡量我们与真实MDP的偏离程度。那么为什么我们需要这个呢？我们需要这个，因为

只要我们将价值观设定得非常高且不断更新，就能保持乐观情绪。这看起来很好，但需要利用现有信息，这样我们的行为才能接近最佳状态。如果情况确实很糟糕，我们也不应永远保持乐观。准确度条件是指，当我们获取关于某些状态-动作对的充分信息，或这些信息需要接近真实值（有噪声的值）时。然后，第三个因素是有限的学习复杂性，这包含两个方面，即更新总数和队列更新总数。因此，在 Model-Based Interval Estimation with Exploration Bonus (MBIE-EB) 中，我们会更新我们的状态-动作值。

我们将重新运行这一乐观的 Q 值迭代。 重要的是，我们必须确保总运行次数有限，以控制对未知状态-动作对的访问次数。我将在一秒内详细解释这一点。因此，我们首先对所有状态-动作对进行分类，在表格环境中进行这种分类是合理的。最终，我们会将每个状态-动作对分类为已知或未知。需要注意的是，对未知状态-动作对的访问次数必须受到某个函数的限制，这个函数取决于 epsilon 和 delta。因此，您不能无限次地更新队列，也不能无限次地访问未知状态-动作对，正如您的算法所不能做到的那样。这些是您的算法必须满足的条件。因此，只有满足了所有这些条件，您的算法才能被称作是PAC。因此，如果第1到第3点都得到满足，那么...

这将是 epsilon 阶 epsilon 最优 [噪声]，但是我将在这里写下来，这样您就可以了解这些类型的边界可以做什么看起来像且除了 N 之外的所有都等于阶数。这个界限是样本复杂度除以 epsilon 乘以 1 - gamma 平方乘以一些对数项。所以本质上，这就是说，如果你能对我没有告诉过你的一些奇怪的 MDP 保持乐观和准确，并且如果你的队列更新总数和访问未知状态-动作对的次数（我经常这样做）还没有告诉你我们到底是如何定义的，如果这是有界的，那么你就会成为 PAC。除了作为此函数缩放的数字之外，您将在所有时间步长上接近最佳，该数字通常也是状态空间和动作空间大小的函数。这也是 epsilon 为 1 - delta 时 [NOISE] 的函数。所以这是一种模板。

因此，如果您能够证明您的算法符合这些属性，那么您就可以证明它是PAC的。好的。那么MBIE-EB是如何满足这些属性的呢？嗯，我们首先需要说明的是MBIE-EB是乐观的。是的，没错。啊，所以第一个问题是关于三个元素，第一和第二部分，它们是否拥有相同的边界，还是有两个不同大小的独立边界？好问题，他在问的是，您是指那里的epsilon吗？Q-更新的总数与您访问其他状态的总次数。嗯，好问题，所以对于第三部分，随着Q的总更新次数，您访问状态-动作对的次数也在增加，嗯，无论何时访问到另一个状态，它们本质上都会高度相关 - 动作对，然后您就可以进行一次Q更新。对于一和二，epsilon是相同的吗？是的，没错。好问题。所以我认为您想要问的是这个。在一和二中，问题是，epsilon是否相同呢？

是的，它们是一样的。 在1、2和3中，Epsilon都一致。 因此，设计算法时，1、2和3都必须相同。 常数可能并不重要，它可以是负数1。 在某些情况下，可在Epsilon之前添加一个常数，只需小心处理即可。 因此我们在这里将它们放置在一起，对吗？ 到处都是相同的Epsilon。 很好。 那么让我们先来谈谈为什么MBIE-EB备受瞩目。 实际上，我们可以把这个放上去吗？ 我觉得这样会更好。 所以我要重新命名MBIE的，奖金术语，这样你就可以看看它是什么样子的。 好的。 我认为这会在一秒钟内上升，然后只需要提醒我们更新的目的是什么。 因此，在MBIE-EB中，我们有一个状态-动作对，我们得到了该状态-动作对的经验奖励，再加上我们的奖金，除以n(s,a)。

没问题，好的，太好了。嗯，我看到至少有一个人在点头。所以这就是我们的乐观奖励。您可以这样称呼它，比如说R波形符。这是我们乐观的回报。再加上s'的总和。这就是我们的经验转换模型。在s'给定s',a'情况下，Q波形符对a'的最大值。好的。这将是我们可以进行的备份方式。就像一个行李员备份，有着我们乐观的奖励奖金。只是为了提醒自己，beta仍然被定义为1除以1 - gamma，再乘上二分之一乘以log2(S、A、M)再除以delta的平方根。好的。因此，在这种情况下，我们想要展示的内容是，我们无需考虑已知或未知的状态-动作对。我们要证明，

在计算这个值时，它代表了真实Q值的上限，最大可以达到epsilon。因此，我们希望能展现出最乐观的情况。现在我们要讨论的是，当贝塔值足够大作为奖励时，执行这个程序我们将保持乐观。好的，那么让我们逐步来理解这个问题。好的，那么我们应该如何来展示这一点呢？让我们考虑一个特定的状态-动作对。考虑一个状态动作对吧。对于s,a，让我们思考一下我们访问了n(s,a)次。

R帽和T，好吗？接下来我们将会放弃我们的数据。这就像说，第一次访问这特定的状态-动作对m次后，你可以使用这些数据来尝试计算一个经验模型，在你有m次计数之前你可以使用这个经验模型，但之后你将不再进行更新。我会在这里做一个注释，即你可能会想知道为什么我们要这样做？特别是，Tom Mitchell是机器学习的创始人之一，他提出了机器学习整个学科的理念。关键在于看待代理学习的基础，以及我们设计的算法如何随着更多数据而不断改进。在某种程度上，这违背了这一点，因为即使你有上万亿个状态-动作对的示例，这些数据肯定会让你的经验模型变得更好，但我们会抛弃掉所有这些数据。只是为了让您明白我们为什么这样做或为什么早期的分析会这样做，我们之所以这样做是为了设定高概率界限。

这个想法实际上是关于高概率边界的概念，类似于我们对强盗问题的观察，它会设定一种置信边界，并在一定程度上确保我们的估计，例如判断 Transformer 模型是否接近真实值。这些边界在很大程度上是成立的。那么，谁见过不同事物之间的联合边界呢？有一些人见过，但大多数人没有。因此，并集边界是一种方法，可以确保如果您有许多不同事件，那么所有这些事件以高概率成立，整体事件也以高概率成立。这就是为什么我们在这里只使用有限数量的数据的原因。理论上来说，这是令人不满意的，因为你应该能够使用更多的数据，算法在使用更多数据时应该表现得更好。根据经验，我们使用了所有的数据。在过去几年中，令人满意的一点是，我的学生托尔·拉铁摩尔（Tor Latimore）是强盗书的作者之一，他证明了你可以消除这种限制。你无法永远通过使用比联合边界更聪明的东西来持续使用更多数据。

但无论如何，今天我们将会执行 - 我们会这样做。所以我们会限制使用最多m个样本。现在，让我们考虑n（s，a）。

因此，每个人都会期待有关状态（s）和动作（a）的真实Q值，因为我刚刚定义了这个。 目前我们不需要知道V值是什么。 我们只是分析这些样本会发生什么。 因此，如果我们将样本定义为r，我们看到的实际奖励，我们看到的真实下一个状态。 总的来说，这实际上就是s和a的Q值。 因此，如果这是s和a的Q值，我们可以考虑需要多少样本才能获得对Q值的良好近似，或者我们的平均值B与真实经验值的平均值之间有多大差距（X_i与Q值平均值的差值）？我们可以使用Hoeffding或其他类型的偏差界限来做到这一点，有点类似于我们在强盗问题中所看到的情况。 因此，对于强盗问题，我们研究了奖励分布，如果有限数量的样本，那么它与真实平均奖励相差多远？

在这种情况下，我们面临离真实 Q 值有多远的问题，特别是当你只有有限数量的下一个状态样本和收到的奖励时。在这种情况下，我们需要注意一些技术细节，因为我们收集的数据取决于历史记录。这使马尔可夫决策过程变得更具挑战性，因为你所收集的数据取决于你的算法。因此，你会倾向于获得更多你认为是好的状态-动作对样本，而更少你认为是坏的状态-动作对样本。这导致了数据的耦合性。整个数据分布并非真正独立同分布，而取决于我们如何对状态-动作对进行采样。尽管下一个状态和奖励是独立同分布的，因为它们是马尔可夫的。因此，我们需要小心处理这些情况，但基本上可以使用类似Hoeffding不等式的方法来表示状态-动作对的 Q 值。

式子中缺少了一些内容，无法准确理解你的意思，请提供更多上下文或者完整的信息。

因此，它只是在表明这种可能性是存在的。当你拥有更多样本时，与真实 Q 值相差很远的可能性就很小，对吧？所以，我们可以将这个结果代入回去，我们可以说这意味着，如果我们考虑所有这些的并集，嗯，让我再表达一下，这就是 X_i 实际的情况。因此，X_i，如果你说数字1超过了 n(s,a)，

因此，这个概念应该与前面提到的Q波形符非常相似。在经验-经验转换中，我们使用经验奖励。不同之处在于，这里我们使用Q星号，而之前我们使用的是Q波形符。是的。这意味着，当你有大量样本时，你可以限制我们正在做的事情和Q星号之间的差异。因此，我们对于状态s的R_hat，状态s_prime的转移加上γ和，经验转移模型，状态s、动作a、状态s_prime的V星号，减去Q星号(s,a)大于或等于负的β除以根号n(s,a)。好的？这适用于所有的T、s和a。好的。所以我们已经应用了Hoeffding，现在我们可以继续进行。

经验奖励、经验转换模型以及如何将最佳 Q 与 Q_star 连接起来是我们当前讨论的主题。现在，我们的目标是将这些与我们这里的方程进行比较。我们所做的就是不断重复这个过程，直到收敛为止。这就是我们在 MBIE-EB 中实际执行的操作。我们使用经验转换模型，结合经验奖励，加入奖励值进行迭代，直到收敛。我们希望将所得到的结果与 Q_star 进行比较。我们要证明得到的值将大于或等于 Q_star。为了证明这一点，我们将使用归纳法。

因此，我们的目标是获得在值迭代的第i次迭代中，状态s和动作a的Q_tilde。 因此我们使用方程1。 方程1就在这里。 如果我们选择Q_tilde的最大动作，我们会让V_tilde_i等于-s的值。对于每个状态和动作对，我们假设进行乐观初始化，将状态s的Q_tilde初始化为0，a等于1除以1减去gamma，根据定义，它比Q_star要好，至少与Q_star一样好。 这是我们的基本情况。 再说一遍，我们的目标是什么？ 我们试图对MBIE-EB表现出积极态度。 我们想说的是，如果我们使用MBIE-EB执行此程序，我们会非常乐观。 我们将通过归纳法证明这一点，这是我们的基本情况。

让我们开始。我们乐观地初始化我们的估计 Q_tilde。现在假设这成立。因此，我们假设状态 s 和动作 a 对应的 Q_tilde_i 将会——嗯，我们假设前一个时间步的状态 s 和动作 a 的 Q_tilde_i 大于等于这一时间步的状态 s 和动作 a 的Q_star。好的？所以，我们要——好吧。现在让我们写出 Q_i + 1。Q_tilde_i + 1 将等于状态 s 和动作 a 对应的经验奖励，再加上状态 s_prime 对应的Q_tilde乘以经验折扣因子 gamma，或者乘以动作 a 的经验模型，再乘以s_prime的值函数V_tilde，最后再加上动作 a 的置信范围的平方根乘以n的权重系数beta（s，a）。好的。这与方程1是相同的。

根据定义，我们可以说，这个值将大于或等于 s、a 加上 gamma 乘以 s_prime（我们的经验转移模型）的总奖励 R_hat 乘以真实价值 V_star。这是因为这是我们的归纳假设。我们假设这在上一个时间点是成立的。然后，加上 beta，再除以 n(s, a) 的平方根。

减去β， 平方根n(s, a)。这样说对吗？因此，如果我们将其代入这里，我们可以说这等于Q_star(s, a)，减去n(s, a)的平方。

因为我们采用了乐观初始化策略，我们对所有生成的 Q-Q_hat 和 Q_star 都持乐观态度。这证明了乐观的有效性。有人对这个证明有疑问吗？好的，这就是乐观策略的证明。另一个关键部分，我不会详细展开，但会简要讨论。另一个非常重要的部分是准确性。我的意思是，准确性是至关重要的。确保最终能够变得准确是非常关键的。因此，这部分的直觉是，你可以考虑定义几个不同的状态，将事物定义为已知或未知。有人需要我继续写下去吗？还是大家都已写完了？如果需要请举手。

在许多有限状态-动作对的 PAC（概率近似正确）证明中，存在这种已知的概念。这意味着什么呢？所谓的状态-动作对是指我们拥有足够数据，并且对真实模型的估计接近的对。换句话说，已知的状态-动作对是在给定状态和动作时，估计的奖励值R和状态转移概率T接近真实的奖励和状态转移概率。直观上来说，根据Hoeffding不等式等，随着获取特定状态-动作对数据的增加，可以看到...（接下来内容未提供，可以继续翻译）。

你的平均估计将收敛于真实平均值，而你的 Transformer 模型也会收敛于传统模型。我们所做的就是在沙滩上画一条线，询问“我们何时拥有足够的数据来满足我们对经验估计的置信度-动作对？” 我们可以控制经验估计与真实模型之间的接近程度。当接近程度足够高时，我们就知道如何计算出接近最优策略的方法。因此，如果一切皆已明了，如果对所有 (s, a) 进行采样，然后最小化动作的值，我们就可以近似计算最优策略。

在期中考试中，检查所有细节很重要，这被称为模拟引理。如果你的模型接近真实的转换模型，奖励模型也接近真实的奖励模型，那么使用它们计算价值函数时，得到的价值函数也会收敛。这意味着模型中的误差会传播到价值函数和政策中。所以，你可以通过传播经验预测误差来控制误差。这对于小控制误差至关重要，而好的预测误差将带来小的控制误差。已知的状态-动作对只是提供了一种量化方法，帮助你评估和管理这些误差。

你需要什么水平才能被认为是足够好，所需的“足够好”水平取决于你所设定的epsilon值。因此，一个非常重要的观念是考虑这些已知的状态-动作对，并思考它们如何帮助我们在定义一些替代MDP方面取得进展。 因此，在这种情况下，我将删除它。[噪音]这表明——所以你知道如何调整证明的某些部分，嗯，我们将继续讨论。我不会详细阐述全部内容，但我很乐意在线下探讨这些。嗯，[噪音]因此，我们还可以定义另外两种类型的MDP。我们可以将MDP M称为素数，它是相等的——这是一个MDP，对于这个(s,a)

动态 - 它的动态和奖励模型是相等的，在已知集合中 (s, a)

这是我们最终使用的MDP，另一个类似的是M-hat prime，它等同于M-hat的MDP。这只是对K的经验-经验估计，对所有其他的模型都等同于M。在已知的集合上，我们使用经验估计而不是真实模型的MDP，然后对所有其他模型使用M波形符。我们可以通过计算我们的Q波形符与其他值之间的差异来衡量其价值，基本上可以进行一系列的不等式推导。

我们可以将从执行政策中获得的价值与现实世界中获得的价值联系起来，并将其与最佳政策联系起来。因此，这些工具能够帮助我们验证我们所了解的状态-动作对的准确性。是的，这里提到的M帽子和M波形符就是这个意思。M帽子代表经验估计，比如仅使用奖励计数和转换模型计数。而M波形符则是经验估计加上奖励。因此，在这两种情况下，转换模型可能是相同的，但会考虑奖励。这种方法直观地帮助我们量化准确性。最后，需要注意的是，在有限的学习复杂性下，这些证明如何起作用是至关重要的。我们必须确保我们不断更新并适应新信息。

我们的 Q 函数且不会反复面对未知的状态-动作对。因此，最后一部分是关于如何证明3，即有界学习复杂度。在这方面的直觉有点像普遍的乐观主义。乐观主义的直觉是，如果你认为世界是美好的，要么世界真的美好，那么你会很少感到遗憾。就 PAC 而言，这意味着，如果你认为世界是美好的并且它确实很美好，那就相当于没有犯错误。这就像选择真正好的动作一样。这样，你就不会遭受比 ε 更糟糕的决策。然后我们希望能够说，当你不犯错或者犯错时选择一个不佳的动作时，这种情况发生的次数是有界的，这就是这个概念的含义。这里的关键思想是鸽巢原理。因此，这个概念是，如果你想要，你不一定需要 MDP 环境，

如果你考虑将你的经验划分成不同情境，每一集中可能会遇到未知的状态-动作对。在这种情况下，我们可以思考一下到达未知状态-动作对的概率是多少。这里的未知状态-动作对指的是我们没有很好的模型，可能只有一次访问过或其他情况。在 T 个步骤内到达未知状态-动作对的概率是多少呢？在这种情况下，如果到达未知状态的概率很低，那么我们接近最优。这意味着如果你不太可能遇到你的模型不完善的情况，你就接近最优。反之，如果到达未知状态的概率很高，这种情况就不太可能发生。如果概率很高，你需要继续探索。

这种情况不会发生太多次。因为如果这个值很大，那就表示你很可能会遇到一个未知的状态-动作对。请记住，我之前说过针对每个状态-动作对，你只能更新它最多 m 次。根据鸽巢定理，这种概率不可能持续很久。基本上，你会拥有一个关于状态数量、动作数量和 m 的函数，这个函数比那个值大。但这也意味着你需要能够访问每个状态-动作对 m 次。因此，这就是我们可能犯错的极限。当我们处理时，我们可能会遇到未知的情况。嗯，这可能需要我们采取更多步骤，在这个过程中可能会做出错误决定。但基本上，对于每个状态-动作对来说，在 m 步中事情只能是未知的，这意味着这里的概率必须是有界的。所以最终，一切都必须是已知的，否则你就必须选择接近最佳的行动。这使我们能够证明，

哦，事情属于 PAC。它已经被限定住了。我们会做出有限数量的错误决策。因此，我们也不会探索到 状态-动作空间中未知的任何部分。因此，我们所达到的一切，我们都有很好的模型，我们正在利用这些模型做出正确的决策，或者正在实现一些目标，然后在这种情况下，我们正在获取信息，因为我们正在对它的内容进行新的观察，就像处于状态-动作对中一样，我们只能得到一些内容，某些方面只能保持未知，直到我们得到一些数据。你介意继续往下看吗？好的。这让我们大致了解了为什么MBIE-EB符合PAC，以及为什么很多，嗯，您用来解释事物的证明类型是PAC。我认为，其中的关键思想实际上是乐观、准确性的概念，以及通过迅速缩小这些置信区间来取得进展的能力，不总是陷入徘徊的能力。

好的。让我们回到贝叶斯原理。PAC MDP 部分就到此为止了。这个领域最近有很多令人兴奋的工作。我想简单地提一下这个问题。这是一种演示形式。今天，在伯克利，一位助教放弃了我们在一些共同项目上的合作。我想强调一下，在PAC和遗憾分析方面，已经取得了很多进展。在过去几年中，我的一些研究生和其他团队在这方面做了很多工作，我们现在也在尝试进行问题相关的分析，这意味着如果算法具有更多结构，我们就需要更少的数据来学习做出正确的决策。现在让我们转向贝叶斯主义，看看贝叶斯主义如何帮助我们。在贝叶斯强盗中，我们假设对于奖励如何分配有一些参数知识。因此，我们将考虑奖励的后验分布。

我们将使用这种算法来引导我们的探索。我们特别讨论了概率匹配的概念，即通过选择具有最佳概率的动作来实现目标，而汤普森采样被证明能够实现这一点。因此，在这些方法中，如果我们有共轭先验这将非常有帮助，因为这使我们能够根据我们观察到的数据以及之前对不同奖励的概率的先验进行分析，从而计算手臂的奖励后验概率。因此，我们举了伯努利的一个例子，也就是奖励只有0和1，而奖励的分布可以用beta分布来表示，其中beta分布可以看作是我们已经观察到的次数的计数。当我们获得+1时，beta表示我们观察到手臂为0的次数，每当我们观察到某个结果时，我们就会更新beta分布。这使我们能够定义汤普森采样来解决这个多臂老虎机问题，在这个算法中，在每个时间步中，我们首先对每个不同的臂进行采样以获得特定的奖励。

然后我们对奖励分布进行采样，计算这些奖励的期望值分布，然后采取相应行动。就像在刚才提到的例子中，我们会采样不同的奖励，并根据这些奖励来采取行动。至少在那个例子中，我们发现我们的方法比置信上限方法更快地找到了利用速度。在MDP的情况下，可以做类似的事情。在基于模型的强化学习的贝叶斯方法中，我们会在MDP模型上进行采样。那么这里有什么区别呢？这意味着我们将同时获得转移和奖励信息。所以我们的奖励问题看起来跟探索者问题非常相似。您还可以使用测试版本。如果你的奖励只有0和1的分布，你也可以使用贝塔分布和伯努利分布。所以这和传统探索者问题非常相似。虽然有一些不同点，但我们不会深入讨论所有可能的分布情况。

举例来说，若在表格领域，T可代表多项式，而该多项式的共轭先验是狄利克雷分布。共轭先验。因此，若您希望您的转换模型为多项式，即其他所有状态和行动的概率，那么其共轭先验将是狄利克雷分布，这有一个非常直观的描述，类似于贝塔分布。在贝塔分布中，您可以将其视为观察到1或0的次数。若观察狄利克雷分布，您可以将其视为达到接下来每个状态的次数，比如S1、S2、S3、S4。因此，狄利克雷分布将由每个状态之一的向量来参数化。同样，我们可以利用这种后验分布来更新我们的探索。因此，在这种情况下，我们将从后验中采样最优决策过程，然后进行求解。

因此，如果我们看一下这个算法的外观，它看起来与多臂赌博机的汤普森采样非常相似，除了现在我们需要开始时定义先验的动态和奖励模型，对每个状态-动作对。因此，这是一个表格。我们假设有一组有限的状态 S 和动作 A。我们可以将它写下来。对于每个状态-动作对的动态和奖励模型，我们都有一个分布，然后根据这些分布采样出一个马尔可夫决策过程（MDP）。因此，对于每个状态-动作对，您需要对动态和奖励模型进行采样。一旦您得到了一个MDP，您就可以计算其最优值。这显然比我们对赌博机所做的计算更加密集，但是这确实是一个合理的方法。然后，您基于采样出的MDP上的Q值选择最佳行动。因此，这被称为MDP的汤普森抽样。它还实现了根据经验进行概率匹配。

大家通常会对大语言模型的表现相当满意，就像汤普森采样对待强盗一样有效。我认为这就是我对MDP和汤普森采样的所有内容了。关于这方面已经有很多不同工作，需要强调一些斯坦福大学的研究人员做得很出色。Ben Van Roy及其团队在这方面做了很多工作，有时被称为MDP的后验采样。人们也喜欢他之前的一些学生，比如丹·鲁索和伊恩·奥斯班德，现在伊恩在Deepmind，而丹·鲁索在纽约大学。他们在非表格MDP领域取得了很好的成就。你听到了吗？总结起来就是针对非表格MDP？是的，这是一个很好的问题。我们是否可以将这种方法推广到非表格MDP呢？是的，稍后我会讨论这个问题。虽然有点难，但是这就是我们的目标。好了，还有其他关于有限状态和行动场景的问题吗？

现在让我们讨论泛化。当然，我所说的都是针对有限的状态和动作空间，这在某种程度上是有限制的。如果考虑这些边界的情况，状态空间的大小以及行动空间都是多项式的。但如果状态空间S是无限的呢？就是说，我们可能会犯无数错误，这似乎有点令人沮丧。当状态空间是无限的，或者类似像素空间这样巨大时，最初的框架是否有用就不太清楚了。尽管如此，尽管这个框架很好，我们希望能够概括这些思想，但我们需要弄清楚如何以实际方式应用它们。目前，这是一个非常活跃的研究领域，在过去几年中涌现了许多不同的想法。值得强调的是，在理论方面，我们还有很长的路要走。当我们讨论函数逼近时，这一点显得尤为重要。

即使在函数逼近和类似Q学习闭环策略控制的算法中，我们发现一些基本算法缺乏良好的渐近收敛保证。如果我们连渐近保证都没有，那么在有限样本情况下很难获得真正优秀的效果。有限样本保证通常指的是保证我们所犯错误的数量是有限的。因此，我们对这方面的理论知识相对较少，这正是我团队目前正在研究的内容。还有其他一些团队也在积极地从事这方面的工作，但依然还有很多工作需要完成。近期出现了一些非常好的实证结果。那么，让我们首先讨论泛化和乐观主义，就像在非常庞大的状态空间中面对的不确定性下所持有的乐观主义。在假设一切都是非常庞大的情况下，我们可能需要对算法进行一些调整。首先，我们有计数器，用来追踪每个状态-动作对的计数，这个计数器是干什么的呢？因此，这不会无限增长，因为如果…

在一个状态空间为像素场景的环境中，你可能永远不会再次看到相同的像素组合。 有人知道如何将这一情况扩展到深度学习环境吗？ 在这种情况下，我们希望找到一种方法来量化我们对特定状态和行动的不确定性。 我们不希望仅基于原始计数来评估，因为这将导致永远都是不确定的情况。 我们是否可以使用类似有界值函数估计（VFA）的方法呢？ 建议是我们是否可以采取某种形式，比如VFA？是的，我们可以考虑尝试使用某种密度模型或其他方法来估计我们观察到了多少相关信息，或者我们在这个区域看到了多少相关状态。这样就能更好地处理噪音。是的，所以一定要使用某种嵌入方法。因此，在这种情况下，另一种可行的方法是对...（以下内容听不清）。

状态空间的压缩是一种常见操作，其中往往面临的挑战是压缩的有效性取决于你要做出的决策。因此，这种压缩通常是不稳定的，但这未必是坏事。有时你可能只想在一些状态之间不断切换，这些想法都是有价值的。通常来讲，我们需要某种方法来量化我们需要总结的不确定性，并且表示相似的状态。如果我们频繁地访问某个地区，那么在优化方面我们就不会得到太多好处。正如我所提到的，如果我们只能一次性地遇到某个事件，那么记录 s、a 和 s、a、s' 的频次就没有意义。另外，我想强调的是，我之前谈到的方法实际上是基于模型的，很多工作都是基于这种方法的。与此相比，在强化学习中，基于模型的深度学习方法的研究相对较少，而最近一年的工作也相对较少。我认为其中部分原因是在建模过程中所犯的错误在进行规划时会造成累积效应。我记得 David Silvers 第一次谈论这个问题的时候。

他最初在2014年的一次演讲中首次讨论这个问题，当时展示了基于模型的、令人印象深刻的仿真技术，而这对规划来说可能是令人困扰的。因此，误差必须被控制得非常严格，以确保误差很小，并且不能确定通过最大化预测损失来获得的表示始终是最佳规划结果。因此，我们希望能够将之前提出的类似MBIE-EB的想法转化为无模型的方法，并考虑一种在深度神经网络设置中编码不确定性的方法。因此，让我们考虑类似于Q学习的方法，比如深度Q学习，在这种情况下——我可能有点离题了——但可以设定一个目标。这个目标可以解决这个问题。回想一下通常的Q学习和Q目标，我们利用当前函数近似的最大值，或者利用当前目标函数近似的最大值。因此，受到MBIE-EB启发的一个想法就是简单地加入一个奖励条款。这可能是

在一个固定的目标下，当更新参数时，可以通过为特定的状态-动作对添加一个奖励项来进行Q学习更新或重新调整权重。这种方式可以帮助规划过程。在无模型环境或零模型设置中，当进行Q学习更新时，可以插入一个奖励项。这就是之前展示MBIE-EB算法的原因，因为这种奖励项更容易扩展到无模型的情况。现在的挑战是确定奖励项的数值，它应该反映出该状态-动作对未来奖励的不确定性。有许多不同的方法正在尝试在这方面取得进展。因此，Mark Bellemare等人考虑使用某种密度模型，试图明确估计您访问的状态或状态-动作对的密度模型。其他人则尝试基于散列的方法。

在某种程度上，这更类似于嵌入。尝试对状态空间进行散列，然后利用这些散列状态空间的计数来更新您的哈希函数随着时间的推移。这是伯克利大学的一些研究工作。因此，有不同的方法来量化这一点。另外，需要强调的是，这些奖励条款通常在访问时计算。例如，在查看 MBIE-EB 时，您可以稍后重新计算这些奖励。因此，如果您将某些内容存储在情景重播缓冲区中，您可能会希望随着时间的推移更新这些内容。因为如果您以后对奖励进行采样，则会从您的重播缓冲区获得下一个状态对，您可能需要调整奖励条款。因此，在尝试进行深度学习时会遇到许多微妙之处，但也确实取得了一些令人鼓舞的进展。举个例子，我们可以看到 Mark Bellemare 团队的初步研究成果，他们比较了《蒙特祖玛的复仇》，这被认为是最具挑战性的 Atari 游戏之一。在这个游戏中有许多不同的房间。如果您使用标准的 e-greedy DQN 智能体，则会发现不同的房间。

在这个案例中，可以看到在进行了 5000 万帧之后，嗯，表现非常糟糕。不过——只穿过了其中两个房间。但是，如果你使用这种探索奖励，效果会更好，所以通过策略性的改进会更有效。明白吗？所以我认为这凸显了进行这种战略探索的重要性。嗯，让我们简单来考虑一下汤普森采样。因此，在这种情况下，我们几年前提出的一个想法是，您可以对模型的表示和参数进行汤普森采样。我这么说是什么意思？我的意思是，如果你有一个非常大的状态空间，你可以折叠你的状态并进行动态状态聚合，并对可能的状态聚合进行采样，作为折叠状态空间的一种方法。嗯，汤普森采样也可以扩展到对表示进行采样。这种方法确实很有效，但并非适用于所有情况。当你希望真正扩大规模时，如果你想要进行零模型学习，这与我们之前见过的有些区别。因为在我们之前看到基于模型的方法之后，现在有点想要从可能的 Q 值中进行后验采样。目前这

我们还在摸索如何将这些想法清晰地表述出来。即使在使用表格的情况下，我们也没有取得很大进展。但这正是那些为深度学习环境努力工作的人所致力于做的事情。有几种不同的主要方法。其中一种叫做 bootstrap DQN，这个想法来自Ian Osband，他在这里攻读博士学位。他们提出了 bootstrap DQN 的方法，意图是在样本上使用bootstrap，这样就可以构建多个代理。这样，您不仅可以获得一个Q值，还可以获得一组C、Q值，然后可以采取乐观的行动。尽管效率不是非常高，但也可以提升一些性能。另一方面，我们另外尝试的一种方法是修复您的嵌入向量，然后运用线性回归。虽然这方法非常简单，但如果您采用贝叶斯线性回归，就能够引入不确定性的概念，实际上在许多情况下这可能会带来更好的性能表现。因此，您可以在深度神经网络上引入一些不确定性表示。但这个领域仍然在不断演进。

许多人正在思考各种不同类型的工作，我认为这个领域将继续发展，因为我们在探索和泛化方面仍然没有取得足够的进展。总的来说，在本节中我们已经做了很多理论上的讨论。你应该确保了解在强化学习中探索和利用之间的紧张关系是什么，为什么在其他类型的设置中不会出现这种情况。你应该能够定义这些不同类型的标准，以理解好的PAC（Probably Approximately Correct）或遗憾方面的算法意味着什么，并且能够将我们在课堂上详细讨论的算法类型映射到这些不同算法标准的形式。所以，当我提到在不确定性方法下的乐观态度对PAC有利还是对遗憾有利，或者两者兼而有之，你应该明白这对双方都有好处。因此，你应该能够从我们一直在做的事情中理解这种高水平，并且明白有许多真正令人兴奋的工作正在继续进行，包括定义新的绩效指标。下一次，我们将讨论元学习。谢谢。

首先，个人部分，然后是团体部分。你会被提前分配到小组，并且房间里的椅子上会有编号，这样你就知道自己该坐在哪里。流程是这样的：首先完成个人部分，提交后进行团体部分的另一次考试，然后一起讨论答案，达成一致后划掉答案，看是否正确，完成后把作业作为小组提交第一份。

有关后勤问题，我们将再次分成两组，不会推迟。是的，好的，很好的问题。是指我们是否要分配到两个不同的房间，是的，我们会再次这样操作。这将是你的任务。可能会与上次类似，但我会再确认一下。这次可能会有所不同，因为一些法学硕士学生将加入我们参与这次之前未曾参与的活动，反之亦然。所以有可能，特别是如果你正处于边缘，我们可能会安排你在不同的房间。我们会及时宣布这一点。

提醒一下，我们现在基本上已经完成所有任务，这是一个专注于项目的好机会。你会收到关于此的反馈。

关于大语言模型项目的信息和评分将在Piazza上签署。Piazza是一个很好的助教，您可以在办公时间内问问题。也欢迎您在任何时间询问项目问题。海报会议的时间将在原始时间上宣布。尽管这不是最佳选择，但我们能做的就是如此。我们将在决赛最后一天早上碰面。这是我们被分配的任务。还有人对此有疑问吗？谁参加了周一Chelsea的演讲？好的。对于那些没有看到Chelsea的演讲的人来说，它将会在线进行，这是关于元强化学习的一次非常出色的演讲。下周的测验将涵盖这部分内容，但相当简单，因为您还没有太多接触到这个概念。再次强调，这是复习测验，将覆盖整个课程内容。但内容将更容易，因为您没有通过作业进行实践。这将是一个多项选择题。我强烈建议您复习去年的测验。

我们将忽略未涵盖到的任何主题，不要看答案就继续。教育研究中一个重要的发现是，强制回忆对学习非常有效。强制回忆是一个很好的测试方法。因此，当你没有答案被提供时，你可以通过它来检查自己是否理解了知识，并查漏补缺。这可能会引发疑问。还有其他问题吗？好的，那我们开始吧。今天我们要讨论批量强化学习，特别是安全批量强化学习的定义。这是一个非常重要的主题，在我们的实验室中投入了大量工作。我要讨论的大部分内容是由我的博士后同事Phil Thomas进行的研究，他现在是麻省大学阿默斯特分校的教授。我们合作的工作以及其他相关研究也将作为重点介绍。让我们来看一个简单的案例。

让我们考虑一项科学实验。我们有两组人，分别称为A组和B组。首先，A组要解决一种特殊类型的分数问题，这是我们辅导系统中的一道题目。他们需要将分数相加，然后将总和降至最低。接着他们进行一些交叉乘法，最后接受考试。A组的平均分是95分。然后B组进行同样的活动，但步骤顺序相反，他们的平均得分是92分。现在问题来了，对于一个新的学生，我们该如何预测他的表现呢？为了回答这个问题，我们可能需要额外的信息。所以，请大声告诉我你认为需要了解的附加信息。现在，新学生入场了，你打算怎么做呢？如果我们的目标是在这次考试中取得好成绩，那么我们应该...

为了帮助新学生在考试中取得好成绩，我们可以提供以下顺序的活动，以最大限度地提高他们的可能性（假设他们已经学会了这些材料）：

首先，可以通过重新复习考试内容来测试新生的知识水平。这可以帮助他们回顾并强化已学知识。

其次，可以提供模拟考试或者练习题目，让新学生实际应用所学知识，以检验他们的理解和掌握程度。

接着，可以针对性地指导新生解决一些比较困难或容易混淆的考试题型，帮助他们提升解题能力。

最后，可以鼓励新生积极参与讨论或者小组学习，以便能够更深入地理解知识点，并从不同角度思考问题。

以上活动顺序的安排可以帮助新学生在考试中取得更好的成绩。

至于对话中关于人数、统计噪音、方差以及统计数据的讨论，可能是指在进行考试分析时，需要考虑的一些统计学知识，如平均值、中位数、方差等。通过了解这些数据指标，可以更全面地评估不同群体的表现和差异性。

你知道，关于分区的事。我正在思考更多的事情，探讨不同的方向。对，团队里的成员都分为不同的组。比如A组都是高年级的学生，B组则是低年级的学生等等。确切地说，也许A组是幼儿园的学生，也许B组是，你懂的，高年级的学生，哈哈，我们好像在倒退哈哈。对的。那么，这些组里都有哪些成员呢？另外，你可能会常常好奇谁是新加入的学生。这个新生究竟是幼儿园的学生还是高年级的学生呢？好的。因此，要准确回答这个问题，你确实需要了解很多额外信息，这就涉及到很多不同的挑战。其中一个挑战是，如果B组和A组不同，那么我们会面临这样一个基本问题，即审查数据。你无法确定如果A组有相同的序列，或者B组是否会有与A组相同的干预序列，会发生什么。这是一个基本挑战，你永远都无法知道。

如果我现在在哈佛，场景可能会是这样，只能观察所采取行动的结果。在强化学习中，我们已经看到许多这种情况，情况类似，我们有旧的关于决策序列的数据，因此需要实际的推理。另一个涉及的方面是对概括的需求。举个简单的例子，你可以将其看作是两个操作。在两个不同的问题中，每一个都可以看作是一次奖励。延迟奖励可能包括0奖励、再次0奖励，然后是测试分数奖励。所以这里只有两个动作，但状态空间有多大取决于我们对学生建模的需求。尽管我们在制定决策策略，但很快就会变得非常庞大。因此，我们需要一种形式的泛化能力，可以跨越状态或动作，或者两者同时，这样我们就无需进行大量实验，就可以找到对学生学习最有效的方法。

今天我们将讨论离线批量强化学习这个问题，也可以称之为离线学习。在这种情况下，我们已经拥有了一些已经收集好的数据，然后我们将利用这些数据来做出更好的决策。这类问题不仅仅存在于强化学习中，也广泛应用于许多不同领域，如经济学、统计学、流行病学等。它在维护领域中出现，比如决定采取哪些行动顺序来确保机器、汽车的最长运行时间。在医疗保健中，也会出现类似的问题，比如确定为患者提供哪些活动顺序可以最大限度地提高他们的结果，改善他们的生命质量。

在许多情况下，最有效的方法会因患者而异，这取决于他们的状况。目前我们面临的挑战之一是，在许多情况下我们依赖于旧数据，这是一个高风险的情境。比如，当涉及昂贵的核电设备或治疗严重疾病时，我们希望确保未来的决策是正确的。有时我们可能没有大量数据，但我们拥有的数据非常宝贵，我们希望能够做出最佳决策。因此，我们需要对未来的运作情况有一定信心。在部署之前，我们希望有对其性能有一个大致的了解。总的来说，我们需要一种良好的方法来估计，通过反事实推理考虑人们的健康状况或治疗方式对人的健康程度可能产生的影响。在说服医生实际采用该方法之前，我们需要对其性能有信心。

今天我想要探讨的是如何进行批量安全增强学习这一普遍问题。安全涵盖许多方面，我主要从信心的角度来考虑这个问题。这种能力意味着在我们部署某项技术之前，我们有清楚的了解它的性能。安全措施有不同形式，比如安全探索，确保避免犯错，也有风险敏感性，考虑到我们所面临的结果是确定的而非随机的，所以我们更关注整体分布而非平均值。然而，我们今天讨论的重点仍然是关于预期结果的信心。总的来说，我们希望能够对我们将部署在患者、核电站或其他高风险场景中的新决策政策充满信心，认为它比目前的政策更好。这种信心的建立是为了确保提供更高的保证。

在这些高风险领域中，我们最近在本季度早些时候看到了一些单调的改进。嗯。所以让我们简要谈论一些符号，有些你们可能很熟悉。我只是想确认一下我们都同意这一点，然后我将讨论一些可能尝试创建批量、安全强化学习算法的不同步骤。因此，虽然通常用π来表示策略，但我们将使用T或H来表示轨迹。我常用大D来表示我们可以访问的数据，比如电子病历系统或发电厂的数据等。在大部分情况下，我们假设我们知道行为策略是什么。所以，我们知道状态到动作的映射，或者被用于收集数据的历史到动作的映射。有人能给我举个例子吗？可能存在我们不知道行为策略的情况吗？是的，确切地说，我们可能执行某些动作而不了解问题本身。

在很多情况下，数据是由人类生成的，我们通常不清楚 pi_b 的具体含义。特别是在查看医疗健康数据时，我们经常无法确定 pi_b 的含义。例如，如果数据是由医生生成的，我们通常也不清楚 pi_b 的具体定义。虽然有一些指导方针存在，但我们通常无法获取人们实际使用的具体政策，即使获得了这些政策，他们可能也未必会将其描述为随机过程。举个例子，当有人以 0.5 的概率进入医生的办公室时，医生可能会以 0.5 的概率对待这个人。医生可能会用确定性的术语来思考这种情况，而不会使用“如果-那么”规则来加以描述。因此，在许多情况下，我们无法准确了解数据背后的含义。最近我们和其他研究人员在这方面进行了一些工作，我稍后可能会简要提到。请问，有谁能举个例子说明 pi_b 的合理性呢？可能是基于某些指导方针吗？

在某些情况下，难以理解的部分就像您需要算法来做出决策，或者有一套明确的指导方针。您是否会说出类似的话或者有所区别呢？略有不同。比如，如果您有一个有维护记录的发电厂，那么就像一个既定的维护计划，记录与计划相匹配，那么您基本上就有了一个可靠的计划。这是正确的。因此，在这些具有固定协议的实际情况下。我常思考另一个例子，就是您如何知道决策是由强化学习代理做出的，还是由监督学习代理做出的。如果您想到像谷歌这样提供广告服务的地方。我们确切地了解它是如何进行广告投放的，所有这些都必须有记录。有一个算法在做出这个决定。在许多情况下，我们可以访问用于生成算法自动机的代码，自动生成决策。在这种情况下，只要我们保存了它，我们就可以查找它。因此，我们通常的目标是考虑如何制定良好的政策，以及具有良好价值观的良好政策。所以，当我们考虑在某种设置中尝试进行安全批量强化学习时，我们就会面临着类似的挑战。

我们考虑如何获取旧数据。我们会将数据输入到一个黑匣子中，并根据这些数据制定出我们认为是最佳的政策。在这个过程中，我们会采用某种算法或转换，而不是直接与现实世界互动，从中做出决策并选取数据。该算法仅使用这些固定数据来制定策略。我们希望当我们将数据输入到我们的算法中时，该算法可以是随机的，输出的政策值可以表示为 A(D)。我们的算法 A 会输出一些策略，可能是确定性函数，也可能是随机函数。无论输出什么策略，我们希望它是好的，至少与行为政策产生的噪音一样好。这是第一个方程所描述的，表示当我们输入某些数据集时，算法输出的任何政策的价值。我们期望其与生成数据集的行为策略相当，不论价值是多少。

因此，这里讨论的是用于生成数据的策略的价值[噪音]。通常情况下，我们不会直接使用策略πb。但是，请问能否举个例子来说明我们如何学习这一点呢？给定一个数据集，这个数据集是根据某个特定策略生成的，即按照这个策略使用。就像动态规划或类似的小[听不清]事情一样。因此，我认为一个重要的观点是，你可以利用这些数据。虽然可能无法进行动态规划，因为你可能无法获取转移和奖励模型，但你可以尝试一些类似蒙特卡洛估计的方法，可以对奖励进行平均。假设在数据集中，你可以看到状态、动作和奖励。在这种情况下，你可以对它们进行平均处理，可以对所有轨迹进行平均。因此，我们可以得到一个估计，通过观察数据来估计pπb，让我们设想一下。

这是一个重要的情景问题。通过观察所有轨迹的总和等于1，即每个轨迹的回报，从根本上来说，这其实就是蒙特卡洛估计。因为您知道这些数据是根据某个策略生成的，因此您可以对其进行平均处理，以估计 π_b。但随后，我们需要一种方法来估计算法的价值函数 V 和输出策略 D。这意味着我们需要采取一些与当前策略不符的行为，因为通常我们希望找到比 π_b 更优的策略，这意味着要做出不同的决策。我想强调的是，有时您可能需要不仅仅比当前行为策略更好，而且需要表现得更好。

一般来说，考虑到实际生产系统，每当我们想要改变政策时，都会涉及时间、金钱和精力。要让医生改变他们的决策方式，或者想要改变发电厂的操作方式，通常都会带来管理成本。因此，通常情况下，您可能需要的不仅仅是比当前最佳技术更好，而是显著更好。

因此，相对于当前性能，我们总是可以添加一个增量，因为新的想法总是至少应该更好。简单来说，这个方程想要表达的是，当我们引入新的数据集时，希望模型输出的策略比当前策略更好的可能性很高。因此，这里的增量值会在0到1之间。接下来，我们将讨论如何实现这一目标。首先，我们要从对策略进行评估开始。在这个例子中，我们将快速回顾一下三个要点，然后逐步深入讨论它们。因此，我们可能需要做三件事来进行安全批量强化学习，针对我们正在研究的具体情况，每个要点都有许多变种，我们需要重新进行政策批量评估。我们需要能够访问旧数据，

然后用它来估计替代政策的效果如何。我们可能希望获得对其效果的置信界限。所以这可以让我们得到一些 VA(D)

这将需要我们进行优化，因为如果不这样做，通常我们不知道什么是好政策。因此，通常情况下，我们最终会评估许多不同的政策。这可以看作是第一部分，我们会获取历史数据，采取提议的政策，将其插入某种尚未讨论过的算法中，并对该政策的价值进行估计。我们将讨论如何利用该政策进行重要的采样。接着，我们会进行高置信度的政策评估和安全政策改进。为了获得置信区间，我们会看一下Hoffding不等式。我们之前就见过Hoffding不等式，当我们开始谈论探索时也提到过。因此，在审视高度自信和回想时，请联想到探索。在探索中，我们经常尝试量化对政策或其模型价值的不确定性，以便对其效果保持乐观态度，并利用其推动探索。

然而，我们也可以计算置信区间，即事情可能达到的最低水平。在部署政策之前，尝试确定政策的潜在表现水平将是非常有用的。我们可以使用霍夫丁不等式来实现这一点。最终，我们将能够进行诸如改进安全政策之类的行动。换句话说，当有人提供一些数据，问道："嘿，你能提供一个更好的政策吗？"时，是否存在一种算法能够提供比您部署时更优的策略，即以高概率提供更好的结果？或者该算法能够识别自身的局限性，回答："不，我无法提供更好的政策。" 因此，我认为拥有意识到自身局限性的算法也是非常重要的。目前，我们在实验室做了大量工作，让人们在使用这些算法，特别是在循环系统中时，能够理解算法输出的质量。因此，在某些情况下，如果数据非常有限，您可能无法确定如何进行改进。因此，在我之前提到的例子中，我们有两种不同的教学方法，以帮助人们理解算法是输出错误还是正确的。

提出了很好的观点，每种方式都有多少人。如果只有一个人尝试过其中任何一种，并且有人说：“你能明确地告诉我什么对未来的学生更好吗？”你应该说：“不”。[笑声]因为只有一个数据点，就像我们不可能从每组中的一个数据点中获得足够的数据，以便能够自信地说出将来我们应该如何教学生。因此，我认为，安全政策的改进需要我们能够明确什么时候我们可以有信心在未来部署更好的政策，什么时候我们不能。因此，我们将研究如何回答此类问题的管道。好的。那么我们先回去，去思考一下离政策的政策评估。所以，嗯，这样做的目的是得到一个无偏的离政策估计。因此，我们希望对替代决策政策的效果有一个很好的估计。所以我们现在有数据，这些数据是在某种策略下采样的，我们称之为行为策略 Pi 2。

我们现在手头有数据集 D，其中包含了一些轨迹样本，我们想要利用这些样本来评估替代政策的效果。虽然可以使用 Q 学习等方法来实现这一点，但我们希望尝试一种不同的方法，以获得更好的置信区间，并且获得一个无偏估计。在Q-learning中，我们知道它是一个离策略的方法。我们想要采取一种方法使我们仍然可以离开当前策略，但同时避免偏见，因为在当前情况下我们总是依赖有限的数据，不会进入渐进状态。因此，我们需要一个方法能在脱离当前策略的同时保持无偏估计，避免出现系统性的高估或低估。

如果我们假设从前收集的数据不再可用，那么我们需要重新考虑回报的概念。回报（G_t）是指在有限时间步数（例如一个序列）或者在整个过程中，根据某一策略获得的奖励总和。在这里，我们的策略的价值是指预期的折扣回报。因此，一个好处是，如果策略 Pi 2 是随机的，那么您可以使用您收集的来自行为策略的数据进行离策略评估，这对于Q学习非常重要。由于我们遵循蒙特卡洛类型的框架，您不需要模型，也不需要满足马尔可夫性质。这非常有益，因为最终我们会得到一个无偏的估计，而且它不依赖于马尔可夫性的假设。在许多情况下，马尔可夫性假设可能不成立，特别是当我们考虑到处于患者数据或其他情境中时，我们可能只是有一组恰好记录在数据集中的特征，因此无法确定系统是否满足马尔可夫性质。

在一些项目中，我们确实发现事实并非如此。 有时，如果你假设世界是马尔可夫，那么评估另一种教育学生方式的效果会非常糟糕。为什么这是一个棘手的问题呢？原因在于我们的分布不匹配。想象一下我们刚刚经历了一个只有两种状态的过程，我们尝试计算从状态S到下一个状态S'的概率。如果我们把这看作是高斯分布，处于π策略下的状态动作序列可能会与其他评估有所不同。一般来说，根据不同策略得到的奖励分布、状态、动作、奖励以及下一步的状态和动作都会形成不同的轨迹。

在这种情况下，tau 的分布不会看上去一样，这不会影响这两项保单的价值。具体来说，如果在两种策略下获得的状态和操作分布是一致的，那么它们的价值就是相同的。这种想法与模仿学习和状态特征匹配相关。在这里，我们讨论的不仅是状态和动作，而是完整的分布或者完整轨迹，因为我们没有做出马尔可夫假设，但基本思想是相同的。我们通常通过轨迹的概率以及该轨迹的价值或者奖励总和来定义价值。因此，如果两个分布是相同的，那么价值也是相同的。我们并不关心政策是否会有所不同，因为我们已经知道如何去估计价值。

这里的关键问题是不同情况下会有不同行动，因此需要根据政策进行调整。举个例子，可能你经常访问某一部分状态空间，但很少触及另一部分。现在，如果有一个替代政策，它在不同部分的出现频率会有所不同。这种思考方式可以帮助我们了解如何查看现有数据并使其更符合我们期望的分布。是否有人知道如何做到这一点？

如果有人给你一堆轨迹数据，你会如何调整这些数据，使其符合你关心的分布？对，重要性抽样。没错，我们可以通过重要性采样来实现这一点。让我们简要回顾一下重要性采样。我们的想法是，针对任何分布，我们可以重新平衡样本以获得无偏的样本。想象一下，我们从某个分布中生成的数据...

当我们希望从某个分布 q 生成数据时，我们的目标是估计函数 f(x) 对吧？这样我们就可以得到概率分布 q(x)。

一个非常巧妙的见解是，我们只需要重新考量我们的数据。因此，我们将专注于来自我们关心的分布下采样的数据点。我们需要重新调整它们的权重，以便它们看起来具有相同的概率，低于q(x)，符合我们期望的策略。所以，重要性抽样适用于任何分布不匹配的情况。比如，当你有来自一种分布的数据，但希望从另一种分布中获取数据时，尤其在物理学等领域，通常会遇到罕见事件，比如希格斯玻色子。在这些情况下，你可以重新权衡事物，以获得符合真实期望的估计。这个方法适用于一般情况。让我们回想一下，这如何在强化学习环境中发挥作用。再次强调，我们有一系列剧集，可以称之为$h$或$\tau$，包含状态、动作和奖励的序列。如果我们想要进行重要性抽样，或者说我来表述一下，我们希望获得类似于$p(h_j)$这样的东西，对应于我们期望的策略。

那个概率就是我们的起始概率。让我们假设不管采取什么策略，情况都是一样的，然后我们会有采取特定行动的概率，假设我们处于该状态时，乘以我们进入下一个状态的概率和我们观察到的奖励的概率。我们可以对 j = 1 到 n - 1 或 l j - 1 进行求和。因此，我们只需重复查看我们选择操作、Transformer模型和奖励模型的概率是多少。这就是我们所了解的历史的概率。然后，如果我们想要使用重要性采样，我们需要的是这个历史的概率，我们需要能够计算 q(x)。

在我们的情况下，这是指历史时间点 j 下的评估策略与行为策略下历史时间点 j 的概率之比。因此，我们希望实现这一点，我们希望一切都能抵消，因为我们无法访问动态模型或奖励模型。不幸的是，就像我们在某些政策梯度工作中看到的那样，这可能会发生。因此，如果我们有 hj 除以 pi b 的概率，我们将再次得到初始状态分布，在两种情况下都是一样的，然后我们就有这个概率比，即 aj 除以 sj 的概率。这是在 pi e 下，aj 的概率除以 um，给定 pi b 的 sj，然后是变换模型和奖励模型。所以这很好，因为这被抵消了，这也被抵消了。

世界的动态决定了下一个状态，而世界的动态决定了奖励。为简洁起见，这里做了一个马尔可夫假设，即将其视为马尔可夫版本。尽管如此，你仍可以根据完整的历史情况来处理，无需系统符合马尔可夫性质。因此，不管动态是依赖完整历史记录还是最后一步，其行为策略和动态效果都是一样的。在评估政策时，您可以将这些因素剔除，奖励模型亦如此。因此，在此洞察中并不需要马尔可夫假设。最终得到的比率是，在评估政策下选择行动的方式与行为政策下选择行动的方式之比。是的，假设相同的轨迹由两种不同的策略产生。很好的问题。

假设有两种不同的政策产生相同的轨迹。针对这个轨迹，在行为策略和评估策略下观察到这种情况的概率是多少？如果在评估政策下更容易观察到这种情况，我们希望增加从该轨迹中获得的奖励。如果在评估政策下观察到这种情况的概率较低，我们希望降低这种情况的权重。因此，我们的直觉是有一系列轨迹以及它们的奖励总和。我们对这些轨迹使用符号$h$和$G$来表示。对于这些轨迹，奖励的总和。如果我们有与评估策略相同的行为策略，为了评估评估政策的效果，我们只需对所有这些$G$进行平均，但它们是不同的。因此，我们的目标是，在评估政策下更有可能发生的$h$，增加它们的权重；对于评估政策下不太可能发生的$h$，减少它们的权重，以便在对这些$G$进行评估和权重时获得真实的期望值。

这段内容主要讨论了重新加权数据以更符合评估策略得到的轨迹分布的问题。提到可以使用不同方法来应对数据中可能存在的错误。一种方法是构建一个对错误具有鲁棒性的估算器，如果估算器出现错误，可以尝试其他方法来提高稳健性。另一种方法是采用经验分布。在讨论过程中对于数据的生成和概率分布的确定性做了一些假设，并提到了解决这些问题的不同方式。

最近的一些研究工作表明，有时候最好使用经验估计，即使已知真实估计，这些研究来自德克萨斯大学奥斯汀分校的Peter Stone实验室，这种估计器的性能十分出色。虽然这些内容是通过LaTex写出的，而非手写，但其中涉及到了历史概率的噪音。具体来讲，针对所评估政策的价值，可以通过历史比率乘以历史回报来计算。其中，n分之一表示对每个动作采取的概率，用符号π表示。因此，对于给定的动作aj和状态sji，利用πe来表示i等于1时，轨迹长度除以πb aji的比率。

在任何情况下，该特定历史时期的回归会发生。这一点很重要因为它提供了一个无偏估计器的机会。这让您在一些假设下能够很好地估计价值，我稍后会详细问您。而且，您无需了解动态，因此不存在动态性问题。也无需奖励，也不必满足马尔可夫性质。任何人都可以这样做，告诉我这可能不起作用的情况。因此，我认为，如果您在使用它进行训练，那么从最初的策略到最终的策略，它们不太可能有很大的不同，对吧？否则，之前提到的样本将毫无意义。这是一个很好的问题。这正是我要问的，您知道，πe 和 πb 有多大差异并且能让我们这么做。这就是我要问您的问题。那么，有人能告诉我，他们认为这个估计器在哪些条件下是有效的吗？在某些情况下，您认为这可能会因为

评估策略和行为策略之间的差异导致性能不佳是因为涉及到在特定状态下采取行动的概率。如果其中任何一个概率太小，事情都可能出现糟糕的结果。正是因为这两个概率都太小，情况可能变得很糟糕。那么在这两种方法中，哪一种更糟糕呢？如果它们中的一个概率是极小的，甚至接近零。嗯，这个情况确实很糟糕。言及 pi b，它的值真的很小，或者说接近零。目前情况可能确实变得很糟糕。对于 pi b 为零的情况，我们并没有观察到它，因为我们所使用的数据是从 pi b 中获取的。因此，我们从未看到过 pi b 为零的情况，尽管它可能是非常小。这意味着，有些不太可能发生的情况可能会发生，而你的行动策略却可能会导致这些情况发生。如果其中一个概率为零的话会怎样呢？对于某些操作来说，这个概率可能会是零的。但是，如果 pi e 不为零的地方为零会怎么样呢？

如果某个特定a的π b等于0，但是a的π e大于0，你知道，大于0可能是1。那里可能有什么问题？就像你认为这是一个让我们举手一下。如果发生这种情况，你认为我们是胡-让我们举手，如果你认为这是一个很好的估计，则可以举手，如果你认为，哦，这很粗糙，则可以举手。因此，如果行为策略采取行动的概率为0，但评估策略采取行动的概率为正。如果您认为这个估算可能非常糟糕，请举手。是的。因此，您知道，如果在某些情况下您从未尝试过操作，例如您从未在数据中看到新评估策略将采取的操作，则您无法使用此功能。所以我们常称其为覆盖。覆盖或者支持。因此，我们经常做出一些基本假设以使其有效。因此，我们的覆盖范围或者支持通常意味着给定s的π b对于所有a和s都必须大于0。

在这种情况下，给定s， πe便大于0。因此您需要支持这一点。πe不需要在所有地方都是非零的。但对于您可能要评估的任何事物，在任何地方，如果您确实希望从评估策略生成数据并且可能采取行动，那么您需要能够达到该状态并且采取该动作具有一定的非零概率。是的，这就是术语问题。那么，我们说πb和另一个部分中的π2是相同的，对吧？是的，这是正确的。

好的。最初我认为您说评估政策是您观察数据的政策。这是不正确的吗？感谢您澄清这一点。呃，行为政策始终是您观察数据的方式，评估则是您希望查看的方式。我很抱歉，这里的符号通常有点[噪音]，因为我认为，嗯，有时人们将评估策略称为目标策略或评估策略，或者，您知道，就是这样，嗯，大多数时候，用于收集数据的策略被称为行为策略。是的。[听不清]您可能只是不想将其包含在产品中。问题在于，如果它们都是0，

这段对话讨论了在强化学习中可能遇到的问题。简化和重述后的内容如下：在某些情况下，使用随机行为策略可能导致在某些状态下没有采取任何行动的情况发生。在这种情况下，评估某个状态下采取某行动的概率为零可能会导致问题。即使在实践中，也需要考虑到可能从未见过的状态和行动组合，这对于某些复杂的任务（如《蒙特祖玛的复仇》游戏）可能尤为重要。因此，对于不同的政策进行评估时，需要考虑到这种可能性，以便更好地理解潜在政策的效果。

因此，如果你有类似的情况，实际上不是一个球体，但是，你知道，如果它是，嗯，如果你在这里有一个行为策略，你可以考虑使用某种距离度量，通过这个度量你仍然可以获得对π的良好估计。所以你有一个半径，这基本上取决于你所支持的策略以及你无法评估的其他任何事情。好吧，总结一下，嗯，你知道，重要性采样是一个很棒的想法，适用于许多统计数据，包括强化学习。我认为 - 对此的首次介绍，嗯 - 我记得它最初应用于强化学习是在Doina Precup在2000年的一篇论文中。尽管这个方法早已存在，但是，嗯，我认为在强化学习领域，这是他们首次引入它的时候。当然，这些思想也出现在政策梯度类的方法中。最棒的地方在于，这是一个无偏和一致的估计量，就像一致性意味着渐进地它确实会给出正确的估计一样。

当样本量 n 趋向无穷大时，估计的值 v_pi_e 将趋向于真实值 e_pi_e。这是一种健全性检查，随着数据量的增加，你会得到更准确的估计。需要注意的是，这是在特定假设条件下的情况，需要得到相应的支持。在我们的情境下，我们可以利用时间过程的特性，这在策略梯度方法中很常见。未来的奖励不会影响过去，因此在计算重要性比率时，我们只需考虑特定的时间步长 t，这样做是有必要的。请记住，Gt 的定义是...【原文缺失部分】

在奖励中，像奖励总和一样，这在考虑重要性采样方程时非常重要。我们可以将其扩展到计算一系列奖励的总和，包括r1 + gamma乘以r2 + gamma的平方乘以r3等。在这个等式中，重要性比率的乘积乘以e乘以奖励。换言之，乘以每个不同奖励项与动作概率的相同比率。然而，r3不会受到r4中更长操作的影响，这样只是引入了额外的方差。类似于政策梯度，我们实际上只需要将比率的乘积应用到我们获得奖励的时间点，而不需要考虑更长操作的影响。这使我们能够进行每个决策的重要性采样。

所以这只是直到某个点才得到奖励的情况，因为未来的奖励无法影响过去。再重申一遍，这和马尔可夫性质无关。这就是现实，时间是不可逆的。好的。另一件重要的事情是，在重要性抽样中，我们只关注这些权重、这些权重的乘积。在选择行动时，根据不同策略下的类似产品，我们经常混淆这些权重术语与我们在函数逼近中讨论的所有权重。与重要性采样相比，加权重要性采样只是通过重新归一化权重之和。你可能会想这样做是因为，就像之前讨论的那样，如果你的行为策略 π_b 很小。简而言之，对于某些轨迹来说，这可能是非常小的，非常小的情况。

这可能意味着您对这些路径的重要性权重非常重要。实际上，有证据表明，一般来说，随着时间的推移，重要性权重可能会呈指数增长。在某些情况下，这些重要性权重可能会非常大。因此，加权重要性采样所做的就是重新归一化。您可以有效地将所有重要性权重限制在0到1之间。然后，在重新分配分布权重时使用这些权重。这种方式很常见，早在强化学习之前就开始使用了，而且也被应用于强化学习。这种方法可能会引入偏差或者保持一致性。这意味着结果会逐渐变得更准确并且降低方差。因此，我们基本上在进行偏差和方差的权衡。我们可以进行Q学习和蒙特卡洛估计。蒙特卡洛估计没有偏差，但方差较大。

Q学习是一个引导程序，因此它会有一定的偏见，但通常会更好，因为方差会低得多。大多数情况下，加权重要性抽样的方差会更低，特别是在少量数据情况下。是的。我想知道你是否可以谈一下，你知道，在说到我们故意设计一些没有偏见的东西之前。所以我们要忽略一些技术，现在又重新引入偏见？对的。我认为，当引入偏见时背后的直觉是什么？这是一个很好的问题。好的。你刚刚提到让我们采用无偏估计，现在又告诉我们要回到偏见，[笑]在这种情况下，我们如何判断什么时候可以这样做？嗯，我认为这完全取决于领域。我认为出现的一个挑战和问题是，通常在没有偏见的情况下更容易建立置信区间。我们更清楚如何做到这一点，但当有偏见时，通常很难量化。嗯，我想稍后简要讨论一下，我们真正只想直接优化偏差加方差的情况，就像我们想要查看准确性、均方误差、偏差和方差之和一样。

因此，我认为你可以使用一种直接在这两者之间权衡的方法，因为你知道我想要最小化均方误差。这样就能给我一个原则性的方式来权衡它们。我经常喜欢进行健全性检查的另一件事是，如果它有偏见但仍然保持一致性，这就是一个很好的健全性检查。或许一开始有少量偏见，但最终如果我有大量数据，它确实会给我正确答案。我们已经解决了 Q 学习中的一些函数逼近问题。这并不像把所有的赌注都下完，谁知道渐近会发生什么。但同样地，这取决于那一天。这是一个领域中的重要挑战。就像我刚才所说的，在这种情况下，加权重要性抽样非常一致。你会收敛，如果你的视野有限，或者存在一种行为政策或有限的奖励。以及，我和菲尔正在讨论的是，如果你看看托马斯和布伦斯基尔。我们在 ICML 2016 的一篇论文中考虑了关于 RL 案例的这个问题。这是对这个问题的一个很好的参考。因此我们将得到这些估计。

使用加权重要性抽样可能会导致偏差，但方差较低，而不使用则可能带来高方差但低偏差或零偏差。为了控制方差，可以在大脑中重新思考政策梯度并考虑基线。从统计学角度来看，当我们有一个变量X时，如果我们的估计是无偏的，则意味着我们的平均值估计与真实期望值相匹配，同时也有方差。假设我们稍微调整这些估计，例如，通过减去一个随机变量Y，可以将Y视为Q函数。假设Y可能代表Q函数，Y的期望值可能是V。你认为Q是在给定状态下所有可采取的行动的平均值吗？

当然，如果重新定义mu，那么X的估计将会回归到试图获取V_pi_e的估计值。也就是说，如果你重新定义你的mu函数，那么X变量会回到试图获得V_pi_e的估计值。如果你在期望值中加减一些项，你仍然会得到一个无偏估计量。因此，我们可以这样表达：X减去Y再加上Y的期望值，结果正好等于X的期望值减去Y的期望值再加上Y的期望值。换句话说，你可以这样做，我是说，在统计学中你可以减去一个变量再加上其期望值，平均而言，这不会改变X的平均值。或许你会问：“为什么要这样做呢？”这样的操作可以针对任何类似的情况进行，其中X和Y是随机变量。这些原则都是通用的，适用于任何随机变量，其中Y是所谓的控制变量。如果这个控制变量有助于减少方差，那么这种操作将会很有用。这也就意味着，你知道，Y必须与X相关联。

如果我们只是随机地去掉一些东西，可能并没有什么帮助。但是，如果我们删除一些让我们对X有一定了解的东西，比如在我们的例子中，我们对于帮助我们进行预测的信息感兴趣，那么我们可能会得到更低的方差。因此，我们可以通过观察这个奇怪的值 mu hat 的方差来了解这一点，其中我们有 X - Y + Y 的期望值。我们可以把它表示为 X - Y 加上期望值 Y。现在，由于 Y 的期望值已经没有变化了，所以我们可以说这等于 X - Y 的方差。这意味着我们将得到一些关于 X 的方差、Y 的方差以及两者之间协方差的信息。因此，如果 X 和 Y 的协方差是正的，这表示这两个变量之间存在关系，其中一个变量提供了关于另一个变量的信息。如果这个协方差大于 Y 的方差，那么就会带来一些好处。这样，最终就可以得到一个估计结果。因此，如果这是真实的，如果这是正确的，如果这是符合实际的，那么 mu hat 的方差将小于 X 的方差。

这个理论相当不错，因为我们并没有改变平均值，而是减少了方差。在某种程度上，这看起来像是“免费的午餐”，但实际上并不是真正的免费午餐，因为我们所使用的信息实际上是关于 X 的信息。这类似于我们在基线术语和政策梯度中所做的操作。我们不仅可以依赖蒙特卡洛回报，还可以减去像价值函数这样的基线。因此，在重要性采样中，您也可以这样做。在这里，X 是重要性采样的估计量，Y 是某个控制变量。一般来说，这可能是您根据马尔可夫决策过程的近似模型所建立的 Q 函数，也可以是来自 Q 学习的 Q 函数。这就为您提供了一些方法来获取一些状态动作值的估计，类似于 Q。这种方法被称为双稳健估计器。

双重稳健估计方法，在统计领域已有很长时间，大约在2011年左右被引入到多臂强盗问题中，与杜迪克等人一起。为什么称之为双重稳健呢？这个概念是，如果结合了来自正常重要性抽样的信息和一些控制变量（比如Q值估计），如果其中一个出现错误，你的估计依然能够保持稳健。也就是说，如果你的模型差，或者你的控制变量有问题，那么你的估计结果就会很差。为什么这一点很重要呢？如果我们考虑其他解决方案，例如对数据进行Q学习，但Q学习可能会存在偏差，或者给出一个不太可靠的估计，无法确定其好坏。这时双重稳健估计方法可能会更有用。因此，如果结果非常好，你可以说“我们得到了一个不错的估计”，或者如果表现很差，你可以认为通过重要性采样可以弥补这个问题。

这意味着，如果你有一个糟糕的模型，或者你对 π B 的估计有错误，你从环境中获取的数据就类似于这种情况，因此我们并不真正了解行为策略是什么。因此，如果有不准确的地方，你也要考虑控制变量是否准确，这样才能做得很好。在某些情况下，两者都存在错误，这样一来，所有的计算都会出错。但这种方法可以让你更稳健地了解你试图评估的策略的不同部分。比尔和我讨论了执行这种操作的不同方法，以及如何以加权的方式执行这种操作，这就是加权重要性采样的合并。那么这可以让你做什么呢？我会简要地展示一下这个方程。本质上，这里的想法是这些权重就像重要性采样的权重。这是原始回报，然后我们可以进行加法和减法运算。这就是 Y，这是 Y 的预期价值，通过像 Q 学习这样的计算可以得到。

使用动态规划来计算，就像操作经验模型一样。你可以通过多种方式估计 Q 函数的 πE 值，它们有好有坏。但是通常情况下，将它们纳入会在方差方面给予帮助。现在，让我们通过经验来看看它的影响。

在这个非常小的网格世界中，我们尝试用其来说明和理解不同技术的优势。X轴代表数据量，即数据集的大小。Y轴代表均方误差，即我们估计策略与真实策略之间的差异，希望这种差异很小。对数刻度下，误差越小越好。

通过构建一个近似模型来处理数据是一个可行的方法。这个模型可能存在误差，就像做马尔可夫假设一样可能是错误的。

在某些方面，有些事情是你无法准确估计的。这就是基于模型的方法。我们使用一个模型，然后计算该模型的V_pi e。我们收集数据，建立一个MDP模型，然后使用它，这就像一个模拟器，然后我们可以计算V_pi e。在这里，你可以看到这是固定的。在这种情况下，我必须记得一个重要点。我认为我们正在使用不同的数据集。在有了这么多集之后，如果模型随着更多数据的出现而改变了，那我必须仔细检查。就像模型并不是那么准确一样。也许我们没有使用所有应该使用的功能，或者环境并不是真正的马尔可夫，所以你可能有一些固定的偏见。你的模型可能逐渐出现错误。如果你的模型不太适合环境，你从中得到的估计可能逐渐出现错误。另一种方法就是进行重要性采样。因此，重要性采样是无偏的。正如我们所希望的，随着我们获得更多数据，估计值会下降。最终，它应该趋近于0。但我们希望有更好的结果。

现在我们正在谈论每个决策的重要性采样。 你会发现，通过利用奖励不受未来决策影响的事实，你可以受益。 这样做可以减少方差，为你提供更稳定的降低。 如果你变得更加稳健，你将会明显改善。 那么双重稳健是如何发挥作用的呢？ 它结合了我们的近似模型和重要性采样。 因此，你可以再次看到，我们在这里获得了显著的提升。 现在，我将主要从均方误差的角度来探讨这个问题，但我认为考虑均方误差的含义非常重要。 所以在这里，均方误差是指，我们估计的 V_pi 与真实值之间的误差有多大？但我们也可以考虑需要多少数据才能获得良好的估计。 因此，看看这个例子。 如果要使均方误差达到 1，也许这就足够了，但也可能不是。 这意味着，根据每个决策的重要性采样估计值，你可能需要 2,000 个集，而如果更加稳健，可能只需要少于 200 个集。

就像这样，你需要针对某个特定领域对这个问题有兴趣。但它表明只有在某些情况下，嗯，在这些估计方面稍微好一些，你可以在估算器的准确度方面获得巨大的收益。

噪音。再来看这张幻灯片，你如何在方差、嗯和偏差之间取得平衡。 菲尔和我想到的一点是，你知道，你可能想要，嗯，低偏差和方差，以及 - 你如何进行这种权衡，让我们考虑一下直接优化它。

因此，我们的神奇估算器只是尝试直接最小化均方误差。好的。 所以，再次假设均方误差是偏差和方差的函数。 因此，如果您知道偏差和方差是什么，您就可以希望直接对其进行优化。 我们知道什么是偏差吗？ 所以，再次偏差要记住，偏差是一个值减去其真实值之间的差值。那就是偏差。我们明白了吗？不，不幸的是，如果我们知道这一点，

如果我们已经知道了偏差，那么我们就可以确切地知道真正价值是多少，无需额外操作。在进行这项工作时，一个巨大的挑战是如何准确估计偏差，包括低估或高估的倾向。简而言之，我们的想法是，通过获得置信区间，可以利用重要性采样。假设我有一些基于重要性采样得出的估计，其中存在一定的不确定性，给出了最可能的数值以及上下限。比如，假设一个值为5加或减2。然后假设我有一个模型，基于这些数据构建，用来评估，得到另一个估计。所以我有一个使用模型得出的估计值为V_pi，设为8。那么我的模型必须具有的最小偏差是多少呢？您如何使用这些置信区间，以获得基于置信区间下限的可能最差情况假设的相关信息？

这些置信区间是真实的，因此我们知道真实值一定在3到7之间。我的模型的最小偏差是多少？有一个对的，因为那就是差异，是的。这为你提供了偏差的下限。那么，你离为你提供偏差下限的置信区间有多远？这是一种乐观估计，你的偏见——你的模型可能更有偏见。嗯，但这为你提供了一种量化偏差的方法，这就是我们在此方法中使用的方法。因此，我们结合重要性采样估计量并考虑它们的可变性。我们必须估计它们的方差，以及模型的偏差，这使我们能够确定如何在它们之间进行权衡。再说一遍，你会得到，嗯，你经常会得到非常可观的结果。这依然是Gridworld，但你会再次得到[噪音]，在某些领域中大约相差一个数量级。你将需要少一个数量级的数据。在本例中，我只是放大了，因此您甚至可能

其他一些方法由于位置较高而不易被注意到。那么，我们可以尝试进行政策外评估，以获取良好的政策估计。目前为止，我还没有深入讨论如何确定这些置信区间的问题，但我已经提到了一些不同的方法，可以用来估计V_pi E。因此，我们希望对我们可能希望在实际应用中采用的新替代策略进行一些估计。在这里，有一些微妙之处，例如你的行为政策支持是什么，以及我们如何进行不同加权的思考，是否能改进这种重要性采样的方式？答案是，确定可以使用稍有不同的权重，关于这点我稍后会详细介绍。另一个在实践中非常重要的问题是，你的分布通常是非平稳的。比如，想象一下，就像你在查看患者数据时，食品和药物管理局发布了新的食物金字塔，导致每个人都改变了饮食习惯。因此，在这种情况下，

患者的情况将发生显著变化，你需要确定数据集中是否存在非平稳性，就像世界动态模型正在发生变化一样。因此，在处理这个问题时，我们有一些其他想法。

假设我们已经完成了禁止政策评估，对替代政策的效果做出了估计，现在我们希望更有信心地超越这一点。我们希望进入一个能够比之前的政策更好的概率输出策略的世界。这意味着我们不仅需要估计新政策的效果有多好，还需要估计它比你的当前政策更好的程度。

首先，我们考虑使用重要性采样和Hoeffding不等式。回顾我们的研究重点是进行高置信度的离线政策评估。

在山地车这个简单的控制任务中，[噪音]，你的AI智能体试图到达特定位置以获得高额奖励。我们将从某些行为策略中收集数据，希望通过这些数据学习到更好的策略，并提升性能的能力范围。请注意，在霍夫丁不等式中，我们研究了一种方法来评估你的平均值与目前的平均值之间的差异程度。换句话说，你的经验平均值 [噪音]减去真实平均值会有多大不同？[噪音]这也会受到你拥有的数据量的限制。实际上，这取决于你收集到的数据量，是以数据量为函数的。这也取决于你的变量范围，例如，在这个任务中，奖励可能介于0到1之间，假设b为1。因此，我们可以尝试使用基于模型的强化学习和Hoeffding不等式来讨论如何在非政策评估的背景下应用它。

因此，我们可以利用旧数据来尝试估计我们评估策略价值的上限或下限。假设我们使用了 100,000 条轨迹数据，而评估策略的真实性能为0.19。通过使用Hoeffding不等式，我们非常有信心认为新策略至少能达到 -500 万。我们知道真实奖励值位于 0 和 1 之间，但Hoeffding不等式给出的上限是 -500 万，这确实有点出乎意料，对吧？就像你所说，0.19明显大于 -500 万，然而这个信息并不是很有意义。因为我们已经知道奖励值范围在 0 和 1 之间，如果我们用Hoeffding不等式来计算，我们得到的结果可能就毫无意义了。所以，虽然得到的上限是真实的，但却毫无帮助，因为它太过负面了。

就好像我们知道这是真实的——真实值是——所有策略都在 0 和 1 之间。那为什么会出现这种情况呢？让我们来看看重要性采样。重要性采样表示我们将乘以权重。正如之前讨论的那样，这可能非常小，比如0.1。假设你有10到L。举个例子，如果你采取一系列行动，比如在山地车领域，你需要采取一系列非常具体的行动才能获得奖励，而在非最佳政策下，这些行动很少出现。假设你知道，大部分数据从未到达山顶，但有一两个数据点实际上到达了山顶，这些是非常罕见的情况。这意味着你的，嗯，

重要性采样权重将会非常高。你知道，这将是1除以0.1直到L。这真是巨大的。所以这些可能会变得非常大。霍夫丁不等式就取决于此。您拥有的潜在回报的范围。我们的潜在回报范围是多少？我们潜在回报的范围是g倍或i的乘积等于我们重要性权重的1到t。所以b等于max。它说：“在最大的情况下，你的回报会是什么样子？”。所以这主要取决于你的真实范围。我们真正的b将在0和1之间，并且这个乘积具有重要性采样权重，这就是问题所在。重要性采样权重的乘积可能是巨大的。因为你的行动序列确实不太可能，然后你就会爆发。

因此，如果我们在这里仔细观察一下，我们可以获得这个分布，其中一些部分是相当大的。这意味着在最终的Hoeffding不等式中，Hoeffding实际上是$|\text{你} - b|$，基本上是$-\frac{b}{\sqrt{n}}$。因此，例如，如果$b$设定为大约200，这在蒙特卡洛方法中是相当合理的，稍微合理一点，对于深度强化学习方面来说。然后，你会得到类似于$1-\frac{1}{0.1}\times200\times 1\times\frac{1}{\sqrt{n}}$这样的表达式，大致上是这样对吧？所以你需要减去这个巨大的术语。这意味着你的限制条件是空洞的，基本上我不确定这种评估政策的效果如何。所以，有人对此有任何疑问吗？为什么会出现这个问题呢？好的，因此，根据Phil之前的一些研究成果，我们可以得出去除那一部分，把它删掉。嗯，如果你去掉这个部分，这会对你的预期值产生什么影响呢？

这段文字的意思是在讨论在估计数据时可能存在的误差问题，即使忽略掉一些极端的数据（噪音），估计结果也会有所减小，但不会高估。如果考虑到制定政策的改进，可能会发现实际执行的政策比想象中的更糟糕，因此有可能低估了行为政策或评估政策的优劣。因此，低估的情况也是安全的，就像不去实施可能会带来好处的事情一样，可能会存在机会成本损失，但并不是坏事。这里提到了定义一个新的置信区间，也可以考虑保守性，这要根据数据量来决定。总的来说，就是讨论了估计数据时可能出现的偏差以及如何解决的问题。

为了准确地调整置信区间，您需要设置置信区间。接下来，您将计算下限，假设您有相同数据量的 100k 条轨迹。这是您得到的新估计值，即mu，V pi的估计值，因此它是V pi e的下界。根据估计值至少为0.145，而真实值为0.19进行比较时，除了安德森之外，所有其他形式的集中不平等都非常糟糕。因此，与其他方法相比，例如Chernoff-Hoeffding等方法，您无需熟悉所有这些。基本上，它仅表示，如果您尝试使用其他方法来计算下限，它们将完全无效，而这种方法则表明：“我们无法确定其准确程度，但至少是0。”

“预测是0.19，但实际值是0.145。” 这并不十分完美，但你知道， 如果——如果你的策略是0.05的话，那意味着你应该尝试新的方法[噪音]。 所以，嗯，他们将这个概念应用在数字营销上，这是Phil与Adobe的一些同事合作完成的工作。 这个方法的好处是，你可以说，你知道，如果我需要的话，我会搞清楚，如果我[噪音]要推出新产品并取得更好的数字营销效果，我可以查看之前的数据。[噪音]我可以自信地说，我可以推出一些能够带来更高点击率和更多收入的东西。[噪音]这些置信区间非常严格，使你能够确定新策略是否更好，这很赞。[噪音][听不清]是的。你也可以，所以这是试图获得这些置信区间的一种方式，实际上您也可以使用t检验，从经验上看，这通常效果不错。嗯，你们中的一些人可能在统计课上已经接触过这些内容。我简单介绍了一下，因为时间已经很少了，您可以将这些想法结合起来。

接下来，考虑尝试获得这些下限，然后将其与优化相结合。因此，您可以考虑执行许多不同的策略来计算所有策略的下限。之后，利用这些信息以一种可靠的方式来决定未来要部署哪一个。因此，您可以说，“我将利用部分数据来优化，然后尝试评估结果，并确保在做决定之前具有良好的置信度。”在数字营销中也可以采用类似的方法。菲尔·托马斯和我进行的研究之一是使用糖尿病模拟器，以确定是否可以推断出效果更好的政策，比如胰岛素调节等。在部署之前，您可以更好地运用类似思想，但我将简要跳过这一点。这是一个非常大的问题，就像在社区中越来越受关注的工作领域。许多人都在思考这个问题，即反事实推理，因为我们现在已经拥有了大量的数据。

随着越来越多的数据涌入，特别是在电子病历系统中，我们希望利用这些数据来改善患者的健康状况。我们拥有各种数据，如在线平台上的数据等等。然而，我们面临许多额外的挑战，比如如何应对长期的问题。在评估不同政策时，重要性采样可能存在不公平的情况，这意味着在不同政策之间可能存在偏差，这取决于它们与我们的行为政策的匹配程度。因此，在决定部署哪种政策时可能会面临困难。

当行为政策未知时，我们有许多不同的工作思路。我们将这些想法结合到深度神经网络中，并且考虑了迁移学习的应用。切尔西周一提到了元学习，其中一个有趣的想法是，在构建这些模型时，是否可以应用相似微调思想到强化学习案例中呢？换句话说，能否建立模型来评估我们的政策，并将其用作对您所关注的政策不匹配的数据？这种方法可能对于构建更好的模型，尤其是在类似医疗保健领域，可能非常有帮助。尽管在这个领域还有许多额外的工作要做，并且还有许多其他团体正在考虑类似的想法。

如果你对这些想法感兴趣的话，也可以在校园里找到一些同样热衷的人。还有一些优秀的同事从不同的角度思考这个问题，他们会从经济学、统计学或流行病学的角度来看待这个问题。尝试跟他们合作也会非常有趣。所以，只是简单地提出一个问题，如果你有一些数据集，我们能够从数据上得出一个好的新策略。你希望在实施之前了解其质量，以确保其有效性。这就是安全地评估和选择政策或进行优化的真正含义。因此，根据您对此的理解，您应该能够定义和应用重要性采样，了解其一些局限性，并列出一些替代方案。你需要知道为什么可能希望能够进行这种安全的强化学习，以及这在哪些应用程序中可能很重要。还有就是定义我们在这些情况下获得了什么类型的保证。就是这样，然后下周继续。

我们计划在周一进行测验，然后在周三讨论蒙特卡罗树搜索。感谢。

好的，让我们来重写整理一下这段内容：

现在开始本学期的最后一堂课。我们要讨论一些物流安排。请注意，项目截止日期是本月20号晚上11:59。海报展示将在周五上午8:30举行，不接受迟到。请记得将海报提交到Gradescope在线平台。我们会提供意见书，也不接受迟交。您应该已经收到有关海报展示的详细信息的电子邮件。有问题请随时联系我们。

办公时间将是最后一周，下周将没有办公时间。大多数人将开始期末考试周。您可以在Piazza上联系我们，我们很乐意帮助解答问题。今天我们要给参加周一考试的同学发送上次测验的成绩。

我们差不多快评完这些分数了。有些人还在迟到，所以我们得评 SCPD 的分数。不过其他周一参加考试的人应该能拿到他们的测验分数，今天应该会回来。今天我们要讨论一下蒙特卡洛树搜索，以及课程结束时的内容。那么为什么要用蒙特卡洛树搜索呢？有谁听说过 AlphaGo 吗？对，AlphaGo是过去10到20年间的主要人工智能成就之一，击败人类在围棋比赛中取得了惊人的成绩，围棋被认为是一种非常复杂的游戏。因此，蒙特卡洛树搜索是实现这一壮举及其他额外功能的关键。虽然这是我们课堂上还没有深入探讨的一个方面。因此，谈论蒙特卡洛树搜索和了解AlphaGo的一些理念是很有用的。此外，考虑到蒙特卡洛树搜索是一种基于模型的强化学习方法，这也需要我们留意。

这是一个非常强大的工具，我们没有过多讨论，部分是因为我们还没有看到模型深度学习案例取得如此大的成功。我很高兴今天或线下更多地谈论这一点。但我认为，展望未来，这可能是一个真正富有成效的研究途径。我们可以讨论为什么这在AlphaGo的Alpha中特别有用。所以，我们首先要做的是再次讨论基于模型的强化学习。然后我们将讨论基于模拟的搜索，这就是蒙特卡罗树搜索的由来。实际上，只是因为每个人都参加不同的课程，我很好奇，这里谁在另一个课程中涵盖了蒙特卡罗树搜索？只有两个。那是什么课？238.是的。是的。相同的？一样的。它被提到有点像[噪音]它被简短地提到。啊，是的。非常简短。是的。是吗？217.

在一般的游戏玩法中，学习特定模型是一个很好的选择，尤其是当我们谈论强化学习时。基于模型的强化学习是指通过学习直接从数据中学习政策或价值函数或两者来处理我们不完全了解的世界。今天我们要讨论的是学习特定模型，即学习过渡和/或奖励模型。一旦你有了一个模型，你就可以用它来进行规划。规划是指采用已知的世界模型，然后通常使用值迭代、策略迭代或动态编程来制定计划。如果想了解更多关于强化学习的信息，可以参考其他课程。

让我们尝试为这些模型计算策略。与之相对，我们之前讨论了许多无模型的强化学习方法，其中没有模型，我们直接从经验中学习价值函数。现在我们将从经验中学习一个模型，然后计划使用它。

除了我们已知的经典决策方法外，我们的规划过程可以使用从经验中学到的模型，例如动态规划，也可以使用我们在课堂上讨论过的其他任何技术。一旦我们拥有这个模型，它就可以充当一个模拟器。有了这个模拟器，您可以进行无模型的强化学习，或者进行政策搜索，甚至是任何您想做的事情，因为您有一个世界模型。基本上，它就是一个模拟器，您可以利用它生成经验数据，执行动态规划等操作。

一旦您拥有了一个世界模拟器，那将非常有用。当然，缺点可能在于模拟器的精确性。最终的估计结果会受到影响。

好的，让我来重新整理一下这段内容。在强化学习领域，我们通常面临建模世界的挑战，这个世界中包括行动、奖励和状态。在很多情况下，我们可以清楚地定义奖励函数，但并非总是如此。例如，设计一个基于强化学习的客户服务系统时，您可能考虑定义一个奖励函数，比如参与度或购买行为等。然而，您可能并没有关于客户行为的良好模型。因此，在许多实际应用中，我们需要隐式或显式地学习动态模型，即状态转移过程。奖励函数可能是已知的，但动态模型通常需要学习。

从循环的角度思考这个问题，我们会考虑拥有一系列经验，比如状态、行动、奖励、下一个状态组成的元组。然后我们可以将这些元组输入模型中，来输出奖励或状态转移。这样我们就可以进行规划和决策。

可以采用动态规划、Q 学习，或者像策略搜索等其他技术来实现这一目标。这些方法必须能够提供选择行动的能力。因此，我们需要一种能够在当前情况下选择下一步行动的方法，而不必计算完整的值函数。在蒙特卡洛树搜索中，我们会利用这一点。我们不必计算整个世界的价值函数，也不一定要设计完整的策略。我们只需要知道为特定行动接下来应该做什么。这种方法的一些优点在于，我们有许多监督学习方法，包括来自深度学习的方法，可以用来学习模型。有些方法比其他方法更适合。因此，在我们的状态转移中通常会考虑到随机因素。因此，我们需要监督学习方法能够预测分布。对于奖励模型，我们通常可以将其看作是标量。因此，我们可以使用非常经典的基于回归的方法。

基于模型的强化学习还有另一个好处，就像我们之前讨论的探索一样。通常情况下，我们可以通过评估模型的不确定性来明确建立模型的质量。一旦在世界模型中引入了不确定性，我们就可以利用这一点来传播我们的决策不确定性。因此，在强化学习中，这一点非常直接，因为例子中，对于强盗来说，我们对于手臂奖励的不确定性直接反映了我们对价值的不确定性，因为奖励只是一个时间步。在马尔可夫决策过程中，我们可以考虑奖励、动态模型以及类似奖励形式的不确定性，然后在规划过程中传播这些信息。这再次引发了我们对于不同状态和行为价值的认识程度有多深，可能的结果是什么，甚至可能性最乐观的情况是什么。然而，现在的缺点是，首先我们需要学习一个模型，然后我们需要建立一个值函数，这样可能会导致两个近似误差源。

由于我们会获得一个近似模型，然后对大型状态空间进行近似规划，因此在这种情况下可能会出现复合误差。 另一个我们在前面课程中涉及到的复合错误的例子是在谈论模仿学习时。 当我们探讨如果你有一个轨迹，然后试图进行行为克隆并学习从状态到行动的映射，以及如果你获得了这个策略并在现实世界中执行它，你最终可能会陷入这个状态空间的某些区域，此时数据不足。在这些状况下，您可能会遇到累积的错误，因为这些错误可能会相互叠加。一旦你进入数据稀缺的状态空间部分，然后进行推断，情况可能会变得非常糟糕。因此，在这种情况下，同样地，如果您构建一个模型并计算一个策略，最终将您带到在现实世界中数据稀缺且模型预测很差的部分，那么您的结果价值函数和策略可能会遭受重创。我认为基于模型的强化学习的另一个重要优势是它对于迁移学习也非常强大。

因此，当 Chelsea 在讨论元学习时，基于模型的强化学习的一个好处是，如果你学习了世界的动态模型，那么当有人改变奖励函数时，你可以零样本迁移，因为你可以直接使用学习到的动态模型，然后根据新的奖励函数计算新的策略。举例来说，如果我是一个机器人，我学会了如何在一个房间里导航，我现在知道每个动作的感觉，比如转弯的感觉、前进的感觉等等，而以前我只是试图到达出口。但是现在我了解了房间的动态特性，当有人告诉我，“别去那个出口，那里有什么危险”，并给我一个不同的奖励函数时，我可以根据自己的动态模型重新规划路径，而不需要更多的经验。这就是零样本迁移的意义所在，这样做非常有用。这也是构建通用世界模型的另一个原因。一些有趣的证据表明，人们在玩雅达利游戏时可能正在系统地构建模型。

当我把冰山移到北极熊旁边会发生什么？因为这样你就可以将这些模型推广到其他经验。好的。在这种情况下，我们要如何写我们的模型呢？我们将再次拥有正常状态、行动、转换、动态和奖励，并且我们将假设我们的模型大致代表了我们的转换模型和奖励模型。所以我们在这里假设马尔可夫假设。因此，我们可以表示我们的下一个状态只是前一个状态和该状态的分布中的动作，我们同样将其用于奖励。我们通常假设事物是有条件独立的，就像我们之前所做的那样。因此，我们只有一个特定的动态模型，该模型以状态和动作为条件，以及以先前的状态和动作为条件的奖励。因此，如果我们想进行模型学习，我们就会遇到我们之前讨论过的监督学习问题。嗯，你有状态和动作，你想要预测下一个状态的奖励。所以我们有这个回归问题和这个密度估计问题。

然后，您可以通过各种方式来完成这个任务。 您可以，嗯，使用均方误差，也可以尝试不同形式的损失函数。 实际上，我们最近在离策略强化学习领域取得进展的一个方法是采用不同于标准最大似然损失的形式。 但一般来说，我们通常会讨论最大似然损失。 因此，我们可以这样做，当然，在表格情况下，这只是[噪音]计数。 所以，如果您只有一组离散状态和动作，您可以计算在这个状态和动作中开始并转移到状态一的次数，以及它们在这个状态和动作中开始并转移到状态二的次数。 您只需计算这些次数，然后进行标准化处理。 通常来说，有许多不同的方法可以表示这些情况。 嗯，我认为特别有趣的是贝叶斯方法，而不是贝叶斯深度神经网络。 目前为止，调整它们相当困难。 哦，还有另一种策略，您知道，贝叶斯深度神经网络。 [噪音] 嗯，我觉得它们真的非常…

大型语言模型之一的优势在于能够清晰地表达不确定性，但迄今为止，它们的训练相对困难。但我认为，我们可以借助许多非常简单的模型和它们的丰富函数逼近器来克服这一困难。因此，当我们需要进行表查询时，我们只需对计数进行平均处理。换句话说，我们计算处于特定状态时采取特定动作、转移到下一个状态的元组，并将其除以我们在该状态下采取该动作的次数。同时，我们也会平均计算所有奖励，即在特定状态下采取特定动作时获得的奖励，以及我们在该步骤中获得的奖励。举个例子来看这种情况。很久以前，我们介绍了一个AB示例情景，其中状态A会经过一个操作进入状态B。随后，有75%的概率它将进入一个终止状态，在该状态下有可能获得奖励1，或者进入另一个终止状态。

有 25% 的概率获得 0 的奖励。 我们假设这是根据马尔可夫奖励过程的一个例子，而不是一个决策过程，但我们可以观察到结果。从状态A开始，我们得到了奖励0，然后转移到状态B并再次得到了奖励0。然后我们有6个时间步长：从状态B开始，我们得到了奖励1，然后再次从状态B开始得到了奖励0。这种情况下我们可以建立一个表格查找模型。为了复习一下，这种表格表示的时间差异学习意味着每个状态是一行，而在这个问题中我们只有两个状态：A和B。

如果您反复播放这组经验数据，然后使用它来估计马尔可夫决策过程模型，并应用于规划以评估出最佳策略，或者如果您的制定策略是等效的，那么这实际上就是在利用数据收集信息。换言之，Temporal Difference（TD）方法给出的解决方案在计算上等同于通常所说的确定性等价模型，因为您通过数据采集、估计，将经验视为数据的平均值。因此，您可以问自己，“如果这是所有世界上的数据，那么相关模型是什么，以及最大似然估计是多少。”接着进行规划。TD方法就是基于这一假设。现在我们来看下内存，蒙特卡罗方法在这些数据上是否会收敛到同样的解？因此，不妨花点时间，转向旁边的人，询问他们是否会这样做，以及他们的做法是什么，为什么这样做或不这样做。

请问您有问题吗？哦，当然有。[笑声]。好的。我认为蒙特卡罗方法会收敛到具有最小均方误差（MSE）的解，而不是最大似然估计（MLE）效果？是的。蒙特卡罗方法不做马尔可夫假设。嗯，所以它们适用于非马尔可夫情况。因此，在这种情况下，无论如何，它们都会收敛到适合此策略评估的解。它们将收敛到最小均方误差。是的，有问题吗？所以你的意思是，如果您使用机器学习模型，可能会将它们收敛到均方误差（MSE），如果您使用不同的损失函数呢？[重叠]好问题。这一定会收敛到最小均方误差，而不是最大似然估计。[听不清]如果您使用不同的损失函数的话[听不清]。蒙特卡罗方法是否会收敛到 -

这取决于损失函数和是否进行正则化。 这是一个很好的问题。如果进行正则化，模型可能会收敛到一个与最小均方误差不同的解，具体取决于正则化的方式和损失函数的选择。一般来说，它不会收敛到与最大似然估计模型相同的结果，然后使用该模型进行规划或政策评估。关键区别在于蒙特卡罗方法没有马尔可夫假设。确实如此，它不依赖于马尔可夫性质。尤其在这种情况下，我们只使用一个样本来估算 A 的值。是的，对于这种蒙特卡罗方法，我们关注的是从特定状态开始的完整回报，而没有使用引导。因此，我们只有一次观察到 A 的值为 0。但是，如果系统确实是马尔可夫的，

这不是一个很好的解决方案，因为我们有其他所有证据表明 B 实际上通常具有更高的价值，我们无法利用这一点。而TD可以。所以TD可以说：“好吧，我知道A的价值这一次是0。”总的来说，我们认为A的价值等于直接奖励加上，在这种情况下没有折扣，所以等于B的价值。另外，我有所有其他证据表明B的价值在这种情况下实际上正好等于0.75，因为有6个例子它的值为1，以及两个例子它的值为0。所以，B的价值等于0.75，蒙特卡洛和TD都会同意这一点。因为每次选择B时，75%的时间你得到1，其余的时间得到0。所以，它的值是0。

因此，TD 估计会说 A 的动作价值也等于 0.75，即与 TD 估计相同。 出现这种情况的一个原因是，需要注意的是，并不是由于备份次数有限，或者抱歉，我会小心谨慎地使用数据的量有限。 因此，如果您通过 TD 运行很多次，蒙特卡罗估计也可以访问所有的数据。 这里说的是全部数据。 另一种选择是，如果您获取这些数据并构建模型。 现在我们有了一个模型，假设你从状态 A 开始，去状态 B 的概率是 1，你总是从 A 到 B。 实际上，你只见过这种情况一次，但是那一次你看到了，你就去了 B，我们可以用它来尝试获取模拟数据。 所以让我...好吧，我再去几次。 因此，本例的想法是，一旦您有了模拟器，您就可以使用它来生成样本。

接着，你可以计划如何利用这些模拟数据。起初，也许这个想法听起来有些奇怪，为什么要这样做呢？隐藏了真实数据，为什么不直接使用无模型方法处理呢？为什么要先建立一个模型，然后再生成数据呢？但通过下面的一个 AB 示例，我们将看到为什么这样做可能是有益的。我们可以获得该模型的最大似然估计或其他估计值，然后从中采样。在这个示例中，我们假设转换模型是，每次从 A 转换到 B。因此，当处于 A 时，我们可以进行采样，然后转移到 B，从而生成一个虚假数据点。我可以多次重复这个过程，得到许多虚假数据。这些虚假数据可能与现实世界相似，也可能不太相似，这取决于模型的准确性。但这些数据绝对可以用来训练，很快我们将看到为什么这很有用。

好的，所以回到这里，左侧展示的是真实体验。在左侧，我们拥有所有这些真实经验，然后我们的任务是建立一个模型，然后我们可以从中进行采样。因此，我们可以获得一个看起来与我们实际观察到的数据非常相似的体验，但我们也可以有其他类型的体验。现在，为什么我们能够做到这一点呢？因为我们现在有一个模拟器，并且在我们的模拟器中，我们已经看到了从A到B的情况。在我们的模型中，还有其他情况，比如我们从B开始，接着得到1。因此，基本上，我们可以将这些经验连接在一起，模拟出我们从未在现实世界中观察到的情况。我们在这里利用的是马尔可夫链的事实。因此，如果该领域并非真正符合马尔可夫链的特性，那么我们最终可能得到的数据看起来与在现实世界获取的数据非常不同。但如果是符合马尔可夫链的，那么它可能仍然是一个近似模型，因为我们能够模拟出符合现实情况的数据。

我们的数据量有限，用于训练模型。但有时我们会遇到一些状态和动作的组合，可能在我们的数据中从未见过。那么，在采样时更新我们的模型是可行的吗？这个问题很好。是的，你可以在采样时更新你的模型，但是目前我们只是从我们的模型中进行采样。这意味着这并不反映真实世界的经验。这可能会导致确认偏差，因为你的模型提供数据，如果你将其视为真实数据并用于模型更新，但这实际上并非真实数据。因此你可能会变得过于自信，因为你将生成的虚假数据看作真实数据。那么，我们如何评估对样本体验的置信度呢？我认为这取决于我们需要多少训练数据来构建模型。是的，正是这样。所以我们如何知道自己的置信度，总的来说，这取决于你的模型的表现，这可能非常困难，尤其是在数据量有限的情况下。因此，我们正在讨论一些探索技术，可以根据模型的质量来驱动这些置信区间，它们同样适用于这种情况。所以，如果你只有非常有限的数据，你可以使用类似Hoeffding不等式这样的方法，来评估我对这个模型奖励的确定性程度。

在今天的大部分时间里，我们不会谈论太多，但可以利用这些信息来尝试量化自身的不确定性程度，以及这种误差会如何传播。是的。我正在思考我们正在构建的模型的下一步概念是什么。我们将采用一种学习某种政策或类似在现实世界中采取行动的方法。如果我们有了模型，那么在实际行动时是否可以使用该模型，并且通过模型来管理我们的状态，并采取最大化奖励的行动？因此，一旦获得模型，就可以以多种方式使用它进行规划。比如，如果是个小问题，比如在桌子上放一个小箱子，你可以这么做。这样你可以使用值迭代算法来准确解决问题，而无需模拟数据。但是，当你面对如Atari等真正高维度的问题时，仅仅进行规划就变得非常昂贵。因此，在这种情况下，你可能仍希望使用其他方法。

使用模拟数据进行无模型规划可能是为了提高样本效率。例如，可以通过从模拟模型中生成大量数据并反复重放这些数据来更充分地利用数据资源。这种方法可以用来规划，即使不做马尔可夫假设，也可以使用过去数据作为条件来构建具有历史功能的完整模型。然而，数据量有限时，特别是随着时间的推移，估计转换模型将受限制。因此，需要权衡考虑是否追求更好的模型，这取决于具体领域的要求，也许在某些情况下马尔可夫假设是合理的。

如果不是真正的马尔可夫过程，您是否会考虑使用少量数据来训练更优秀的模型呢？在这种情况下，通常涉及到函数逼近误差和样本估计误差之间的权衡。由于数据量不足，您可能会出现两种错误情况：一是由于函数逼近不准确造成的错误，二是由于样本估计误差造成的错误。因此，在这种情况下，若您使用蒙特卡罗方法对新数据进行评估，您可能会得到与原始数据应有的结果不同的采样体验。但是，若您对原始数据运行时序差分学习，可能会得到非常相似的结果。基本上，我们不要求100%贴合真实经验，而是使用蒙特卡罗策略评估。另一种方法是，我们假设系统是马尔可夫的，然后生成一堆数据进行模拟，再对其运行蒙特卡罗学习或时序差分学习，这样也可能得到相同的答案。所以，您知道，这可能与之前蒙特卡罗方法收敛到的V(A)值不同。

现在或许可以这样再次解释，如果我真的不相信系统是马尔可夫的，那我在虚假数据上运行蒙特卡罗就没有意义。但这说明了一点，一旦你拥有了样本，你就有了选择的可能性。因此，也许您会想要采用双步马尔可夫过程，或者在模型中做出不同的假设。然后，您可以根据需求进行各种规划。您可以先收集数据，构建模型，然后再决定如何利用它进行规划。很快我们将看到一种特殊的方法来实现这一点。现在，正如你们刚才问到的，如果我们拥有一个糟糕的模型，通常就会得出次优的策略。有时我们可能会非常幸运，因为最终为了做出决策，我们只需确定 V(s, a) 的值即可。

总的来说，如果你的模型很糟糕，并且你进行了总体规划，那么你的政策也会受到影响，不会是最佳的。一种解决方案是对原始数据进行无模型的强化学习，而不是基于模型的规划。目前尚不清楚这种方法是否总是有效解决问题，因为取决于为什么你的模型出现错误。如果模型出错是因为选择了错误的参数类别或者系统不符合马尔可夫假设，那么进行的 Q 学习也无法解决问题，因为 Q 学习是基于世界符合马尔可夫性质的前提。因此，无模型方法的成功取决于出现错误的原因。另一个方法是明确推理模型的不确定性，这需要考虑探索和开发。这种方法只适用于特定类型的错误，涉及到可能存在抽样估计误差的情况，但仍然基于一个基本假设，即你的模型类别是正确的。

举例来说，如果你将世界建模为一个多项式分布，但所拥有的数据较少，那么你之前的指标估计可能会有偏差。但是，如果事实上世界并不是一个多项式分布，那么所有的打赌都会失败。因此，理解我们在算法中做出的假设以及所面临的不确定性形式总是明智的。现在我想说的另一件微妙而有趣的事情是，如果你拥有一个非常好的模型，通常情况下，或者说如果你有一个完美的模型和准确的参数估计，那么就足以做出正确的决策。然而，当你尝试训练模型时，如果模型的数据或表达能力有限，那么在使用它做决策时，具有更高预测准确性的模型实际上可能会更糟糕。

我喜欢的直觉是，多年前我们发现了这一点，其他人也意识到了这一点，当我们在考虑一个智能辅导系统时，这是一个挑战。想象一下你面临一个非常复杂的状态空间，比如，试图模拟一个人在厨房泡茶的场景。

AI 智能体拥有各种功能，比如能感知蒸汽、观察日落，或测量水的温度。举个例子，泡茶需要水温超过100度。泡茶的关键在于要注意水温。然而，如果你试图构建一个世界模型，尝试模拟日落、蒸汽等现象，你可能需要考虑很多特征来捕捉这些事物在网络中的表现。因此，仅仅依靠最大似然估计模型可能无法包含做出决策所需的所有要素。一个在预测方面表现良好的模型，不一定在决策方面也会表现好。几年前我们就遇到了这个问题，这指出了构建决策模型并不一定要与预测准确性模型完全一致。因此，了解到底哪些功能对你所关心的实际用途和价值至关重要。好的，接下来我们讨论基于模拟的搜索。

在班上有没有人听说过前向搜索算法？其实有一些人听说过，但并非所有人。我想讨论的是前向搜索算法，而不是使用Q学习对模拟数据进行训练。假设我们已经有了一个模型，现在想尝试另一种决策方法，那会是什么呢？一种方法就是前向搜索。那么前向搜索如何运作呢？前向搜索的理念是考虑所有可能的行动。比方说，假设只有两个动作可选，A1 和 A2。然后，我们会考虑所有可能到达的下一个状态。在这个小世界里，假设只有S1和S2两种状态。因此，在当前状态下，我可以选择执行动作一或动作二，之后可能会过渡到状态一或状态二。接着，当我了解了所有这些信息后，我得到了状态，我知道我再次可以做出决策选择A1或A2，因为这就是我的动作空间。再之后，根据我的选择，我将再次进行状态转换，或者有时到达终止状态。这就是前向搜索算法的基本原理。

也许我的 AI 智能体会面临失败或错误，或者我可能会前往其他地方。在这种情况下，我可以想象未来的各种可能性，随心所欲地探索这些可能性。举个例子，假设我想考虑接下来的 h 个步骤，然后我停了下来。一旦我拥有了这些信息，我就可以利用这些信息以及奖励信号来思考。在进行这些模拟推演时，我可以考虑奖励信号在我认知不同情节时的变化。由于我假设拥有一个模型，因此可以考虑如果处于状态t时采取行动a2会带来怎样的奖励，或者如果处于状态s2时采取行动a2会有什么样的奖励。通过生成不同的特征并综合奖励信号，我可以沿着不同的路径获得一系列的奖励，以便更好地理解情况。

在这种情况下，您在讨论如何在不同行动类型中做出最优选择时，了解到对于某一特定状态，采取最佳行动对期望值具有最大影响。您提到了拥有一个模型，用以评估家长的行为和自身状态对达到特定状态的概率。在这种情况下，您可以考虑将潜在的未来情况推演到一定的深度，类似围棋比赛中的赢或输的情况。您提到希望备份这一决策。可以将这一思考过程看作一种动态规划的形式，尽管有些效率不高，因为可能存在许多相同的状态。您会分别考虑不同的行动序列和结果状态，以评估在每个状态下可能获得的未来奖励。

接着，若要弄清楚下一步该如何行动，我会从我的叶子节点开始，对所有叶子进行最大处理。所以假设在这种情况下，我只是增加了一个小符号。这个符号是a1、a2，假设在这时我停止。所以我获得了 r(s, a1)。

假设您拥有两个状态 s1 和 s2，其中一个状态的价值为 s1，另一个状态的价值为 s2。您希望确定对于某一动作，您的新价值会是多少，这取决于给定 s1 的概率。假设您当前处于状态 s0，对于动作 a1，乘以 V(s1)。

人们经常谈论极小极大树。在博弈论中，其他AI智能体会试图最小化你的价值，而你会尽力最大化它。这与动态规划非常相似，只是效率较低。但我们很快就会明白为什么要这样做。有人有疑问吗？好的，那么让我解释一下。我们这样做是因为我们需要一个模型，如果没有模型，我们无法精确计算期望值的最大化。我们知道，就像我们只扩展了两个状态一样。为了确定给这两个状态分配多少权重，我们需要知道到达每个状态的可能性。这就是我们使用模型的地方，我们使用模型来估计奖励。因此，基于模拟的搜索类似，只是我们只是用模拟来模拟，而不是计算所有这些数量呈指数增长的期货。

相反，我们只是在这里说我要从这里开始，我有一个模型，我需要在这里制定一些政策。但假设我有一个策略 𝜋，然后我就使用它。因此，我查看了当前状态的政策，它告诉我要做一些事情。所以我遵循了这个动作，然后进入我的模型并在 s' 进行采样。所以我查找我的模型，然后说：“考虑到我处于这个特定状态并采取了该操作，并且我只是模拟下一个状态，下一个状态会是什么？”这就像我们之前从模型中模拟数据一样。假设这让我到达这里，即状态 s1。然后我再次抬头。我查看我的政策并说：“s1 的政策是什么？”假设这是 a2，然后我也 - 然后我按照它到这里。所以只需模拟出一条轨迹即可。只要遵循我的策略，模拟它，直到它终止。这让我再次感受到这项政策有多好。因此，在这种情况下，我们可以用模型模拟完整的轨迹。啊，一旦我们有了这些，你就可以对

在进行无模型强化学习中，可以使用蒙特卡罗或者TD学习等方法进行模拟轨迹。因此，在进行这种模拟时，我们需要一种策略来指导行为。当我们考虑处于某个状态并选择一个行动时，需要一种方法来在模拟世界中做出选择，即如何确定要采取哪个行动。我们遵循当前的模拟策略，假设我们希望有效地改进政策。因此，你拥有一个策略，拥有你的模型，然后从一个状态出发，为每个可能的操作模拟轨迹，就像进行蒙特卡罗推广一样。我会从某个状态开始，然后在真实世界中采取行动之前，在头脑中考虑可能采取的所有行动，然后基于行为策略pi从每个动作中产生值，最后选择最大值。

所以当我处于状态 s_t 时，在我的大脑里考虑了采取动作 a_1，然后在根据策略 pi 的指导下进行了多次尝试。接着尝试了动作 a_2 并进行了多次尝试，再接着是动作 a_3，这整个过程在我的头脑中进行，基本上给了我在策略 pi 下动作 a_1 时状态 s_t 的 Q 值的估计。所以就像是当我考虑采取某个行动并遵循策略时，我的 Q 函数是多少，然后对每个行动这样做，再选择最大值。这有点像进行政策改进的一步，因为这取决于我们用来模拟体验的模拟策略是什么，这有意义吗？所以我们有一些预先存在的模拟策略，我并没有告诉你们我们是如何得到的，然后我们用这些模拟策略来模拟体验。好的。问题是，我们是否真的可以比政策改进一步做得更好，因为我们如何获得这些模拟策略呢？如果我们有一个效果良好的模拟策略，我们可以执行这一步，但我们如何在更通用的环境中执行这个操作呢？这个想法是，如果你拥有这个模型，

你实际上可以利用这个 Expectimax Tree 来计算最佳价值。 在处于状态 St 时，我们不仅考虑当前的行动，而是思考在 Expectimax 树中，如果选择行动 a1 会怎样？或者选择 a2，然后转移到 S1 还是 S2，然后下一步该怎么办？基本上，我们尝试在当前状态的近似模型下计算最佳的 Q 函数。然而，问题在于这棵树会变得非常庞大。通常来说，这棵树的大小至少会随着状态数 S 乘以行动数 A 指数级增长到深度 H。如果深度 H 达到水平线，因为在每一步中，动态规划会考虑所有可能的下一步状态，并考虑所有可能的下一步行动。因此，这棵树的增长是指数级的。就像 AlphaGo 这样的系统，它在下棋过程中，你知道，在...

在某人获胜或失败前，这个 H 值可能会在50到200之间。 如果状态空间非常大，则完全考虑每种情况几乎是不可能的。 因此，蒙特卡罗树搜索的思想是要寻求更好的方法。 无论如何，要实现这一点，我们需要一种模拟策略，而且我们不能像用Expectimax那样完全计算。 那么我们该如何在两者之间折中呢？ 因此，蒙特卡罗树搜索的想法就是设法兼顾两者，尽管我们真正想要的是Expectimax树，其中我们考虑所有可能的未来并选择最优解，但是我们需要以更易计算的方式来实现这一点，为什么会有这种可能性呢？ 好吧，让我们思考一下。 假设我们有一个初始状态，它指向所有这些节点，其中一些可能的行为真的很糟糕。 因此，有些情况可能会很早地变得明晰，比如使用少量数据。

在围棋游戏中，假设你想要做出最佳决策，你可能需要考虑搜索整个决策树，或者只是轻微扩展它。事实上，如果你选择搜索整个树，你会立即陷入计算困难，因此当有更好的方法可供选择时，你无需花费大量精力来完全扩展这棵树。我们的直觉告诉我们，构建部分决策树是更明智的选择。因此，我们从当前状态开始，通过采样下一个状态的操作，就像模拟搜索一样，来逐步填充决策树。可能我们先选择动作A1，然后再选择状态S1，接着选择动作A2。因此，初始阶段看起来与模拟搜索完全相同，但我们的想法是，重复执行此过程并逐渐填充树。也许下一轮我们偶然选择了动作A2，然后选择了状态S2，再选择动作A1。这样，您可以逐步填充这个Expectimax树。在极限情况下，您会填满整个Expectimax树。然而，在实际应用中，您几乎永远不会这样做，因为这在计算上是非常困难的。

因此，我们要做的就是对一些场景进行模拟，每个场景都可以看作是从头开始的。你所处的是根节点和当前状态，然后你一直扩展直到达到终止状态或水平 H，然后重新回到起始状态并尝试另一个路径。当完成所有这些后，你可以像在 Expectimax 中那样采取行动的最大值和状态的期望。你只会填充树的一部分，因此可能会有一些树节点丢失。为此，有两个关键方面。一是，对于已经有一些数据的树节点，要做什么？所以，如果你已经尝试了这两种操作的某个状态，你应该选择哪个操作，当你到达一个没有尝试过其他操作或者只尝试了一种操作的节点时，你应该怎么做？这通常被称为树策略和转出策略。

零样本策略适用于当您到达一个节点时，您知道只尝试过一项操作，或者没有更多数据，或者您以前未曾到达过该节点。举例来说，可能您探索了一个之前未曾到达的状态，这样它就成为一个新节点，从那时起您便执行零样本策略。接下来我们将展示一个案例。之后我们的想法是，在计算Q函数时，我们只需对从该节点获得的所有奖励进行平均。这看起来有些奇怪，因为我们不再谈论最大值，也不再谈论采取什么具体行动——以正式的方式明确考虑状态的期望，我们只是对其进行平均。之所以可以这样做，是因为，嗯，我们将以某种方式对动作进行采样，随着时间的推移，我们将采样看起来更好的动作，因此我们期望，最终，数据的分布将收敛到真实的Q值。是的。只是为了确认一下，您是否在说之前的模拟中，嗯，有不同类型的平均和移动部分，因为看起来我们之前。

我们通常会进行一系列推演，然后将结果组合在一起。因此，即使这些推演是相似的，但它们并不完全相同。我们会进行多次模拟，并对结果取平均值。关键问题在于我们使用的策略会随着每次推演而变化，而不是保持不变。在对这些结果取平均时，采用的方式也是不同的。重要的是，并非使用简单的蒙特卡洛搜索，而是修正策略并从中获得多个推演。初始行动可能各不相同，但随后会遵循特定策略。在蒙特卡罗树搜索中，当进行 k 次推演时，每次推演的策略通常会有所不同，这是有意为之的，以期获得更佳的解决方案。整个过程中，您的代理，也就是我们的AI智能体，旨在找到应该采取的最佳行动。

在现实世界中，我们会得到一个 S 素数和一个 R 素数，接着我们将讨论的是现在AI智能体需要在头脑中做的所有事情，比如推导出所有可能的情况，以便做出下一步行动的决定。因此，在采取下一步行动之前，AI 智能体会制定许多计划，通常可能会放弃整个计划，此时世界会给予一个新状态和新的奖励，然后AI 智能体会重新开始整个过程。因此，关键在于决定下一步应该采取什么行动。根据我们当前拥有的信息，我们希望以一种能够获得最佳预期价值的方式来做出决定。在蒙特卡罗树搜索中，通常您可能还需要执行另一个步骤来计算新模型。因此，如果您在线上执行此操作，您可能会获取最新的数据，重新训练您的模型，并让该模型再次运行蒙特卡罗树搜索，然后再决定下一步该采取什么行动。因此，关键在于树的策略和推演。

这些方法都会产生影响。在最基本的普通版本中，推出通常是随机完成的，因此有许多扩展，其中关键部分是树策略。因此，最常见的方法之一称为上置信度树搜索，这与我们过去几周讨论的探索相关。我们的想法是，在我们推出时假设我们处于状态 s_t，我们正在使用模拟模型来思考我们将采取的下一步行动和状态。换句话说，当我们做出决定时，我们会考虑过去采取的行动，例如我已经采取了动作 a1 和 a2，并做出了各种尝试，从中得到3次胜利。所以三个1代表了胜利，一个1和一个0代表了不同的结果。因此，在遇到状态 s1 时，我们会根据迄今为止获得的数据持乐观态度来决定下一步行动。可以将这种方法看作是一种强盗策略。

对每个决策点将其视为一个独立的实体，就像一个单独的强盗一样。然后，考虑当前状态下已经采取的所有行动，计算出每个行动的平均奖励是多少，以及这个信息是从特定节点的图表和操作中获取的次数是多少。在这个过程中，您可以通过图表A1中的经验平均值获得状态1的数据，然后乘以一个折扣因子，通常代表你在特定节点的访问次数。实际上，这是针对图中的一个节点进行的计算。因此，我们会考虑每次进入该节点并执行特定操作时获得的平均奖励是多少，以及这个计算是基于多少次尝试。这种方法能让您持乐观态度，考虑到在到达图表不同部分之前，模拟模型看起来怎么样，然后集中精力于加强这棵树的相关部分。因为我认为...

或许能够得到更有价值的政策。 这意味着我需要更少的计算来确定一个好的策略。这是合理的吗？ 就好像有一个预言机，可以告诉你最佳策略是什么，你只需要填写决策树的部分。我们在这里所使用的是，基于迄今为止所见数据的推测，我们会关注树中看起来即将发生的部分，通过采取最佳行动，我们最终会获得这些价值，然后将这些价值传播回根节点。因此，我们保持对每个动作的奖励的置信上限，这和之前所做的完全一样。我们将每个节点视为一个独立的强盗。这意味着，下次到达树中的同一节点时，我们可能会做出不同的决策，因为情况可能会发生变化。TD方面可能也有所涉及，听起来有点像TD。

在某种意义上，如果你考虑奖励强盗问题，那么国家的价值，或者说现有的行动，是在思考这种转变所带来的奖励吗？这是一个很好的问题。那么，这个强盗的奖励指的是什么呢？我们将它视为从该节点的全面推导而来，因为这是我们正在平均的，我们正在计算的。这与TD方法有所不同，我们是在每个节点上执行此操作。正如之前提到的，您可能会看到s1、a1在图表的这一部分，而s1、a1在另一处，我们不会将它们合并到一个账户中。我们将每个节点视为完全不同的。尽管我们通常使用马尔可夫模型来模拟，但在树的内部排序中，您可以做到这一点，这使得实现变得更加复杂。如果您想象将其基本上视为图表而不是树。现在，我认为您提到的另一点，我们将在一会儿回到这个话题，是一个很好的想法[笑声]。很好，那么强盗设置的局限性是什么？我们稍后再回到这个话题。好的，让我们在讨论这个问题时以围棋为背景。对于那些不熟悉围棋或者从未玩过围棋的人来说。

这个游戏是围棋，有着至少2500年的历史。被认为是最具挑战性的经典棋盘游戏之一，在人工智能领域长期被视为重大挑战。围棋是一种局面复杂、搜索空间巨大的游戏，需要考虑各种可能的棋盘局面。在围棋中，玩家以两种不同的石头标志移动，通常在19 x 19的棋盘上进行，但也有人尝试在较小的棋盘上进行游戏。游戏的目标是占领尽可能多的领土，因为放置棋子的位置是有限的，围棋被认为是一个有限范围的游戏。在围棋中，玩家需要制定奖励函数，来帮助在已知规则下做出最佳决策。

哦，你知道，不同的方法可以实现不同的功能。 最简单的方法是检查游戏中是白方获胜还是黑方获胜。在这种情况下，这是一个非常稀疏的奖励信号，你只能在游戏结束时获得奖励。你只需要一直玩下去，看看哪一方取得了胜利。因此，在这种情况下，你的价值函数本质上是指当前状态下获胜的预期概率。那么，如果我们进行蒙特卡洛评估，它是如何工作的呢？

让我们设想一下这是你当前的棋盘局面，你有特定的策略。通常假设对手是静止的。然后，最后你会看到结果，也许你赢了两次，输了两次。那么，当前初始状态的价值就是50%。好的，那么在这种情况下，我们如何进行蒙特卡罗树搜索呢？

所以你一开始有一个单一的初始状态，此时你尚未模拟树的任何部分，因此你还未采取过任何行动。

所以在你只进行随机采样的情况下，你可能选取了a1。然后你遵循默认策略，通常这是随机的。尽管对于像AlphaGo这样的情况，你通常希望有更好的策略，但是你也可以只使用随机策略，这样你会继续采取随机动作来到达下一个状态，直到最终你会看到自己是赢了还是输了。在这种情况下，这是双人游戏。因此，我们建立了一个极小极大树，而不是期望极大树，但基本思想是完全相同的。这是我的第一次展示。在我搞清楚如何真正展示我的作品之前，我会进行很多这样的尝试。好的，那么接下来，这是我的第二次展示。这是我脑海中第二次出现的声音[噪音]。然后说，上一次，我采取了这个行动。所以现在我有了我的默认策略。这次我打算采取其他行动。通常来说，您希望尝试填写所有操作，至少从当前节点。您尝试操作的特定顺序可能会产生很大差异，早期通过尝试操作顺序的启发式可能会节省大量成本。

现在假设您必须尝试每个动作，一次。在这种情况下，一旦您尝试了某个动作，就不会再尝试该动作。这意味着您只需执行一次推动。简单来说，您在重复执行这个过程。当您到达某一步时，如果我之前尝试过这个动作，我会使用我的UCT选择最佳动作。我会考虑这个动作的奖励以及计数信息。这个是Q明确描述的。所以我会选择在此部署中看起来最好的动作。然后，我会专注于扩展这部分树。您继续这个过程，逐步构建出树，直至耗尽您的计算预算。然后，您会回溯到树的底部，逐步返回到每个动作节点，并选择最大值。每个状态节点都在考虑一个期望值，或者说您在运用极小极大算法。

您只需构造一个法学硕士。那么，对手会做什么？就像是一个静止的对手，这意味着什么？很好的问题。好的。所以，实际上，我认为关于人们为什么去上班的另一个真正重要的见解之一是自我对弈。因此，在这种情况下，您通常会使用当前的智能体作为对手。我会采取我刚刚为对方计算的任何政策。看，特别是，让我们想象一下，我保留了那棵树。所以它已经知道它能做什么。在每一个点上，我都会让另一个智能体告诉我在那种状态下它会采取什么行动。其中一个真正如此，所以我认为自我对弈是一个非常重要的见解。为什么它如此重要？因为如果我和一位围棋大师对弈，我会在很长一段时间内得不到任何奖励。这对于代理来说是一件非常难以学习的事情，因为没有其他奖励信号。基本上，你只是在玩大量的游戏，然后很长一段时间都没有信号。因此，您需要某种信号，以便您可以开始引导并真正制定出良好的政策。如果我像

五分钟前那个人如果和我比赛，我可能会战胜他们[笑声]。嗯，至少有一半的机会我会胜出。这种情况是可以接受的，因为即使有两名球员表现都很差，也会出现一名赢家和一名失败者，从中你可以获得信号。因此，在游戏中进行自我对弈的概念是非常有用的，尤其是在双人游戏中。这有助于你开始接收有关成功或失败的一些奖励信号，从而帮助你解决这种稀缺奖励信号问题，并且提供学习机会。因为你总是在一个比你当前水平稍微困难的环境中挑战自己。实际上，我认为，如果我们能够将同样的概念应用于许多其他领域，那将会非常棒。就像在医学等领域进行自我对弈，或者客户关系等领域[笑声]。这样做很棒，因为通常很难获得所需的奖励信号，这正是其中的一大乐趣。在自我对弈中，如果我们陷入局部最优解怎么办？这是有可能的，是的。那么，在自我对弈中，如果你陷入这种困境，将会如何应对呢？

你总是在努力实现最大化，这有点类似于政策改进，不断努力做得更好。你不断努力追求胜利，但会遇到这样的情况：或许你赢了一半的时间，但接下来可能会有更多可以利用的机会。只要有计划，你就能够识别这些机会。你是否认为在不断与自己对抗、转变为更专业的玩家过程中会获得额外好处？起初你可以慢慢开始，轻松进入，然后逐渐挑战更难的模式以加快学习速度。你可能会面临这样的问题：是应该与五分钟前的自己比赛，还是在某些时刻努力与某人竞争会更有效？这是一个很好的问题，有时候可能需要更大的挑战跳跃，这样会加速学习。但要找到最佳点是比较棘手的，因为你仍需要足够的奖励信号来指引你。

为了构建高度选择性的最佳优先搜索树，我们需要以一种非常具体的方式来构建它。目标是要比Expectimax更高效地制作整个树，在这个过程中比简单的一步策略改进要更出色，采用一些特定的、基于模拟的策略。这种方法可以并行进行，无论你有一分钟还是三个小时来计算下一个动作。如果你需要做出客户推荐、做出重要决策，或者每天都要做出决策的情况下，三个小时的计算时间是非常现实的。你可以通宵运行八个小时，然后得出决策。这种方法允许你充分利用你的计算资源优势，不论情况需要多快，总是能提供答案，因为你仅仅需要进行更少的推演。不过，有一点需要留意，即在每个节点都做强盗动作有些奇怪。直觉上，这种行为看起来有点怪异，因为在强盗游戏中，为什么我们要在不确定性下表现得如此乐观呢？

我们之所以这样做，是因为我们实际上害怕做出错误的决定导致的后果。在面对不确定性时，保持乐观的想法是，无论结果如何，你要么得到了丰厚回报，要么学到了一些东西。奇怪的是，即使我们在头脑中做出了错误的决定，我们也不会感受到痛苦。基本上，我们只是希望了解根据信息价值应该采取什么行动，这样我们就能确定正确的路径。因此，如果我们模拟出了不良行为，其实并不重要，只要它能帮助我们更好地做出决策。我们只是用一个非常简单的例子来阐明可能存在差异。假设这是Q的潜在价值，A1的值是这个，A2的值是那个，我们面对的是不确定性。如果你持乐观态度，你总会选择这个，因为它价值更高。但如果你想确保A1更好，你应该选择A2，因为这样做可能会更新你的信心区间，让你更加确定。

虽然对于确定 A1 是最佳选择有着绝对的信心，但这种方法并不总是完全有效的。有时候，由于可能采取错误的行动会带来高昂代价，因此会选择 A1。但如果最终你只需知道正确的行动是什么，有时在计算方面会选择 A2，因为此时置信区间可能分歧较大，无需做额外的计算。虽然不确定每个节点的最佳策略是什么，但这种方法非常有效。关于围棋，我们基本上就讨论到这里。还有一些优秀的论文，探讨了新的扩展和应用于国际象棋等多种游戏，这些成果真的令人惊叹。我强烈建议您阅读其中一些论文。最后，让我简要回顾一下本课程的内容，因为这是最后一节课。本课程的目标是什么，在结束时，一些人会有机会在周一进行练习。

我认为你希望从中得到一些关键的信息。 那么问题是，强化学习相对于其他技术的主要特点是什么？与人工智能和监督学习不同，代理可以自主收集数据并在环境中做出决策，这使得审查数据和监督学习的 IID 假设变得不同。强化学习与计划也有很大不同，因为它需要依赖从环境中获取的数据来做出决策。在很多情况下，解决如何把一个问题表述成强化学习问题是很有挑战性的，这可能会对解决问题的难易程度产生重大影响。对许多人来说，了解如何描述患者、学生或客户的状态空间是一项艰巨的任务。

函数逼近和样本效率是一个重要问题。如果我平等对待所有客户，那我将会拥有大量数据。但这样可能得到一个糟糕的模型。因此，在这些情况下会有许多权衡考虑，我相信大家都会思考一些有趣、令人兴奋的新方法来解决这个问题。另外，你们对一些常见的强化学习算法非常熟悉，并已经实现了很多。要了解我们如何评估强化学习算法的好坏，无论是从经验、计算、计算复杂性、数据需求还是性能保证等方面，了解并探索挑战的利用是强化学习所独有的。这在规划或机器学习中都不会出现。重申一遍，如何快速收集数据以做出正确决策是一个关键问题。如果你对强化学习更多了解，还有许多其他课程可供选择，尤其是Mykel Kochenderfer 的一些很好的课程。Ben Van Roy 也值得关注。

我想说的是，我认为我们已经看到了一些惊人的成就，比如围棋领域的成功。机器人技术带来了一些令人兴奋的进展，但我认为大多数人的手机上还没有像人脸识别那样普及强化学习功能。因此，我认为在许多其他应用中应用这些概念仍然具有巨大潜力。如果你能探索这个领域并做一些工作，我会很乐意听取你的回馈。在我的实验室里，我们一直在思考如何将强化学习应用到其他领域。 我认为另一个关键的问题是，当我们设计这些强化学习代理程序时，如何以安全、公正和负责任的方式进行。因为这些系统通常会成为一个循环系统中的一部分。 因此，允许代理程序展示他们的推理过程和局限性将变得至关重要。最后，我要强调的是，获取你们的反馈对我们非常有帮助。 这有助于我们在未来几年改进课程，确保我们继续做出有益的举措，或停止那些不够有帮助的举措。如果你能花大约10分钟的时间提供反馈，将不胜感激。

完成课程评估后，请补充以下内容，让我们了解哪些方面有助于您的学习，明年哪些方面可以改进。您的反馈对我们非常重要，感谢您的配合。谢谢！[掌声]



