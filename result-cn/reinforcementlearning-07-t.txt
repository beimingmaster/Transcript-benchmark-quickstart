好的，我们的作业二可能已经开始了。如果您没有深度学习背景，本周我们将举办一些课程，如果有任何问题，请随时在Piazza上联系我们。关于这个项目，我想确认一下，目前在Piazza上默认建议选择I-50。是否还有其他选择？在Piazza上有一则帖子欢迎大家设计自己的项目。这是完全可以的，你可以跟其他助教交流或者与我们讨论。这算是额外的选择。如果你对昨天发布的关于强盗和华法林的默认项目感兴趣，想了解高年级博士生或博士后的一些建议，这是个很好的机会。特别是，如果你以前没有做过强化学习项目，我不会期望你在短短三周内就定义出一个先进的项目。所以，如果你对深入了解强化学习研究感兴趣，这可能是一个不错的机会，可以查看一些相关内容。

我们建议的项目与人们进行接触后，很好。我想友情提醒另一件事，即我们已明确发布了每个作业的常见问题解答。有些助教提到，一些学生可能在办公时间之前没有机会看到这些内容。因此，如果您在复习作业时遇到问题，首先要做的就是去Piazza，特别是查看顶部固定注释，那里包含与作业相关的常见问题解答。请务必在来参加办公时间前阅读这些内容，然后当然也欢迎您来参加办公时间。这些都是非常有价值的资源。还有其他问题吗？关于课程进度，我们上周一完成了DQN。今天我们要深入讨论一些关于深度Q学习和最近扩展的内容，总结我上周一匆忙完成的内容。然后，在下周开始讨论策略梯度方法之前，我们将讨论一些关于模仿学习和大状态空间的问题。因此，我们首先回顾一下DQN的内容。

DQN是将Q学习与深度神经网络作为函数逼近器相结合的概念。相较于之前的研究，该算法的两个关键变化是引入了经验回放和固定Q目标。通过固定Q目标，意味着在更新Q值的过程中，我们会使用固定的目标网络，即在计算Q值时所使用的权重暂时保持不变。因此，我们可能会每隔100步、每隔50个周期或其他时间间隔更新一次目标网络。这为监督学习提供了一个更稳定的目标，因为监督学习的部分可以被看作是在最小化由权重对当前估计造成的误差，即最小化TD误差的过程。

因此，我们首先在重放缓冲区中恢复Transformer。我们进行小批量处理，从一组状态-动作对中采样额外的Token和下一个状态元组，然后进行备份，更新我们的Q函数并重新拟合。就像许多线性值函数方法一样，它使用随机梯度下降。真正酷的是，他们在50轮比赛中做到了这一点。他们对这50轮游戏使用了相同的架构和超参数，并且达到了人类水平的表现。所以这部分我们之前已经讨论过很多了。然后我们简短地讨论了接下来几年的三种主要扩展。再次强调，当前深度强化学习已经有许多扩展和大量工作。其中三种方法如下。第一个是双Transformer。我们在之前提到过，在处理函数逼近时讨论了最大化偏差的问题，当您使用相同的表示来选择一个动作并估计该动作的值时。

在避免最大化偏差问题上，Double DQN提供了一种解决方案。我希望再次讨论这个问题，因为在课后有几个问题需要澄清，而我们之前没有太多时间讨论。我们所面对的情况是，我们有一个当前的Q网络，它由一组权重参数化，用于选择动作。通常我们会采用某种E-greedy方法，以一定的负epsilon概率选择当前Q网络权重下的最佳动作。另外还有一个较旧的Q网络用于对这些动作进行评估。因此，当我们考虑权重如何变化时，我们会使用这些较旧的权重w来评估动作，并用这些权重W来选择动作。这可能会让人觉得和DQN所做的非常相似，因为DQN是使用固定的权重集进行目标更新，即采用r加Gamma乘以Q的方式。

在标准深度 Q 网络 (DQN) 中，通常会使用一个权重更新的方式，即将目标网络的权重 w 减去当前网络的权重 s，这个方法在双重深度 Q 网络 (Double DQN) 中可能会有所不同。不同之处在于，双重深度 Q 网络允许您保持两套权重，并且可以在每个步骤或批次中轮流使用它们。因此，当引入 DQN 时，它更多地是一个固定权重的概念。比如，在时间步 t 到时间步 t + 100 的整个时间段内，目标网络使用相同的权重。而在双重深度 Q 网络中，您并不一定需要这样。您可以在这两套权重之间来回切换，这就是我们在双 Q 学习中看到的情况。换句话说，在第一步时，您可以使用权重 1 进行操作，使用权重 2 进行评估。

在第二步中，您可以使用权重二进行评估，使用权重一进行操作。 这意味着您可以更快地传播信息。 因此，不必等待 50 次或 100 次更新，您可以即时更新权重，这样就可以在目标权重之间切换，从而可以更新两个网络的权重。这两个网络是相同的。通常来说，当评估不同的方法来改进这些技术时，是否需要权衡信息传播速度和信息稳定性呢？因此，我们可能会发现，如果要学习的系统本身相对较好且稳定，我们希望选择信息传播速度更快的系统；但如果系统存在较大噪音或不稳定性，我们可能需要更稳健的方法。这是一个很好的问题，这些方法通常需要在系统稳定性和信息传播速度之间进行权衡。不幸的是，我认为这个特性目前并不十分完善。

因此，在大多数情况下，人们对深度神经网络的评估是基于启发式方法和不同的评估基准，以此来归纳总结。然而，在如何系统地评估深度神经网络在稳定性方面的表现，特别是在强化学习环境下，目前仍缺乏一个合适的系统化方法。因此，目前存在很多机会来进行理论分析或更正式的理解。目前的观点是，在 Atari 游戏中和可能还包括 MuJoCo 中，初始的方法似乎一直有效，但也许并没有尝试描述成功的特征。是的，没错。我想了解更多关于切换的信息，包括为什么切换以及如何切换。是的。所以，问题是，我们如何在这些 w 和 w 减之间切换，以及你将如何描述为什么以及如何进行这种切换？在深度 Q 网络（DQN）的设置中，你可以设定 w 减。开始时，w 减等于时间步长为零时的 w。然后在接下来的 50 个集合中，您可能会保持 w 值相同，但会进行更新。然后再过 50 个集合，

我们之前讨论过的问题是您没有利用您获取的信息来更新参数 w。因为您一直在使用旧的参数集，所以本质上您没有利用在这 50 个步骤中获取的信息来更新对下一个状态 S' 采取行动 a 的评估。另一种方法是考虑保留两组不同的参数。您可以将其想象为两组权重，一组用于选择操作，另一组用于评估操作。比如，在时间步骤一中，您可以使用其中一组参数来评估操作并使用另一组来选择操作，然后在下一个步骤中切换。这样做会导致两组参数都频繁更新，而不是每 50 个步骤更新一次。

继续迅速传播这一信息。在这里，有许多图表可供选择，关于您更新频率的事情，您会在这些图表之间来回切换。您可以把这些都看作您可以调整的超参数。但这并不是要保持这个目标固定50步，或者N步，这些都是参数，您可以在它们之间来回切换，这就是在双Q学习之前所做的事情。就像在普通的DQN设置中一样，当我们使用目标权重时，用于动作选择或类似评估的目标权重不就是另一个排队网络吗？那么，除了您更频繁地搜索双Q之外，这与双DQN有何不同？这是。或者更多地是——问题是，今年与去年有什么不同？几乎是一样的。所以，我认为这里的主要区别在于，只要您为您的目标保持一些权重，您就可以切换。这意味着您可以进行某种切换。现在，您实际上拥有相同的网络，需要在内存中维护的两组权重。这也就是说，您可以非常频繁地来回切换。

在这段时间内，帮助减少偏差最大化的方法往往是有帮助的，尽管并非总是有效。尽管稳定性仍然存在问题，但仍有改进的空间，这有助于避免最大化偏差。我们还讨论了优先体验重播的概念，通过一个小型表格示例来说明备份对结果的影响。在使用经验重放缓冲区中的 SAR S-prime 元组时，我们应该选择哪一个进行备份，以及如何传播这些信息呢？根据这个算法或者这篇文章提到的一个事实，如果能以最佳方式执行，有时可以获得指数级的加速和收敛，尽管这很困难且计算量巨大。因此，建议根据DQN错误的大小确定某些内容的优先级，即当前估计值与目标估计值之间的差异。因此，我们讨论了如何将这些差异用作优先级的方式，有可能是随机选择的方式。

我们还讨论了另一个事实：将 Alpha 设置为零会使得分布均匀，因此元组没有特定的优先级。 另一个我们几乎没有时间深入讨论的主题是关于决策。 因此，决策是在2016年的ICML会议上被评为最佳论文。 让我简要回顾一下，因为我们讨论速度非常快。 在这种情况下，直觉是，为了决定政策的最佳行动，您可能需要编写一个状态值函数，该函数可能与在给定状态下采取不同操作相对有利的函数不同。 你需要了解每个行动相对于其他行动的优势，从而确定你的政策应该采取的行动。 因此，观察游戏得分等因素，显然与价值息息相关。 你可能需要其他类型的函数来帮助决定当前在游戏中执行哪些操作。 因此，优势函数应运而生，这是由贝尔德很久以前提出的概念。 贝尔德运用反例论证了为什么价值函数的近似可能会效果不佳。 贝尔德之前的研究也提到了这一点。

好的，让我来解释一下。当你考虑你的 Q 函数时，它实际上表示了从一个状态出发并采取特定行动的策略所带来的值与在该状态下采取该行动的值之间的差异。所以，这可以被隐式地表示为 Q pi，也就是在策略 pi 下的状态 S 的值。那么，采取特定行动与遵循当前状态下的策略之间的区别被称为优势。这个优势是指这个行动相对于这个状态的好处是什么？因此，在 Deep Q-Network（DQN）的 Dueling 中，他们不是使用一个仅用于预测 Q 函数的网络，而是使用一种架构，将值函数和优势函数进行分离预测，然后再将它们结合起来，这样您可能会在这个部分得到不同类型的信息，而在另一部分得到另外一些信息。因此，需要一些解耦来确保您捕捉到的特征与您想在 Q 函数中看到的重要内容相关。现在，我上次简要提到的一件事是，

优势函数是否可分解？在这种情况下我的意思是什么？我的意思是，如果我们有一个最终要使用的 Q 函数，我们是否可以将其分解为唯一的 π 函数和 v 函数？因此，最终我们希望得到一个确定的结果。问题是，当我们在架构中将其分解为价值和优势时，是否存在一种唯一的方法来实现这一点？存在吗？嗯，并没有。所以，如果给Q和V添加一个常数，嗯，那么您将得到相同的优势函数。因此，并不存在一种唯一的方法，您总是可以改变您的，将您的奖励值乘以一个常数，这将不会改变您的策略，但会改变您的价值函数。嗯，所以，有许多不同的方法来分解优势函数和价值函数，这并不是唯一的分解方式。因此，他们定义它的方式是说，如果A是采取的行动，那么让我们强制状态和行动的优势为零。

因此，他们将其与贪婪方法进行比较。实际上，这只是其中一种方法 - 从某种程度上来看，可以将其看作是监督学习的类比。他们希望有一个稳定的目标，并且如果有大量关于它们的数据，希望能够学习这些优势函数和价值函数。这类似于选择一个特定的固定点来定义优势函数。然后，根据经验，也可以使用平均值。因此，可以对优势函数进行平均，这更像是一种启发式方法。他们的研究结果再次表明，我们正在逐步引入这些额外的技术。从DQN开始，然后考虑添加双Q学习，接着再考虑添加优先重播。然后是对决斗方法的研究。发现在大多数情况下，与优先重播的双Q学习相比，对决斗方法的效果要好得多。现在，让我看看能否找到蒙特祖玛的结论。是的，对于蒙特祖玛来说，这种新方法基本上效果也不怎么样。

正如这些方法并未真正解决探索困难的问题一样，它们正在以更好的方式在神经网络中传播信息，并尝试改变我们训练网络的方式。 是的，如果您对此有疑问，请先告诉我您的姓名。您能不能说大声一点？我听得不太清楚。好的，我会尝试提高音量。我说话的时候其他人能听到吗，还是只有他一个人听到？好的，明白了。总之，这三种方法最终产生了很大的影响。我们简要讨论了一些实用技巧。嗯，我不会过多地介入这些内容。最重要的是，在尝试玩 Atari 游戏之前，我们鼓励您先建立自己的技能表现。您可以尝试不同形式的损失函数。学习率很重要，但在这种情况下，我们会使用 Adam 优化器，这意味着您不必过于担心它。在尝试不同的探索策略时会遇到一些问题，我们稍后会在本课程中讨论。因此，目前我们仍在考虑简单的 E-greedy 方法。一篇不错的论文已经发表了，

我认为在 2018 年初，Rainbow 这篇论文旨在结合当前许多最新的方法，以探究它们对改进的影响。他们回顾了大量数据和经验，达到了 2 亿帧的经验数据。通过引入一个被称为 Rainbow 的算法，结合了双 DQN、优先级、决斗等多种最新进展，进行了一次新的尝试和探索。研究发现，将这些改进方法结合起来，可以获得显著的性能提升。这一观点非常有价值，因为通常很难确定这些方法的收益是如何相互叠加的，或者它们只是在做相似但略有不同的尝试。因此，这种不同类型的思路在最终性能上的综合效果是增加的。这也表明在某些情况下合并不同的想法可能提供更好的性能。这些研究涉及了大量的数据和经验。

好的，让我重新整理一下。

现在我们正在使用无模型的强化学习（RL）深度神经网络。这些网络是非常强大的函数逼近器。你应该能够理解如何表示Q函数，并且可以使用基于蒙特卡罗或TD风格的方法。此外，在某些情况下，可能需要执行函数逼近，而在其他情况下则不需要。你应该了解如何使用表格方法、线性值函数方法和深度神经网络。

算法上看，这些方法看起来很相似，但在某些情况下可能需要函数逼近，而在其他情况下则不需要。最好能够列举一些超越DQN的拓展方法以及它们的原理。

在我们对强化学习算法的追求中，我们希望这些算法能够优化、泛化、探索并在所有统计和计算方面都有效。我们花了很多时间研究泛化和优化，但效率方面我们尚未深入探讨过。

因此，其中一个挑战是，如果您希望正式定义效率（例如引导智能体所需的数据量），那么就需要学会做出正确的决策。已经有一些困难的结果是已知的，我们的实验室以及其他团队都在研究这些下限。对于表格MDP情况，我们基本上已经确定了严格的上限和下限，这表明存在一些非常棘手的MDP环境，我们可能需要大量数据来解决。但是，当涉及到非常大的领域时，情况就变得很棘手，因为简单的方法不再适用。以前，只要进行大量的探索就可以了。您可以尝试比ε贪婪更好的策略，但很快我们就会讨论这一点。即使我们采用比ε贪婪更好的策略，我们仍然可以证明在某些情况下学习仍然很困难，可能需要更多的数据。因此，另一个选择是利用其他监督信号来加速强化学习。那么，我们如何使用这些额外信息来提高强化学习的效率呢？今天我们要讨论的就是有关模仿学习的一些内容。下周我们将继续探讨这个话题。

政策搜索和政策梯度方法是另一种强化学习结构的不同方式。在政策梯度方法中，必须定义一个策略类，有时可能是一个复杂的类。这可能看起来像限制，但有时您可以通过编码领域知识来设计类。特别是在涉及模仿学习和大型状态空间时，您可能希望从额外的帮助或监督中受益。比如，像《蒙特祖玛的复仇》这样的游戏，可以研究在这种情况下深度 Q 网络（DQN）的表现。《蒙特祖玛的复仇》是一个长篇探险游戏，玩家需要穿越世界，做出决策并探索各种房间。在游戏中，房间用白色方块表示。

在左侧我们看到一个经过训练5000万帧的DQN，只能通过前两个房间，表现很糟糕，没有取得太大进展。而在右侧，我们看到一些明确尝试探索的迹象，使用了一些后面会详细讨论的技术。但需要注意的是，它仍然没有完全完成游戏。这表明有些游戏确实很困难。自从我们的实验室开始以来，我们基本上已解决了蒙特祖玛问题，并取得了额外的进展。Uber人工智能实验室在解决蒙特祖玛问题上做出了很好的工作。起初，人们对此感兴趣是因为开始使用模仿学习和演示。特别是当考虑到强化学习可能起作用的情况时，通常在任务相对简单或我们已经看到很多成功案例时。到目前为止，数据很容易获取和并行化也很容易时，强化学习才会表现出色。

在数据昂贵、容不得失败的情况下，采用我们之前讨论过的方法可能会变得更加困难。比如，如果尝试用我们刚才描述的方法来学习如何操纵遥控直升机，由于直升机成本很高，会导致困难。在很多情况下，这种方法的实际性会受到限制。 
一个好处是，如果你能够给予AI智能体丰厚的奖励，你就可以快速塑造其行为。比如《蒙特祖玛的复仇》中的挑战之一是奖励非常稀少，智能体必须尝试各种不同的行为，才能得到反馈表明其是否正确。这些奖励从何而来？这实际上是一个非常深刻的问题。
现在，让我们来看看确定奖励的挑战。如果要手动设计奖励，根据不同的任务，这可能会很脆弱。另一个选择是演示。如果必须编写驾驶汽车的奖励函数，那将变得相当复杂，就像你不希望在...

在人类生活中，驾驶车辆与行人之间的道路遇到相似的难题。任何开车者都希望安全到达目的地，而不是离开道路。这就需要一个复杂的奖励函数来指导行为。大多数人很容易通过示范学会驾驶并展示最佳实践。这正是演示学习的核心概念。近年来，在演示和模仿学习方面已经做了大量工作。最早开始关注从演示学习奖励的论文大约可以追溯到20年前，大约在1999年或2000年左右。自那时以来，已经有许多应用程序涌现，如高速公路驾驶、导航和停车等。尤其在驾驶方面，有很多成功案例，同时也在机器人技术领域得到了广泛应用，例如教导机器人拾起杯子等任务。此外，还存在一些有趣的问题，如路径规划和目标推理等。

在这些情况下，直接定义奖励函数会变得非常复杂，甚至容易出错。奖励函数的脆弱性问题在于，你的代理会努力优化奖励函数，但这可能导致不符合你期望的行为。因此，从演示中学习的设置，或者说，从逆强化学习和模仿学习中学习的概念非常重要。尽管它们之间存在一些差异，但在某种程度上它们是可以互换的。这些方法的核心思想是利用提前准备好的演示数据来引导或直接学习新的策略。你可以聘请一位专家，无论是一位完美的专家还是一位很好的专家，来提供一些示范轨迹，告诉代理在各种情况下该采取何种行动。通常情况下，这种方法更容易实现，考虑什么时候更容易指定这些示范轨迹之一，是非常重要的。

对于每种情况的常见问题是非常有用的。 那么在设置中会存在哪些问题呢？问题设置指的是我们拥有的状态空间和动作空间，以及一些通常未知的过渡模型和缺乏奖励函数，而是一系列教师演示，这些演示来自于我们当前假设的最佳策略。在行为克隆中，我们如何直接学习教师的策略？我们如何匹配和直接从这些演示中获得接近最佳策略 π∗ 的近似值？逆强化学习通常在探讨如何恢复奖励函数。一旦获得了奖励函数，就可以用它来计算策略。最后一步通常是与学徒学习相结合。因此，我们的目标通常是获得奖励函数，然后基于此生成一个良好的策略。在某些情况下，您可能只对奖励函数感兴趣，这种情况下您可能不想去恢复策略。

但您只是好奇的是另一个代理的奖励函数。如果您想了解，请说，环境如何。我认为这是一个很好的例子。在许多科学领域，例如生物学，研究者经常希望了解有机体或动物等的行为。通过观察它们的行为，比如追踪猴子或类似动物，可以帮助我们了解解决问题的奖励函数是什么，目标或偏好是什么。在许多情况下，这是有用的，即使在将来可能会进行一些优化处理，但通常这只是为了理解目标结构或有机体个体的偏好结构。这种情况也可能发生在人类身上，比如人们在导航、通勤或购买偏好等方面的选择。或许稍后可能会进行优化，但您只是好奇。

人们的行为可以揭示潜在的奖励结构和偏好模型。模仿学习仅仅是使用老师的示范作为上限，也许代理学习可以表现得比老师更好，就像我们发现了新的可能性一样。有一个很好的问题，即专家行为是否存在一个上限，或者是否存在代理可以超越这个上限的情况。虽然今天我们不会深入讨论这个问题，但当前在将模仿学习与强化学习相结合方面有很多工作待完成。因此，有很多工作要做，例如逆强化学习与强化学习的结合。比如，您可以使用逆强化学习引导系统，然后您的代理将在此基础上继续学习。Pieter Abbeel的团队也做了一些出色的工作，他们假设专家提供了最佳路径的噪声演示，然后目标是学习最佳路径而不是噪声演示。因此，通常您确实希望超越专家，尽管这也有其局限性。

我们稍后会探讨这个问题。在无法继续在新环境中收集数据时，你可能会遇到一些限制。好的，让我们从行为克隆开始，这可能是最简单的一个，因为本质上在行为克隆中我们只是将其视为一个标准的监督学习问题。因此，我们会建立一个策略类，用某种方式来表示，我们从状态到行动的映射。这个映射可以是一个深度神经网络，也可以是决策树，或者其他许多形式。然后，我们将根据训练样本来估计策略。因此，我们只需学习状态到动作的映射。这种方法早就存在了，已经有30年左右的历史了，有很多很好的例子，比如ALVINN，这是一个关于自动驾驶系统的早期神经网络论文。

使用行为克隆或监督学习来模仿轨迹时可能会出现问题。首先，让我们探讨监督学习中的情况。在监督学习中，我们假设状态和动作是独立同分布的，忽略了时间结构。在这种情况下，我们尝试学习一个分类器，用于根据状态选择合适的动作。然而，这种方法可能存在错误，错误的概率为 epsilon。如果在T个时间步长内执行这个操作，预期的错误总数可能会是 epsilon 乘以 T。因此，当我们尝试在监督学习或强化学习环境中这样做时，就会出现问题，因为我们的决策会影响下一个状态。

现在让我们来和您的邻居讨论一分钟。比方说，您认为在这种情况下行为克隆可能会有哪些问题？如果这是一件简单的事情，也许您可以考虑一下如何去解决。当我们尝试将标准的监督学习应用于这样一个真正基础的情形（MDP）时出现问题，您可能会怎么办呢？好的，现在我们来猜想一下。

我想问的是，如果我们假设底层世界是一个马尔科夫决策过程（MDP），在这种情况下，你认为总的期望误差会大于还是小于我们预测的误差数呢？根据监督学习方法的预期来看。那么，谁认为我们会有更大的总预期误差呢？谁认为会更少呢？我知道这可能让很多人感到困惑。如果有人认为答案是我们会有更多好处呢？也许认为情况确实如此的人可能会说，为什么他们认为如果现实世界是一个MDP，而我们尝试使用监督学习技术，我们可能会出现更多错误呢。请先给出名字。我的理解是，当我们在规划更长期的时间范围或者考虑未来行动的连贯性时，我们会更谨慎，但是一旦我们采取了某种状态和行动，我们的预测就会变得更不准确，因为我们无法长期规划。这可能会导致我们在进展过程中出现更多错误。

这是正确的。 因此，当提到这一点时，正确的是，我们会将这些错误叠加在一起，其中一个具有挑战性的方面是，错误可能会相互叠加。 这是因为你在做出决策时所处的状态分布取决于你所采取的行动。 因此，如果你将这个情况想象成为导航的例子，比如，如果我应该从右边的门出去，当我看着从这扇门出去时，我看到他向右边走。 我本应该像我所学的那样，尝试通过一个监督学习分类器来决定我应该做什么。 但是我的监督学习器有点出了问题，所以我没有向右走，反而实际上向了左边走了。 这样，我就来到了房间的一个部分，因为他本应该朝门口走去，而我却没有这样做。 所以，现在我在这里不知所措。 就像，现在我处于一个状态分布中，这是我以前从未遇到过的情况，我很可能会犯错。 事实上，我的错误概率可能并不是我在这里的错误概率低估了这样一个事实：你未来获得的数据与你过去获得的数据具有相同的分布。 当我们有监督学习的保证时，通常是安全的。 如果你的数据来源于独立同分布分布，那么- 如果.

未来这将是你测试错误的原因。在强化学习或决策过程中，你的行为将取决于你将要观察到的数据。因此，我没有按照正确的行动方式进行，在这里而不是在那里。由于我的数据不同且数据分布不同，所以现在我的监督学习算法无法提供保证，因为它没有接触过类似的训练数据，我们不能总是假设。这就是问题的所在。2011年，卡内基梅隆大学的Drew Bagnell小组注意到了这一点，他们认为这对于所谓的行为复制是一个巨大的困扰。尽管有很多经验证明了这一点，他和他的前学生Stephen Ross指出了为什么这可能会是一个根本性的问题，以及一些人们通过经验会遇到的问题。核心思想是，一旦你偏离了正确轨道，那么你会开始犯错，整个轨迹的其余部分很可能也会是错误的。你可能会犯更多错误。这意味着你犯错的总数不再是预期的ε×T，而是ε×T的平方，情况将会更糟。

这实际上是由于这些复合错误导致你的状态分布与你掌握的数据非常不同。 这个问题将再次出现，再次出现，这种对等的思考，即你所遵循的政策与你想要遵循的政策所得到的状态分布是什么。 这个问题在强化学习中再次出现。 所以，嗯，这确实只是一个基本问题，你知道，根据你所学到的策略与真实的策略相比，你将得到的数据分布是什么，并考虑这种不匹配。 所以一旦你离开赛马场，你就不会再有任何相关数据。 所以，呃，德鲁·巴格内尔和他的学生提出的思考这个问题的想法之一是，好吧，如果我们可以获得更多数据怎么办？ 那么，如果我，你知道，我-我-我只有少量数据可供开始，该怎么办？ 我已经学会了我的监督学习政策，你知道，我在每个州应该做什么，有时我会犯错误。 所以，有时，你知道，我-我那样出去，我的赛车驶离赛马场。

当我面临不确定状态时，我该怎么办呢？如果我能够询问专家，问问：“我应该做什么？”这样的话，当我陷入没有任何数据指导的情况时，我就可以得到指引。就好比走在路上，突然不知道下一步该怎么走，这时你可以向专家请教，他们会告诉你：“向右转。”没问题，你就可以从右侧出去了，解决问题。因此，如果你能向专家获取标签，那么你将得到关于你当前状态的标签信息。只要你的数据集包含了可能遇到的所有状态，你的监督学习就能够很好地进行。但更常见的问题是，你遇到的情况可能超出了你的训练数据涵盖的范围。这就是DAGGER的理念，通过不断聚合数据集来不断增加数据。所以，处于这种情况下，一开始你没有任何数据，你需要初始化你的策略。按照你的策略行事。因此，在这种情况下，我们假设你可以依靠专家，一些专家的策略。因此，你知道，专家可能会采取某些步骤，而你也可以采取其他策略，你可以预测一条路径。

因此，有时您会遵循当前的一项政策判决。然后，您会进入这种状态。在这种情况下，您可以向专家请教，以确定在每个州应该采取什么行动。您会记录在遇到的每个状态上采取的行动，并将这些信息添加到数据集中，然后使用监督学习来训练策略。因此，数据集中所有用于训练策略的内容都带有专家的标签，并且您会逐渐增加数据集的规模。这个想法是随着专家数据不断增加，你可以获得更多专家行为标签，这些标签会跨越您实际观察到的轨迹。这种方法有很好的属性，可以确保您会根据在诱导状态分布下遵循的办法最终收敛于一个良好的策略。是的，只是为了确认一下，这种方法假设我们在整个空间中拥有一个最优策略，就像在没有专家可供请教时的情况一样。这是一个很好的问题。

刚才看到这个内容，我觉得需要仔细审查一下。我理解这是指假设专家可以为你提供行动建议，而不需要直接访问 Pi星。因为如果你可以直接访问 Pi星，那就不需要学习了。所以，在这种情况下，就好像抛硬币决定专家是否直接告诉你采取什么行动，或者你要遵循其他策略。因为无论如何，你需要专家在旁边，最终告诉你他们会采取什么行动。对不起，这就是你的数据集的情况。所以，这就是情况。我很想了解这是如何高效进行的？我可以想象，在某些情况下，手动设置命令行会导致 GPU 的训练和输入效率低下。人们通常是如何实现自动化而无需手动干预的呢？是的，这是一个很好的问题，你需要专家在每一步或者轨迹结束时周围，可以返回并提供标记。

这个过程非常昂贵。如果你要这样做，你知道，数以百万计的时间步长是非常具有挑战性的。我认为这就是为什么这一系列研究在某些方面的影响力不如我们接下来将要看到的关于如何进行逆强化学习的其他一些技术。所以，真正的假设是，你需要一个在循环中真正参与其中的人，他真的在循环中，而不只是要求他们提供一次演示，然后你的算法就会失败。实际上，在大多数情况下，更现实的做法是，你开着车在街区绕圈10次，然后你可以离开，我们将进行所有的强化学习，而不是要求你一直在场。或者，比如，我们可以标记汽车正在行驶的所有轨迹，并不断告诉你是对还是错。我认为这只是标签密集型的。这会非常昂贵。所以在某些有限的情况下，比如，如果你的行动速度非常慢，比如，在军队做出决策，或者其他人处于非常高的水平，对于非常稀疏的决策，这可能是非常合理的。因为基本上你可以在每次决策之间进行无限的计算。

如果你这样做是为了做出类似于实时赫兹级别的决策，我认为这会非常困难。嗯，是的。这是否可以与专家接管系统兼容呢？是的，就像有人坐在方向盘后面让特工开车，然后，嗯，就像意识到出现紧急情况并接管方向盘一样。所以，这是否与专家接管兼容呢？是的。我的意思是我认为，这可能是获得标签的更简单的方法。因此，您可能会认为，如果有专家在场，那么所采取的每个操作都会与专家采取的操作相同。也许他们不会进行干预。否则，他们只会在出现不同情况时提供标签或干预措施。但它仍然需要类似于专家一样进行监控，这本质上仍然是一个心理挑战。你知道，这仍然具有很高的成本。所以，对的。是的。我的意思是，看到行为克隆为何可能不是一种好方法以及原因的正式描述，我感到很高兴。嗯，我认为在某些情况下，DAGGER 非常有用，但在很多情况下，实际上...

为了更容易理解，请允许我重新整理这段内容：

假设我们没有人类介入的情况下，逆强化学习更倾向于第二类。在逆强化学习中，会发生什么呢？首先考虑基于特征的奖励函数。在这种情况下，我们可能会遇到一些无法观察到的转移模型。很多技术并非要求对转移模型有深入了解。在许多现实领域，这种方式非常强大，尽管在某些情况下可能并不是最合理的。假设我们只是不了解奖励函数。当您对转移模型也一无所知时，可进行一些扩展。通过一系列演示，现在的目标不再是直接学习策略，而是推断奖励函数。那么，如果没有任何关于最优策略的信息，只凭演示，能推断出有关奖励函数的什么信息呢？就好像我们仅仅根据演示而非专家意见进行推断。

在演示中展示了关于动作状态等的一些信息。即使在不了解最优解的情况下，你能推断出关于R的一些信息吗？就好像，我的意思是，R会与R星相似吗？随着演示的进行，我猜想R会接近R星。这是在没有最优解方面的假设下所做的猜测。那么，如果我没有提供有关观察到的策略是否最优的任何信息，你能从中收集到任何信息吗？我可以说，教师们根据他们的策略所作出的选择仅仅是在替代奖励函数下打包的热空气。所以，对于特定的人，你可以根据他们的奖励函数做出一些推断，假设他们是一个理性的智能体，也就是说，他们追求自身利益。这是正确的。但是，如果你希望这些信息针对通用奖励函数 ，这意味着可能我低估了它，所以它看起来...

这段描述的意思更为微妙。它并不会主动告诉你任何事情，对吧？就好像当你看到我四处游荡时，就好像你在看一个AI智能体四处乱跑一样，你并不知道它是否正确地理解了真正的奖励函数。演示并不会提供任何信息，就像它们也不会揭示奖励函数的内容一样，除非你已经掌握了一些相关信息。如果我们不做出任何假设，并且不假设智能体在全局奖励函数方面已经做出最佳选择，那么你将无法获得任何线索。现在，更具挑战性的是下一个步骤。因此，我们先假设教师的策略相对于真实的全局奖励函数是最佳的，那么智能体可能会寻求在将来进行优化。这样一来，你将会看到专业司机的表现——司机可能会车来车往。嗯，想一想在这种情况下，你可以从中推测出什么样的奖励函数。

当涉及解释AI智能体行为时，可能存在不止一种奖励函数可解释其行为。因此，需要考虑奖励函数是否独一无二。想象数据方面可能并没有问题。如果我给你提供了10万亿个代理智能体遵循最优策略的例子，你或许会想看看是否能够了解奖励函数的性质。问题是，是否存在一个与这些数据一致的奖励函数，或者可能存在多个呢？或许可以花些时间与周围的人讨论，以查明究竟是否存在一个真正适用的奖励函数？若是你有无限数据，那么这将不再是数据稀疏的情况，可能比金钱更加珍贵。好的，我将进行一个快速的民意调查。

好的，让我来梳理一下这段内容。首先，提到了对奖励函数数量的调查，询问是否认为只有一种奖励函数还是存在多种奖励函数。然后引出了关于始终与任何最优策略一致的奖励函数的讨论。接着提到了将旧功能作为策略进行划分，然后随机化的想法，也讨论了在不同状态下可能有不同的奖励处理方式。最后提及了关于选择一个常数来指定奖励函数以使任何策略都变得最优的概念。

以上就是整理后的内容，希望对您有所帮助。

如果一个行动只能得到零奖励，那么根据所说，任何政策都是最优的，因为永远不会得到奖励。尽管如此，这种情况下是相当沮丧的。在这种情况下，所有的策略都是最优的。因此，如果只观察轨迹，那么在零奖励情况下，某个奖励函数的策略是最优的，但并非唯一可能。这个问题似乎是在2000年由Andrew Ng和Stuart Russell提出的，当时他们讨论了逆强化学习的论文。问题在于，如果没有进一步的假设，这并非唯一，有许多奖励函数都是一致的。因此，我们必须思考如何打破这种联系以及如何引入额外结构。比方说，如果你有一个一致的奖励函数，如果你再加上一个常数，那么最终的奖励会是什么呢？

他们感到沮丧，这让我想起了你。有很多奖励功能可供选择。如果所有状态都相同，那么任何常数也都是相同的。因此，通常会提供许多不同的奖励函数。有许多不同的奖励函数可以使任何策略都变得最佳。换句话说，如果您尝试根据某些数据推断出什么函数，您可能会得出许多奖励函数，其中数据相对于这些奖励函数是最佳的。第二部分是关键的，因为我们试图推断出什么样的奖励函数会使这些数据看起来像来自最佳策略，假设专家是最优的。因此，让我们思考一下如何在更广泛的状态空间中做到这一点。因此，我们需要考虑线性值函数逼近器，尤其是在我们特别需要样本效率的情况下。

当我们面对一个巨大状态空间且无法有效探索时，考虑使用线性值函数逼近器。在这种情况下，我们认为奖励与特征化表示状态空间的权重线性相关。我们的目标是根据我们的演示数据计算出一组良好的权重。通常情况下这并不唯一，但我们将尝试采用不同的方法来实现这一点。因此，策略  Pi 的价值函数可以表示为奖励的折现总和的期望值，即在该策略下各状态的预期到达情况。根据政策你所达到的状态的分布，这就是它们的说法。我们现在需要重新表达这一点，假设奖励函数是线性表示的。因此，我们可以这样写，也可以重新表达。

因此，我们可以通过将在每个时间步获得的状态特征与权重相乘来表示它。 由于权重向量对所有内容都是恒定的，因此可以将其移除。 这样，你基本上只需要考虑你遇到的状态特征的折扣总和。 我们将其称为Mu。 我们之前简要讨论过这个问题，但是现在我们谈论Mu是在我们的策略下对状态特征的折扣加权频率。 你在不同功能上花费了多少时间，在不同状态上花费了多少时间，这在到达这些功能时会打折，因为有些状态可能比当前更远。 这与我们之前讨论的固定分布有关，但现在我们使用折扣。 为什么会有这种好处呢？ 现在我们要讨论的是，我们可以开始考虑状态分布，而不是直接考虑奖励函数。

嗯，思考不同状态分布的概率，不同状态分布本质上代表不同的政策。对于特定的奖励函数和不同的政策，就会达到不同的状态分布。因此，我们可以考虑使用这个公式来进行学徒学习。在这种情况下，我们拥有一个很好的学徒学习环境。目前，我们正在使用线性价值函数逼近，将其称为学徒学习，因为我们的学习方式类似于代理从演示者那里学习。所以，现在我们有了特征状态的折扣加权频率，始终进入状态的特征空间。然后，我们需要注意以下几点。因此，如果定义 Pi 星的价值函数是什么，那么它等于我们获得奖励的预期贴现总和。根据定义，这优于或至少与任何其他政策的价值一样好。

Pi 相同，要么与最优策略相同，要么不同。等于意味着相同，我们假设未知相同的奖励函数，只是在不同的状态分布下。如果你按照另一种策略行动，奖励会根据你所处的状态分布而异。因此，若我们认为专家是按照最优策略行动的，为了确定权重向量W，只需找到W乘以最优策略下的状态分布。请记住，这是我们唯一知道的，我们从示例中得出这一点。这必须比用相同权重向量乘以任何其他策略的状态分布更好。还有其他问题吗？因此，通过观察，最优策略的价值与在该策略下得出的状态分布乘以该权重向量直接相关。这个价值必须高于其他策略下的状态分布乘以相同权重向量的结果。

在考虑与使用相同权重向量的其他策略相关的状态分布时，可能会出现不同的情况。这种情况下，您可以将这种情况视为状态的固定分布，类似于在难民国家所设定的政策比例分布。特别是在服务方面，可以从 Mu 的概念角度来思考这个问题。这种情况的一个有效方法是将其视为状态的稳定分布。这种观点是合理的，因为本质上一切都是状态的稳定分布，并且在此基础上添加了折扣因子。因此，与以前看到的稳定分布非常相似。因此，我们希望找到一个奖励函数，使得在计算值函数时，专家策略与您的状态分布在相同权重向量下比其他分布下更佳。

所以，如果我们能够找到一种策略，使得我们的状态分布与专家的状态分布相匹配，那么我们就能表现出色。这意味着，基于该策略，我们访问状态的分布与您作为专家演示时的状态分布接近。因此，如果匹配得足够好，我们的价值函数误差也会很小。换句话说，通过匹配特征或状态分布，我们能够找到与真实价值接近的估计值。这意味着，对于任何参数w来说，无论真实奖励函数是什么，只要我们找到一种策略，使得我们的...

在状态特征匹配中，无论真正的奖励函数是什么，你都知道你将接近真正的价值。 是的，所以，当我们找到这个权重 w 时，它将被用来计算我们猜测的期望，也就是猜测的状态值。对吗？ 是的。一旦你有了这个权重 w，你可以将它与你的策略结合起来计算，比如一个状态的值，或者你可以对其求和。 我认为，如何直接将这个过程转化为在没有状态时做出决策？因此，我认为关键问题是，如果我们获得了这些权重 w，或者我们要解决什么问题？我们是在解决策略问题，还是其他问题？在这种情况下，我认为，一个合理的思考方式是，如果我们求解策略π，那么我们就在求解权重 w。所以，这就是说，假设您正在优化某个策略π。如果你找到了一个策略π，那么现在我们知道了转换模型，尽管它并不总是完全准确的，但如果你知道了转换模型，对于给定的策略π，你就可以计算出价值函数μ，因为它只是遵循着...

就好比，你可以使用蒙特卡罗方法来推断。举个例子，如果某人给了你一个圆周率 π 的值，然后告诉你一个转换模型，你可以据此推断出 π 的估计值。接着，假设有人对某个政策进行多次推断，估计出其平均值，并检查这个值是否接近其示范政策的平均值。如果这个值非常接近，那就意味着无论真正的奖励函数是什么，你们得到的价值都会与示范政策的价值匹配，就像你匹配的价值一样，这意味着你可以很好地采取这个政策。是的，我正在研究关于 w 的某种约束条件，但我不太清楚它的来源。我很好奇，因为我可能错过了这部分内容，或者我们还没有讨论过。问题是，为什么我们要对 w 加上这个约束条件。我需要仔细检查一下细节，但我相当确定它存在，所以当我们做这些近似计算时，这个备用方案就派上用场了。

哦，关于你提出的问题，错误是有限的，这样事情就不会出现问题。 当你证明这一点的时候，我认为你主要使用了霍尔德不等式，并利用了权重受限这一事实来确保最终结果是有界的。所以，你可以再确认一下这部分。一般来说，希望奖励函数是有界的，特别是在计数方面，这很重要。你需要确保贝尔曼算子像一个收缩映射一样收敛，我们已经讨论过这一点，如果通过近似线性值函数不一定会收敛，但如果奖励是无限的，情况会变得更糟。我正在试图将这个概念融入我之前了解的其他内容中，这基本上是一种考虑政策的最佳可能性的方式，对吧？比如，如果我们抛硬币100次，得到99次正面和1次反面，这可能正是因为……

在AI领域使用的一些术语对应：

硬币公平 -> 公平硬币
奖励函数 -> 奖励函数
状态特征 -> 状态特征
权重 -> 权重
策略 -> 策略
专家 -> 专家

嗯，考虑这样一个情况——如果一个专家代理在同一任务上进行了100次操作，这可能是由于奖励函数处处为零，这种情况不太常见对吧？这些问题提供了一些处理奖励可能为零的方法。虽然这并不能完全解决问题，但给我们一些线索。因此，通过匹配专家的政策，可以尝试学习一种新的方法。换句话说，当您的策略在访问相同状态时几乎与专家完全一样时，即表示您在模仿他们的行为。尽管无法确定您得到的权重是准确的还是对奖励函数的良好估计，但这可能并不重要，因为您真正关心的是您是否能够...

如果你按照专家的策略频繁访问所有的状态，那么你就会采用相同的策略。虽然我们仍然不清楚真正的奖励函数是什么，但已经找到了专家的策略。对于寻找更为有效的非线性模拟，深度神经网络也进行了大量工作。这种观察促使了一种用于学习策略的算法，通过尝试找到某种奖励函数，使得某个状态下的控制策略比其他一切都更好。因此，最终找到当前状态下最优的控制策略。

这个过程可以让您获得新的参数，因为您的Transformer模型在这里。然后重复这个过程，直到差异足够小。现在，这并不是完美的。如果您的专家策略并不是最优的，那么如何将它们结合在一起可能会有些棘手。我不想深入讨论这个特定的算法，因为它已经不再是大多数人在使用的东西。人们可能会转向更深层的学习方法，但我认为关键在于，如果您匹配状态特征，那么就足以说明这两种策略是相同的。实际上，更广泛。好的，请先提醒我您的名字...对于使用L1范数而不是其他范数（比如L2范数），有什么特殊含义吗？很好，这是一个很好的问题。问题在于，为什么我们在方程七中使用L1范数。这实际上是非常重要的。

嗯，虽然这可能不是唯一的选择，但你必须匹配所有状态。这就是正常人在这里所说的。因此，你可以将μπ视为s中的真实存在。我没有明确展示这种依赖关系，但这是s的一部分。因此，规范所说的是，在总结所有这些错误时，它必须是一个。所以，你实际上正在评估所有这些错误。你可以选择其他材料来调整分析。当你开始执行时，试图限制值函数中的误差在∞范数中可能特别容易推理。好的，所以我们仍然面临我们讨论的问题存在模糊性，有无限数量的不同奖励函数，我们讨论过的算法不能解决这个问题。在模仿学习、强化学习、逆向强化学习等方面已经做了很多工作。其中两篇重要论文如下，第一篇是最大熵逆强化学习。

这段文字描述了在选择奖励函数和策略时如何在不确定性中进行权衡，以保持对专家数据的尊重。作者提到，尽管不确定奖励函数和策略的具体形式，但可以尝试选择具有最大熵的分布，以避免过度拟合，并确保匹配预期的状态频率。同时，强调了需要仔细考虑预期的状态频率，并将获取的数据与专家数据进行比较。最后指出，这些方法可以应用到 Transformer 模型的节点。

如果你无法访问过渡模型，但在现实世界中可以访问它，那么可以尝试新的策略。观察你的状态分布以及如何与你的专家演示相匹配是关键。这就是通常要做的事情。因此，最大熵逆 RL 具有巨大的影响力。另外，来自Drew Bagnell小组的评论也很重要，他是DAGGER的提出者，该小组一直在思考并做出了很多对逆强化学习的重要贡献。此外，还将其扩展到更广泛的函数逼近器。Stefano Ermon来自斯坦福大学，他将其扩展到使用深度神经网络，并再次强调特征匹配的重要性。在这种情况下，这两种方法都与DAGGER工作相比较，或者假设您一开始就有一组固定的轨迹，然后您可以为未来做更多事情。特别是，一般的生成对抗性逆向模仿学习会有这些初始轨迹，并让代理继续收集更多的数据。

因此，他们可以收集更多的数据，然后计算状态频率。此外，他们还可以使用某种鉴别器来进行比较。其中一个挑战是，在非常高维的状态空间下，比如Mu的形式，写下状态分布是非常困难的。因此，在这种情况下，他们主要专注于类似MuJoCo风格的任务，比如机器人风格的任务，其中涉及许多不同的关节，但仍然很难表达出状态分布。在这种情况下，他们关注的是思考一些事情，比如鉴别器，它可以区分专家演示和代理生成的轨迹。如果能够区分它们之间的区别，那么就表明不匹配。换句话说，我们可以利用类似鉴别器这样的函数，试图了解在真实高维状态空间中具有相同状态分布是什么含义。这就是GAIL（Generative Adversarial Imitation Learning）。GAIL还有许多扩展。

之前我们提到，在某些情况下，学习奖励函数可能会带来实际好处。但对于有些情况下难以实现这一点的人来说，这似乎是一个关键问题，这个观点是否正确呢？是的。因此，之前提到，正如您所说，有时候我们可能真的需要奖励函数，但也许您告诉我们我们无法真正实现这一点。我认为在这里我们主要讨论的是频率论式的统计方法，当我们谈论统计方法时，通常很难明确奖励函数这个词的含义。人们经常做的一件事是，当他们想研究动物行为或类似事情时，他们有一个先验，可以用另一种方法来处理，即使用贝叶斯先验奖励函数，然后进行贝叶斯更新，根据所观察到的数据，尝试对可能的奖励函数进行后验推断。这样，可以避免类似于无法事先确定奖励为零的情况。因此，如果您有一个结构化的先验，这可能是一种仍然利用信息来减少对人类、代理人或动物奖励函数不确定性的方法。

对话内容涉及词函数的现实先验和探索方法。在探索过程中，有人使用先验而不是奖励函数，像汤普森采样这样的方法需要这样做。在追求接近频率论时，人们通常使用狄利克雷分布、多项式或高斯分布等作为先验。在实际领域中，使用这些先验的好处可能在于编码领域知识，以确定期望的奖励类型。

在这些情况下合理是很重要的。如果你只是想了解奖励函数和偏好函数，可能需要进行贝叶斯分析，或者找到其他方法来覆盖这些方面。很多时候我们关心的是高性能而非奖励函数。如果找到与专家策略匹配的策略，通常就能解决问题。行为克隆存在错误叠加的问题，但现在有更好的方法来处理这些情况，我们对这些方面感到满意。谢尔盖·拉明和其他人的工作结合在一起，可以采取探索性策略，从而得到比原有策略更好的结果。演示器的使用很好，例如来自YouTube之类的，可以免费提供演示，但质量不一定有保证。因此，通常可以用演示器来指导学习，但并不一定受制于它。总的来说，

在实践中，模仿学习已经在许多领域，特别是在机器人领域，取得了很大进展。模仿学习在离开课堂进入行业时非常有用，因为通常更易于获得示范，并且可以有效引导学习，例如在复杂的Atari游戏中。尽管存在许多挑战，特别是在我们不了解最佳策略的情况下，比如在医疗保健领域或智能辅导系统中。面临的一大挑战是，在缺乏最佳策略的情况下，可能会尝试做出超越现有数据的决策，这是一个巨大的挑战。那么，如何结合逆强化学习呢？也许可以以一种安全的方式进行在线强化学习。因此，模仿学习的一个动机是为了安全性，如果你当前的安全性做得不太好，那么就需要找出未来如何进行安全的探索。

没问题，下周再见！我们将开始讨论政策搜索。