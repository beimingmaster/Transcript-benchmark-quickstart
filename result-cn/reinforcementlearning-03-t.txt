今天我们要讨论的是无模型政策评估。在上次讨论中，我们开始正式定义马尔可夫过程、马尔可夫奖励过程和马尔可夫决策过程，研究这些不同形式的过程之间的关系，这是我们建模顺序决策的方法，尤其是在不确定性问题下。我们思考如果有人给了我们一个关于世界如何运作的模型，那怎么办？我们已了解奖励模型和动态模型，但确定正确的行动方式可能仍然困难。所以，我们如何采取行动，或者如何找到一种政策，可以最大化我们对未来奖励的预期收益？即使我们有了一个模型，仍然需要进行一些计算来尝试找到最优策略。因此，我们接下来要讨论的是当我们没有提前获得世界模型时，该如何进行这些工作。让我们首先回顾一下政策评估的一般问题。上次我们谈到政策评估是政策内部的一个步骤时，我们提到……

评估的内容迭代在政策评估和政策改进之间反复进行。政策评估旨在评估一项给定政策的表现优劣，即特定政策的预期累积回报折扣值是多少。今天我们将讨论动态规划、蒙特卡罗策略评估以及TD学习，并探讨比较这些算法的方法。简而言之，回想一下上次我们如何定义马尔可夫奖励过程中的回报：G_t表示从时间点t开始的奖励累积的贴现总和。因此，我们立即收到的奖励是R_t，之后以折扣因子Gamma乘以下一个奖励，Gamma通常取值在0到1之间。

因此，通常我们会考虑未来的回报而非眼前的奖励。状态值函数被定义为预期回报。通常，如果环境是随机的，预期回报将与具体的回报不同。例如，当您今天尝试开车去机场时可能会获得的奖励可能会与明天尝试同样行为时所获得的奖励不同，因为交通情况会有所不同，而且是随机的，随着时间的推移而变化。因此，可以比较在某一天，如果您需要两个小时到达机场，而平均而言，您可能只需要一个小时。我们还定义了状态动作价值函数，它表示在遵循特定策略π的情况下采取行动a后所获得的预期奖励。因此，假设您处于状态s，并采取了行动a，之后便遵守了由策略π给出的建议。那么预期奖励的折现率是多少呢？我们发现Q函数非常有用，因为可以将其用于策略改进等任务，这样可以帮助我们思考，好吧，

在追随某个具体策略之前，我们开始做了些微不同的事情。我们想知道如何才能实现这一点，并且这是否有助于提高我们获得的奖励。因此，经过讨论后，我们决定进行政策评估采用动态规划。通过了解环境如何运作，我们可以应用动态规划。在这种情况下，在本课程中我们将“动态”和“转换模型”这两个术语互换使用。所以，当给定模型p和奖励模型时，可以使用动态规划来评估策略的优劣。我们讨论的方法是初始化值函数，通常将其视为向量。考虑到存在一组有限的状态和动作，为特定策略初始化值函数为零，然后迭代直到收敛。我们说某状态的值完全等于我们。

在这种情况下，我们考虑的是遵循特定政策得到的即时奖励加上未来奖励的贴现总和，还考虑了从上一次迭代所获得的奖励。在收敛方面，通常会使用某种范数来比较值函数在一次迭代和下一次迭代之间的差异。因此，我们计算 V_Pi_k 减去 V_Pi_k_minus_one，并希望这个差异小于某个设定的 Epsilon。这个计算代表着在特定策略下状态 s 的 k 水平值的精确得分。换言之，在每次迭代中，我们可以确切地知道如果只执行有限数量的时间步（例如 k 步），我们将获得什么值。也就是说，在接下来的 k 个时间步中遵循特定策略会带来怎样的结果。

同样，你可以把这个理解为在你永远行动时，即将实现的价值的近似值。因此，如果 k 很大，比如200亿，那么这个值可能非常接近最优值。而如果 k 是1，这个估计可能非常糟糕。随着时间的推移，这个估计会逐渐收敛。因此，用图示的方式思考其中一些问题也是很有帮助的。因此，让我们考虑一下，当你处于状态 s 时，我用顶部的白色圆圈表示，然后你可以采取行动。因此，动态规划所做的就是在该位置计算 V_Pi 的估计值，即“即时奖励加上折扣因子Gamma乘以下一状态的V_k然后减去当前状态对Pi策略的期望”。s 是 s 的素数，那么我们如何用图示的方式来思考这个问题呢？我们从该状态开始，然后考虑我们可以到达的下一个状态。再假设我们处于一个随机过程中，

有时候，红灯亮，有时候红灯灭，我们的状态会根据情况不同而有所变化。比如，我们正在驾车前往机场，在到达某种状态后，可以考虑采取一些特定的行动，尤其是在我们设想问题正在解决的情况下。然后根据这些行动，我们就会进入其他可能的状态。因此，我们可以构建一个轨迹树，根据我们的策略开始行动，我们可能到达这个轨迹。每次做决定时，我们都会采取一种行动，因为我们在评估策略。每次出现自然选择时，我们会看到可能到达下一个状态的分布。你可以把这个想象为 S 素数和 S 双素数之间的时间分布。这就是你的 AI 智能体可能会面临的未来。我认为用图形化的方式思考会很有帮助，这样我们可以思考潜在的未来，以及如何利用它们来计算政策的价值和差异。在我们所进行的动态程序中，

通常情况下，当我们尝试计算策略的价值时，我们会考虑对下一个状态的预期价值。换句话说，如果我们按照这个策略行动，那么该值就是未来奖励的预期折现总和，而这个预期正好分布在将来可能出现的各种情况上。当我们观察到一个动作时，考虑我们可能达到的所有可能后继状态时，我们希望对这些状态下的特征以及我们可能获得的所有奖励抱有期望。这就是动态规划的基本思想，或者可以说它正在处理的问题。当我们深入思考动态规划的运作方式时，我们会发现它通过准确引导和计算单步期望来估计所有可能未来状态的价值期望。那么，动态规划有何作用呢？再次强调，它明确指出：“在给定策略下，对于状态 s，其值函数 V_Pi 完全取决于状态 s 的即时奖励 r、状态 s 的策略 Pi，以及对于下一个状态 s'，其概率为 Gamma 乘以从状态 s 执行动作 a 后得到的即时奖励加上未来状态 s' 的值函数 V_Pi(k-1)。” 因此，动态规划是一种自举方法，我们使用“引导”这个词来描述其中的原理，这是其中最关键的部分。

实际上，并非总结所有这些较低的潜在奖励。而是表示，“我不需要这样做。”之前，我计算过如果我开始说话会发生什么。因此，我现在已经知道该状态的价值是多少，我将引导并使用它作为实际进行所有可能的替代选择。因为我知道预期折扣是多少，或者我知道，嗯，对不起，模型是什么，它也可以直接预期超过s prime。所以，我的问题是，这里是否存在一个隐含的假设，即奖励在给定的状态下评估状态值函数不会随着时间的推移而改变。因此，我认为这个问题是在问，嗯，这里是否有一个明确的假设，即该值不会随时间变化？是的。本例的概念是，我们计算值的情况适用于无限视野情况，因此它是静态的。它不随时间步长改变。从这个角度来看，我们今天不会过多讨论有限视野的情况，在这种情况下情况会有所不同。

在这个情况下，在所有的时间步长中，您总是有无限多个时间步长要走。因此，价值函数本身是一个固定量。那么，为什么像我们这样进行引导是可行的呢？嗯，这是因为实际上我们有一个 V_k 减一的精确表示。将其代入，而不是对许多不同的历史进行显式求和，您就不会得到任何近似误差。请注意，未来的奖励存在很多不确定性。所以，在进行动态规划时，我们需要考虑的是，如果我们知道模型，然后了解动态模型和奖励模型，我们就能精确计算即时奖励。我特意多次强调这一点，是因为当我们开始研究其他方法（例如蒙特卡洛方法和 TD 方法）时，

他们将改变做法，尝试通过其他方式来计算这棵树。因此，为了计算保单的价值，我们实际上需要考虑所有可能的未来情况，以及在每种情况下我们可以获得的回报。我们正在努力简化这个过程，尤其是当我们不清楚世界如何运转，无法访问动态模型或奖励模型时。总而言之，动态规划的核心在于计算策略的价值，大致等于预期的下一个值减去对即时奖励的 pi 的期望，再加上 gamma 乘以你之前计算的值，需要一个模型来引导未来的回报，利用你在 V_k 减 1 时得到的估计。它基于马尔可夫假设，也就是说只关注当前状态，不考虑如何到达这个状态。

我的当前状态值都是相同的，我可以假设这一点，并且可以基于当前观察单独计算它。有没有任何疑问呢？我们主要是回顾上次内容，但稍微提到了一些之前没有提及的内容。因此，这些内容现在非常有用，因为我们要讨论没有模型的政策评估。我们现在要讨论的是蒙特卡洛政策评估，在我们不知道世界模型的情况下可以应用它，我们将讨论如何开始比较这些不同形式的估计器，也就是政策价值的估计器。在蒙特卡罗政策评估中，我们再次考虑回报。因此，回报和G_t是政策下未来奖励的贴现总和，我们可以表示政策的价值是公平的，让我们考虑在我们的政策下我们可以获得的所有可能轨迹，以及它们的平均返回是多少。

因此，让我们重新考虑我们刚刚建立的那棵树。每个类型的分支都有特定的奖励，然后取所有分支的平均值。这是一个很简单的想法，即价值等于预期回报。如果所有路径都是有限的，你可以采样大量路径并取平均值。因此，蒙特卡罗政策评估的好处在于不需要特定的sp-动态或奖励模型，只需要能够从环境中采样。因此，不需要了解特定参数模型如何运作。我们只需要从这里到机场不断驾驶，数以百计次，然后求出平均时间。如果我总是遵循相同的策略，比如一直走高速公路，那么经过100次实验，我就能很好地估计我在高速公路上驾驶时到达机场的时间。这就是我的策略。因此，这个方法不需要指导，只需采样即可。

尝试将根V_k保持为-1的做法行不通。该方法仅是将每个轨迹的所有奖励相加，然后对其取平均，并不需要状态满足马尔可夫性质。仅仅取平均并不足以计算未来回报，因为缺乏下一个状态的概念。这种方法只适用于情景MDP，需要一个可重复执行多次并在每次结束后到达终点的过程，就像开车去机场一样，可能路程很长，但最终会到达。如果一个机器人永远在执行任务，那么蒙特卡洛策略评估无法使用。

通常我们以增量方式执行此操作，即在每个情节结束后，我们更新当前估计为V_pi，以维持运行估计。

我们希望随着我们获得更多数据，这个估计会接近真实值。现在我们来看看这个算法。这里提到的算法称为首次访问蒙特卡罗策略评估算法。在开始时，我们假设尚未访问过某个状态N次。因此，初始时N是0。此外，从任何状态开始的回报或平均回报也为0。因此，我们现在初始化这些值，意味着我们认为我们从某个状态收获任何奖励的次数为0，我们也还没有访问过任何状态。

然后，我们进入循环。在每个循环中，我们会采样一段情节，即从起始状态开始执行，一直执行直到结束状态。举个例子，我从家里出发，一直开车到达机场，然后计算我的回报。所以，假设我花了两个小时到达机场，那么我的奖励就是两个小时。但在这个算法中，你会为每一步t内计算回报。

在这里，G_i,t 是从时间步 t 开始定义的，代表在接下来的时间内你将获得的剩余奖励。我们将在火星漫游者的示例中具体说明这一点。在每个情节中，对于你访问的每个状态，当你第一次遇到一个状态时，你会记录并更新该状态的总回报。然后，你可以通过计算这些估计值的平均值来得出对该状态价值的当前估计。有时你可能会在一个情节中多次处于同一状态。举个例子，假设你闯了红灯并怀疑时间步数，你可能每分钟查看一次自己的状态。在这种情况下，你在情节中的不同时间步会处于相同的状态，但你只会使用第一次看到该状态的时间步。然后，你将汇总直到情节结束你获得的奖励。所以，状态的记录是很重要的。

在不同的时间步长和同一集中，我们会多次获得相同的状态，因为它们之间存在差异性。如果在同一集中看到相同的状态，我们会在第一次访问时仅考虑第一次出现的情况，放弃其他所有的情况。当首次遇到某种情况时，我们会计算未来可能奖励，直到该集结束。如果在同一集中碰巧再次遇到相同的情况，我们将忽略这些数据。我们马上会讨论一种不同的方法。那么，我们如何评估这种方法是否可行，以及如何评估这种估计方法是否具有特定的优势呢？起初，由于数据有限，这可能是错误的。那么，我们该如何判断这一估计方法是否良好，以及如何比较我们今天要讨论的所有估计器和算法呢？请举手，因为我对此很感兴趣。在这里，我们正式看到了其他类别中偏差和方差的定义，尽管并非所有人都了解。因此，让我们快速回顾一下，考虑一个由参数θ参数化的统计模型，对给定θ的观测数据x的分布p进行建模。

因此，我们需要一个被称为θ帽的统计量，它是一个函数。因此，θ帽是基于观测数据的函数，提供了θ的估计值。在我们的例子中，我们将获得这个值，即我们正在计算的值的估计值。这是我们事件的函数，也是对按照此策略获得的真实贴现预期奖励的估计。因此，估计量偏差的定义是比较任何数据集的统计量的期望值和真实值。因此，这意味着，如果我根据尝试开车去机场3次来计算，你知道，我到达机场的预期时间。我刚刚展示的算法是公正的吗？平均而言，这也是我到达机场的真实预期时间。估计量方差的定义将我统计量与其期望值的平方进行比较。预计在真实参数下我可以获得的数据类型。

将均方误差与这两者结合起来。普遍情况下，我们主要关注的是我们的估计与实际值之间的差距有多大，这涉及到偏差和方差的综合。通常来说，不同的算法和估计器在偏差与方差之间需要做出权衡。当我们回到首次涉及蒙特卡罗算法时，我们使用的V_pi估计器是对我们策略的真实预期奖励折现总和的无偏估计。这只是一个简单的平均值，是公正的。根据大数定律，随着数据量的增加，它会趋近于真实值，这也被称为一致性。一致性意味着数据量无限增加时会趋近于真实值。所以，这是合理的，但效率可能不高。正如之前所讨论的，你可能会处于同一个状态，在同一个红绿灯前待了很多时间步。

是的，你每次访问蒙特卡洛时，只会使用剧集中的第一个状态来更新。简单来说，每次你访问剧集中的一个状态时，记录你从该状态一直到剧集结束所获得的累积奖励，然后计算这些奖励的平均值。换句话说，每次到达一个状态时，都会计算该状态到剧集结束的奖励总和，并对所有这些奖励求平均。这通常可以提高数据的效率。

关于偏差的定义，我感到有点困惑，我们如何会产生偏差，即使我们实际上并不知道 theta。我们如何计算偏差呢？是的，考虑到我们不知道 theta，那么问题是如何计算偏差呢？如果你能够精确地计算偏差，通常意味着你知道 theta 的值是多少，在这种情况下，为什么还需要估计器呢？一般来说，我们并不清楚偏差是多少，通常我们可以对其进行限制。因此，通常我们会使用诸如集中不等式之类的方法，它们更擅长限制方差。通常情况下，我们并不确切了解偏差是多少，除非你已经明确了基本事实。在实践中，我们可以通过不同的方法来估计偏差。因此，在比较不同形式的参数模型时，可以使用这些方法。

有时候，您需要考虑的是结构风险和结构风险最大化，以尝试比较估计器和模型的数量。我不打算在这里展开讨论这个问题，但我很乐意在办公时间进行深入探讨。因此，每次访问蒙特卡洛时，我们都会进行更新，以得到另一个估计量。需要注意的是，这通常会给我们带来更多的计数，因为每次看到一个状态，您都可以更新计数。然而，这会导致估计量存在偏差。可以证明这是关于 V_pi 的有偏估计量。您也许可以直觉地理解为什么会存在偏差。在第一种情况下，对于那些之前已经遇到过这种情况，或者不完全陌生于这种分析的人来说，第一次访问蒙特卡洛时，您是在获得一个状态的 IID 估计，即一个状态的回报，对吗？因为您认为，每一集都是类似的。

在大语言模型（LLM）中，从某一状态开始，并从那里进行估算，那么这种情况就是独立同分布（IID）的情况。当只在第一次看到该状态时才使用其返回值。如果在同一集中多次遇到相同的状态，那么它们的回报是相关的，而不是相互独立的。因此，数据不再具有IID性质。

这也解释了为什么在修改每次访问蒙特卡洛时，估计器可能会产生偏差，因为不再对IID变量进行平均，这是直觉的原因。这是否会对检查员悖论产生偏见？这一点不太清楚。这是一个很好的问题。我很高兴看到这个问题并做出回答。

然而，这种方法的好处在于它是一个一致的估计器。因此，随着获得更多数据，估计结果将趋近于真实值。根据经验，这种方法的方差通常要低得多。直观地说，方差应该更低。因为我们对更多数据点进行平均，通常是在同一数据点上。因此，如果只访问一个数据点，则 -

如果您不太可能重复访问相同的状态，那么在一个情节中，两个估计器通常会非常接近。因为您不会多次访问相同的状态。但在某些情况下，您会多次访问同一状态，从中获取更多数据，如果每次访问都使用这两个估计器，通常效果会更好，但会有一定的偏差。因此，需要在准确性和效率之间进行权衡。根据经验，通常使用这种方法要更好一些。在实践中，您可能更倾向于逐步进行，而不是频繁操作。可能会选择跟踪运行平均值，然后随着计数的逐步更新。在更新之前，您不需要等待最终的奖励，这是错误的。您必须等待最终奖励，因为只有在获取完整奖励后才能进行更新。所以，总的来说，如果您在每个状态下都有大致相同数量的估计，那么这两种方法大体上是等效的，但其中一种可能会有更小的偏差。例如，如果您这样做了，那么您是不太可能。

在翻译中应尽可能保持原文的信息完整和准确，同时让读者更易于理解。下面整理了内容，更符合中文表达习惯：

如果在每个状态下，与首次访问时的计数近似相同，并且有相同数量的剧集，您会认为剧集的方差会更小吗？我要讨论两种算法中某个状态的计数是否相同的情况。就剧集而言，除非只访问一个状态 - 如果您在一个剧集中仅访问一次状态，那么计数应该相同，它们将完全相同。但如果情况不是这样，如果您在一个剧集中多次访问某个状态，那么在达到相同计数时，单次访问的计数将更好，因为这是公正的，方差也会基本相同。对此还有任何疑问吗？好的。关于增量蒙特卡罗，政策评估基本上与以前相同，唯一的区别是您可以慢慢调整每个状态的运行平均值。重要的是，当逐步调整估计值时，

如果你把 alpha 设为 Ns 上的 1，那么每次访问蒙特卡洛都是相同的。本质上，你只是在精确计算平均值。但其实你没有必要这么做。所以，你可以调整它，使你的运行平均值更偏向于最近的数据。你可能想这样做的原因是因为你的真实领域是非平稳的。我们假设领域的哪个部分可能是不稳定的。这是一个高级主题。在这门课程中，我们大多数时间不会真正讨论非平稳领域，但实际上它们非常重要。嗯，我不知道你的机械部件是否坏了或者有什么问题。比方说，如果你处于制造过程中并且你的零件正在发生变化 - 随着时间推移会发生故障。因此，你的动力学模型实际上是随时间变化的。所以你不希望重复使用旧数据，因为实际上你的 MDP 已随时间变化。这通常是经验上的原因之一，就像人们训练推荐系统之类的事物一样，你知道，新闻等所有这些都是非平稳的。

因此，人们经常会对这些模型进行大量再训练，以应对这种不稳定的情况。我看到后面有一个问题吗？好的，是的。从经验来看，这通常对处理非稳定领域非常有帮助，但如果涉及非稳定领域，就会出现许多不同的问题。因此，我们现在基本上会忽略这一点。好的，让我们检查一下我们的理解。在蒙特卡罗方法中用于政策评估。让我们回到火星探测器的领域。在我们的火星探测器中，有七个状态。我们的探测车掉下来了，它会探索，奖励状态 S_1 为1，状态 S_7 为10，其他位置为零。我们的政策对所有状态都是采取行动 A_1。现在假设我们不知道动力学模型是什么，所以我们只能观察轨迹。如果您到达状态一或状态七，您采取的下一个操作将终止奖励。我不确定，也许是从悬崖上掉下来或者类似的情况。但每当您到达 S_7 或 S_1 时，您采取的下一个行动都会获得一定奖励。要么是1，要么是10，然后您的进程终止。因此，让我们想象一下这个政策下的轨迹是从 S_3 开始。

当你采取行动 A_1 时，你得到的奖励是零，这是为了奖励你。接着，你转移到状态 S_2，再次执行动作 A_1，但依然得到零奖励，使你继续停留在状态 S_2。连续采取行动 A_1，每次奖励都是零，直到你到达状态 S_1，采取行动 A_1 后得到奖励 1，结束这次体验。这就是火星探测器的一个场景。在这种情况下，我们只需要花一两分钟随意与邻居交谈，并计算首次访问蒙特卡罗估计和每次访问蒙特卡罗估计中每个状态的值，包括状态 S_2。之后，我会介绍首次访问的算法，取决于是否仅在事件中更新状态一次，或者可以多次更新状态。如果我们尚未确定所使用的值，您也可以提出这个问题。因此，如果您不确定，请将所有状态 S 的 V_Pi 初始化为零。

如果你需要更多时间，请举手；否则，我们会继续。 好的，让我们继续。 有谁想分享他们自己或身边的人对于第一次访问蒙特卡罗对每个州的V值估计的看法？我认为首先，这个估计是针对每个州的，除了最后一个州。哪些州？到目前为止我们只更新了其中一些。那你为什么不给我完整的向量呢？好吧，那我们就从这里开始。那么，S_1的V是多少呢？是1。好的。S_2的V呢？也是1。S_3的V呢？还是1。S_4的V呢？也是1。[噪音] 有人有不同看法吗？没有。没有。好的，接下来是S_5的V呢？是0。S_6的V呢？是0。S_7的V呢？[重叠声] 是的。因此，我们只更新了我们实际访问过的州。好的，所以，这里是1。

在每一步的访问中，蒙特卡洛估计仅为S_2。因此，我们只选择了S_2，因为这是我们访问两次的唯一状态。对于这个状态，它的估计值是多少呢？我们继续增加。是的，是否仍然为1呢？是的，为什么？因为当我们增加时，Ns和Gs也都是2。因此，两者都增加了两次。因此，从S_2开始到整个序列结束时，两次回报在这两种情况下都是1。所以，它等于1加1再取平均，仍然是1。是的，它们都是1的原因是因为伽马等于1，这样假设是正确的。如果伽马项存在，结果可能会有所不同。

我选择 gamma 等于1只是为了让数学更简单一些。否则，它将是γ因子。好的，太好了。所以，你知道，第二个问题有点转移注意力，因为在这种情况下它是完全相同的。但如果状态S2的回报与另一个状态不同，嗯，就像在某个状态遭到惩罚一样，那么它们可能会有不同的回报，然后我们就会在那里得到不同的结果。好的。所以，在这种情况下，蒙特卡洛更新——我们必须等到剧集结束，但是当我们更新到剧集结束时，我们更新了S3、S2和S1。那么，当我们考虑如何对可能的未来进行平均时，蒙特卡洛在做什么。那么，蒙特卡罗正在做什么，嗯，我在这里放置了这种增量版本，您可以将其用于非平稳情况，但您也可以以其他方式思考它。嗯，所以，请记住，如果您希望每次访问都等于此值，则只需在此处插入S的N分之1作为alpha。因此，这就是蒙特卡洛评估所做的，它只是对这些回报进行平均。所以，我们正在做的是，如果我们考虑一下我们的树正在做什么。

在我们的示例中，我们的树是有限的，每个分支都会最终结束。这是因为我们只能在达到回报后才能对回报进行评估。因此，在某个时间点，比如在火星示例中，当我们达到状态 S_1 或 S_7 时，过程就会结束。那么蒙特卡罗政策评估的作用是什么呢？它通过对树中一条路径的总结来近似计算所有可能未来的平均值。因此，它会对返回值进行采样，直到达到最终状态。它会把沿途所有奖励相加，就像是奖励、奖励、奖励。在这里，您可以得到每个状态操作对的奖励，从而总结了这种情况下的所有奖励。这种总结就是它的样本值。因此，请注意，它不考虑特定状态 S 和操作 A 下一个状态的概率，也不是自举，其计算方法是通过平均跨多条路径进行状态期望的评估。

只有当你一路走下去，看到完整的回报时，它才能更新。 所以，这就是样本。 它不使用动力学模型的显式表示，也不自举，因为这里没有负回报的概念。 它只是综合所有回报。 问题呢？ 斯科蒂提到，类似这样的政策评估在极少数情况下可能表现很差？嗯，这个问题很有趣。 问题是，可以公平地说，在极少数情况下这样做会出现很糟糕的情况吗？这很有趣。 这些是高方差估计器。 所以，如果你是蒙特卡洛，一般来说，你实际上只是在探索未来，对吧？你很可能需要尝试很多未来可能性，直到得到一个较好的期望值。 另一方面，像AlphaGo（解决围棋等棋类游戏的算法之一）这样的例子，它们使用蒙特卡罗。 所以，你知道，我认为，当你开始控制时，你需要小心一些推论。 当你开始时——因为那时你可以选择行动，嗯，你经常会在其中做一些权衡。

然而，即使遇到罕见事件也不必惊慌。但是，通常情况下，如果您有其他可用信息，那将会更有帮助。这取决于您的其他选择。因此，通常这是一个高方差的估计量。您可能需要大量数据，并且需要考虑情境设置，因为如果操作永不终止，就无法执行该操作。因此，您必须能够判断进程何时终止。在动态规划政策评估中，我们引入了一个伽玛因子，因为我们需要处理概率为1的状态之间的情况。但是，如果遇到一个永不终止的案例，因为这个序列永不结束，从技术上讲，我们需要另一个伽玛因子来评估政策。这就是政策评估。问题在于，即使在这些情况下，我们是否仍然需要伽玛因子，以及如何处理可能存在自环或小循环的情况。因此，通常情况下，这个γ通常可以使用伽玛因子。因此，当您计算这些值时，会包含伽玛因子。您是对的，如果已知进程将会终止，则...

当您的进程总是会终止时，就没有必要将折扣因子 gamma 设为小于1。如果存在可能不会终止的情况，比如在流程中有自我循环或者小循环，会导致无限循环而不终止，那么蒙特卡罗方法就无法使用，折扣因子也无法提供有效帮助。有时候为了模拟燃料成本或其他相互作用，将伽马参数设置为小于1是合理的。这是否有物理原因，比如燃料成本，是一个问题。通常情况下，我会将这种因素纳入奖励函数中。如果你想尽快达到目标，比如解决随机最短路径问题，可能会更倾向于设置该因子为最终状态以承担负成本，或者用其作为激励因素来鼓励实现目标。设置这一参数通常很微妙，因为如果设置不当，可能会带来难以预料的效果。

当超参数设置得太高时，你的AI代理可能会表现出奇怪的行为，实际上有些害怕采取行动，更倾向于留在安全区域。在某些情况下，如果奖励微不足道，你的代理可能会被误导。因此，在现实案例中，设置这些参数通常有些棘手。它们是高方差估计器，需要这些情境设置，并且没有引导。一般来说，它们会在一些通常温和的假设下逐渐接近真实值。如果有时间，我们将在课程结束时讨论重要的采样问题，否则可能会延后。在处理非保单数据（即从其他保单中收集的数据）时，我们会如何进行操作呢？

接下来，我们来谈谈时间差异学习。当看萨顿和巴托的著作，以及与里奇·萨顿等在该领域非常有影响力的人交谈时，他们可能认为这些核心对强化学习的贡献可能是其中最重要的。

不同于其他关于自适应控制的思考方式，时间差异学习的概念独树一帜。从本质上讲，它会结合蒙特卡罗估计和动态规划方法，同时又是无模型的。在这种方法中，我们不会根据 data， guidance programs和 samples 显式计算动态模型、奖励模型或估计器。所以，请记住，动态规划，就像我们迄今为止定义的那样，引导着我们的思考。实际上，你可以访问真实的动态模型和奖励模型，但却是通过 VK 减一引导的。蒙特卡洛估计器没有自我更新。它们一直走到轨迹的末端，然后总结奖励，但通过采样来近似期望。因此，引导法用于近似未来奖励的折扣总和。通常进行抽样是为了近似对状态的期望。时间差异学习的好处在于你可以在情景过程或连续过程中完成它。另一个好处是，你不必等待…

在大多数大语言模型训练中，有一个特定的时间点用于更新模型参数。当您观察到一个新情况并采取行动，然后转移到下一个状态并获得一些奖励时，您可以立即更新您的值。这种实时更新对于立即应用新知识非常有用。那么在时间差异学习中，我们要做什么呢？我们的目标仍然是估算策略π下状态值函数v的值。返回值的定义保持不变，我们要思考贝尔曼运算符。如果我们了解MDP模型，贝尔曼操作符告诉我们会获得的立即奖励以及未来奖励的折扣总和。在每次蒙特卡洛访问的增量中，我们使用一个回报样本来更新我们的估计值。因此，我们得到的新估计值等于旧估计值加上一个学习率alpha乘以刚刚观察到的回报减去现有值的估计。但这一更新通常要等到整个剧集结束后才能执行。时间差异学习的原理是什么呢？

为什么不使用旧的 V π 估计器表示当前状态呢？这样一来，就无需等待整个剧集结束。因此，您无需使用 GI，而是使用刚刚获得的奖励加上 γ 乘以下一个状态的值。通过这种方式进行引导。假设我不必等到整个剧集结束，而是在开始状态获得奖励后进入下一个状态。那下一个状态的价值是多少呢？我不知道。我会查找我的估算器，然后将其插入，将其看作对回报的估算。因此，最简单的 TD 学习算法就是将即时奖励加上折扣未来预期价值，然后将其代入实际达到的状态中。现在，请注意这是随机采样。通常我们不会得到那么丰厚的奖励。对于贝尔曼算子，通常我们会在给定状态 S 和动作 a 的情况下求出状态 S 的期望值。但在这里我们没有这种情况。

我们只有下一个状态作为信息。 我们把它作为我们的估计器输入。 因此，我们仍然需要对其进行抽样以近似预期价值。 就像动态规划一样，我们会进行引导，因为我们会利用先前对 v pi 的估计。 我们将其表示为sub a和sub k-1，以显示迭代的过程。是的，如果你想看的话，我可以在下面举例说明。不，在这种情况下我就不打算这样做。你也可以这样表述，问题在于我们只关心迭代中发生了什么。你也可以将其视为k+1时刻的p，这是k时刻的V，也就是说，你会随时间更新它。问题是，与动态规划相比，你需要对每个状态执行此操作，动态规划是对所有状态执行此操作 - 所以你会有一个一致的VK，然后进行更新。在这里，我们可以将其视为一个值函数，你只需根据刚刚达到的状态来更新该值函数的一个条目。因此，任何值函数的整个以前价值函数都不具有这种良好的概念。因此，我会让它留在这里。现在，人们经常讨论TD误差，即

时间差值误差，是比较你的估计值和实际情况之间的差异。因此，你的新估计可以通过以下方式计算：将即时奖励加上伽玛乘以实际状态的价值，然后减去当前对状态价值的估计值。需要注意的是，这个值应该近似于状态值函数的期望值，因为我们要对其进行平均。这关注的是时间差异。换句话说，即时奖励加上伽玛乘以下一个状态的价值，与你对当前状态价值的估计之间的差异有多大。需要注意的是，这个差异不一定为零，因为第一次操作始终只是一个样本，它代表了未来情况。只有在存在确定性的情况下，即仅有一个可能的下一个状态时，差异值才会被定义为零。因此，如果我试图开车去机场，遇到交通堵塞和没有遇到交通堵塞的时间分别占一半，那么就会有一些时间差异。

根据您当前的起始状态，您面临两种可能的下一个状态选择：要么遇到交通堵塞，要么不会影响交通。因此，您可能会达到达到流量的 v pi 或者未达到流量的 v pi。即使数据是无限的，由于一部分是基于当前状态的预期结果，另一部分则是实际达到的下一个状态，因此TD误差不一定会收敛为零。优势在于，您可以在状态操作奖励元组后立即更新这个值估计，而无需设置情境。对吧，斯科蒂？如果保持alpha恒定，这会影响收敛吗？是的，这是一个很好的问题。通常情况下，您需要对递减的alpha做出某些温和的假设，以确保这些估计能够收敛。嗯，还有问题吗？嗯，您能谈一下这个估计值的偏差吗？好的。问题是，您是否认为这是一个好问题，您对这个估计值的偏差有何看法？我们是否有先验了解，或者我们认为它可能存在偏差？

在回顾动态规划时，V_k减一。嗯，就像是在无限地平线上的无偏估计一样。举个例子，如果我们要追求无限地平线的价值，那么k就等于2。无论你如何更新它，感觉都不太好。通常来说，在执行这种引导时，结果会是一个有偏估计器，因为你是依赖于之前的估计值，而之前的估计往往是错误的。[笑声] 这会导致你的估计结果偏向某个特定方向。所以，这绝对是一个有偏估计。嗯，它可能还会有相当高的方差。[笑声] 因此，它可能具有高方差，也可能是有偏的。但另一方面，你可以非常快速地对其进行更新。你无需等待一集结束，就可以利用大量信息。因此，与蒙特卡罗估计相比，它通常具有更低的方差，因为你正在引导，这有助于平均化大量的变化。[听不清] 现在，问题是它是否是一个初始化函数。实际上不是。

这是一个评估器，用于估计不同属性的函数，可以以不同的方式初始化。引导是指使用一种通过自举法计算的 V_Pi 作为您真正期望的折扣回报总和的近似值，但如果这个估计值并非真实值，就会引入偏见。需要注意的是，在使用动态规划时，我们不会因为引导而产生偏见，因为在引导时，实际上是在使用 V_Pi，而这正是真正的值。所以问题在于，它是真实值的近似，这导致了偏见的产生。因此，如果您知道真实的动力学模型，那么引导就是有效的。在真实的奖励函数中，您需要精确计算 k 减一的 Pi，但在这种情况下不适用，因为我们引入了偏差。那么，零时差学习是如何工作的呢？我在这里讨论零时差，因为在 TD 学习和蒙特卡洛学习之间有一些有趣的区别。在这种情况下，您可以想象，折扣总和不是由立即奖励加上未来奖励来计算，

在这个设定中，并非单独获得每个奖励，而是将所有奖励叠加起来。在这种连续情境中，你可能会先拿到前两个奖励，然后再拿到 Bootstrap。因此，在一个连续模型里，有一个连续算法，处于只获取即时奖励和引导之间。但我们现在要讨论的是即时获得奖励然后自助取材。所以，Temporal Difference (TD) 学习的原理如下：你需要选择一个特定的学习率 alpha，它可以是时间步长的函数。你初始化价值函数，采样状态-动作奖励对和下一个状态。在这种情况下，因为我们在进行策略评估，所以让我来解释一下-这将等于状态 st 的 π，然后更新你的价值。好的，让我们再回顾之前的例子。我们说第一次访问蒙特卡洛你会得到1110000，每次访问都会得到一个奖励。

在这一集中，所有州的时序差分（TD）估计是多少？ 我们所做的是循环，对状态动作元组进行抽样，更新刚刚观察到的状态的价值。接着，我们再次抽样另一个元组，并继续这个过程。如果情况是这样的：我们起始状态是S3，然后经历了S3，A1，奖励为零，接着转移到S2。那么我们得到的序列就是 S2，A1，零，S2，S2，A1，零，S1，S1，A1 加一，最后终止。那么，你是否考虑过花点时间思考一下TD学习的价值以及它可能带来的影响呢？[噪音] 有人能分享一下他们认为这个价值是什么吗？[噪音] 是的。在最后全是零的情况下，产生的价值是什么？

这种情况是正确的。稍等一下，我们只有全零后继状态。因此，在这种情况下，我们只更新了最终状态的值。我想解释一下为什么会发生这种情况。在这种情况下，我们从一个状态开始，采取一个动作，得到一个奖励，然后转移到下一个状态。我们只更新这个状态的值。所以在这里我们做的是，我们进入状态S3，更新它，执行动作A1，然后得到了零的S2。所以S3的新值也等于零。然后我们转移到S2，采取动作A1，再次得到零，我们回到S2，得到了-所以我们更新了S2，它也是零。然后我们重复这个过程。最终我们到达了S1状态并且得到了一个奖励。因此，这可能是有用的，也可能是无用的，因为你以最简单的形式丢弃了数据。你有一个SAR的三元组，然后它就消失了。你没有保留它。所以当您最终看到那个奖励时，您无法再回溯。

这段文本讨论了蒙特卡洛方法在计算每个状态的回报时的延迟性和更新状态的问题。与传统的蒙特卡洛方法不同，现在在某个状态处的情况下不会保留先前状态的信息，也不会更新先前状态的值。这种做法可能需要更多的样本才能准确估计值函数。

在大多数情况下，总奖励与获得良好值函数估计所需的样本数量成正比。具体需要多长时间才能获得奖励以及估计值函数所需的样本数量，这取决于情况以及过渡动态。对于特定状态需要的样本数量，特别是在奖励是随机的情况下，情况会更加微妙复杂。

在传播这些信息回来时，所需的时间取决于情况的动态变化。当你有完全相同的路径并再次执行时，你会更新S2，如果再次得到相同的路径，那么信息会再次传播回S2，然后返回一次，再返回到S3。这样你可以逐步传播这些信息，但不会像蒙特卡罗那样立即实现。有一个问题，您能否强调一下这与我们上次谈到的Q学习之间的区别？因为它们看起来有一些相似的概念。TD学习和Q学习非常相似，特别是在控制时。因此，我们将关注行动。TD学习基本上是Q学习，在策略中进行修正。有一个下一个问题，就像你实际上实现了这个，所以你会继续正确循环并更新，或者是只是运行奖励？这取决于情况，不同的人可能有不同看法。

如果您真的非常关注内存使用情况，只需删除数据即可。在许多现有的深度学习方法中，您可以维护一种情景重放缓冲区，然后可以从中重新采样样本，以更新您的价值函数。重要的是处理数据的顺序，可以选择先进行一次数据传递，然后再进行一次，或者反过来。这些更新最终会传播，并影响到某些alpha返回到S_2那里。只要确保收敛即可，我们稍后会讨论这一点。当谈到元组采样时，实际上是进入一条轨迹，然后按顺序迭代该轨迹中的SAR 元组。所以，实际上我们是在不断重复这个问题。

我们正在沿着这条路径更新根据元组。 我们将您的代理视为在某个状态下采取行动后获得奖励并进入下一个状态。 因此，并不存在完整的轨迹。 就像开车时不知道接下来两分钟会发生什么一样。 我们不需要等到掌握完整的轨迹再更新，而是在每个时间步之后进行迭代更新。 我们选择元组的顺序可能与您获取和估计值有关。 接收元组的顺序会影响您的价值。 如果根据您在世界中体验这些的方式来理解，那么接收元组的顺序就映射到您在世界中体验它们的顺序。 这样，当前的状态S_t加上一个动作T将成为下一个时间步的状态ST。 这些数据不是从完整轨迹中采样的，而是通过当前状态和动作元组确定的。 如果您可以访问批量数据，您可以选择要训练的数据，这将影响您的收敛性。

您不需要提前确定要选择哪种问题。还有一点要提的是，有一些微妙之处，如果您将alpha设置为类似于1除以T这样的值，您可以确保在这些情况下收敛。有时如果alpha确实很小，也会在较小的条件下确保收敛。但是如果您将alpha设置为1，肯定会导致振荡。当alpha等于1时，实际上是在忽略之前的估计，对吧？所以，如果您将alpha设为1，您实际上只是在使用TD目标。如果根据这个图表并且将其视为对未来的期望，那么时间政策差异的政策评估会做些什么呢？因此，这就是上面所讨论的公式。其作用是通过使用下一个时间步S_t+1的样本来更新其值估计，以近似预期的下一个状态分布或未来分布。然后它会进行引导，因为它会插入。

您之前对 V_pi 的估计值加1。这就是为什么它是动态规划和蒙特卡罗的混合体，因为它会引导和利用蒙特卡罗，不会对所有后续状态进行明确的期望，而只是抽样一部分。为了比较这些不同算法及其优缺点，有时要视应用程序而定。虽然 TD 学习可能是最流行的选择，但最佳算法取决于具体领域。这取决于数据限制、计算资源或内存等因素。让我们花些时间简要讨论一下这个问题。所以让我们花点时间回顾一下这三种算法各自的特性。当没有当前领域的模型可用时，它们是否适用，无论处理连续的非情景领域还是非马尔可夫领域，以及它们是否能够在极限情况下收敛到真实值。

假设我们现在处于一个表格中，而不是函数逼近领域。您认为这些表格提供了公正的价值估计吗？如果您想在任何时候使用您的估算器，确保它是无偏的。那么，为什么不花时间去填写这个表格呢？请与旁边的人交流，我们将逐步解决这个问题。

嗯，当您没有当前领域的模型可用时，有哪些选择？动态规划需要当前领域的模型吗？是的，需要。好的。那蒙特卡洛呢？可用。对，可用。那么Temporal Difference (TD) 呢？也可用。是的，两个都可用。做这两者中的任何一个，TD通常被称为什么？作为无模型算法，它不需要明确的模型。它取决于对现实世界下一个状态的采样。

哪种方法可用于连续的非情景域？比如，您的进程可能永远不会结束。嗯，那TD学习可行吗？是的，可以。可以使用蒙特卡罗吗？

不行，不行，可以用DP吗？可以。是的。好的。其中哪一个，嗯，DP需要马尔可夫吗？是的。确实如此。哪一个-蒙特卡罗需要马尔可夫吗？不需要。TD需要马尔可夫吗？是的，确实如此。所以，嗯，时间差异和动态规划依赖于这样一个事实：当前状态的值不依赖于历史。所以，无论你如何达到当前状态，它都会忽略这一点，嗯，然后在引导时也使用它，它假设情况并非如此——所以，蒙特卡洛只是将你从现在位置到下次回报的总和。该集结束。请注意，根据您到达该特定状态的时间，您的回报可能会有所不同，并且可能取决于历史记录。因此，蒙特卡洛并不依赖于马尔可夫世界。嗯，你可以在部分可观察的环境中使用它。TD假设世界是马尔可夫的，因此我们到目前为止定义的动态规划也是如此。所以，你引导说，嗯，

在目前状态下，我的未来价值预测仅基于当前状态。因此，我可以获得即时奖励以及我转移到的任何状态。但这些只是足够历史统计数据，可以用于我的引导估计器。因此，这依赖于马尔可夫假设。那么，在非马尔可夫领域，我们该如何应用呢？嗯，这是一个很好的问题，非马尔可夫意味着什么？就是说，这些算法可以应用于哪些情况。是的，您可以将这些算法应用于任何事物。问题是它们能否在极限情况下收敛到正确的值。如果世界不是马尔可夫的，而这些算法也不是，那么它们就不能正常工作。就像[笑声]我们在AI辅导系统的一些工作中看到的那样，早些时候我们使用了一些来自分数导师的数据，我们试图应用马尔可夫技术，但结果并不理想。我的意思是，它们收敛到完全错误的结果，而且无论您有多少数据也无济于事，因为您使用的方法建立在错误假设之上。因此，您需要能够评估它们是否不符合马尔可夫假设或者尝试控制偏差或采取一些行动。嗯，否则...

即使在数据量无限的情况下，对政策价值的估计也可能出现错误。那么在极限情况下，收敛到真实值怎么样呢？假设我们再次处于马尔可夫环境中。对于马尔可夫环境，动态规划（DP）是否会在极限情况下收敛到真实值呢？是的，蒙特卡罗方法呢？是的，TD（Temporal Difference）呢？也是的。它们当然会如此。在次要假设下，渐近性不成立，所有这些都需要次要假设。在较小的假设下，它会收敛到价值的极限真实值，取决于α值。作为价值的无偏估计，蒙特卡罗是无偏的吗？是的。好的。TD不是。DP有一点奇怪。这可能有点不公平的问题。DP总是会给出该策略的准确值减去一个值。因此这是完美的，这是确切的值。如果您有K1-K减1个时间步来执行操作，那与无限时间步值函数不同。

你能解释一下最后两行有什么不同吗？ 就像我不明白价值的无偏估计量和收敛到订单真实值的东西之间的区别。 你的问题问得很好。 那么，问题是公正和一致之间有什么区别？ 嗯，所以当我们说收敛到极限的真实值时，这也被称为正式的一致估计量。 因此，该值的无偏估计意味着，如果您有有限数量的数据并计算统计量（在本例中为该值），并将其与真实值进行比较，那么平均而言，该差异将为零，并且对于 TD 之类的东西来说则不然。 但是，嗯，这可能是针对有限数量的数据进行评估的。 一致性意味着如果你有无限量的数据，你能得到真实的值吗？ 因此，这意味着，对于 TD，偏差必须渐近为零。 如果你有无限量的数据，最终它的偏差将为零。 但对于少量数据，你知道，

根据您提供的文本，进行更易理解的整理如下： 

你问的很好。在无偏估计值和收敛到真实值的差异，以及公正与一致性之间的区别是什么？所谓的收敛到真实值在这里也被称为一致估计值。无偏估计值意味着，当你用有限数据计算统计量（比如这里的值），并将其与真实值比较时，平均来看两者的差异为零，但对于类似于TD的情况并非如此。然而，这可能是基于有限数据进行评估的。而一致性意味着，如果你拥有无限量数据，你是否能获得真实值？也就是说，对于TD，偏差必须渐近为零。随着数据量趋近无穷，最终偏差将趋近于零。但对于少量数据，你可以看到，

对于有限数量的数据而言，实际上，你并不知道确切的N是多少。嗯，它可能是一个有偏估计，但是随着你拥有的数据量趋近于无穷大，它必定会收敛到真实值。因此，你可以获得一致的有偏估计。是的。关于蒙特卡洛，我记得你提到过，实施是否存在偏见会有影响 - 我记得你说如果每次采样都是无偏的[重叠]是个好问题。所以，这个问题恩，是的，这是第一次采样的无偏估计。每次采样都是有偏的。太好了。有问题吗？嗯，也许这个问题有点愚蠢，呃，一个愚蠢的问题，但是无模型的政策评估方法是否会不收敛呢？是的。问题在于是否存在无模型政策评估方法不能够收敛呢？是的，当我们使用函数逼近时，我们经常会遇到这种情况。当你刚开始时，现在我们处于表格情况，这意味着我们可以把值表示为表格或向量。而当我们转向无限状态空间时，情况就会发生变化。

很多方法甚至不能保证收敛到任何东西，甚至不——我们甚至没有讨论它们是否收敛到正确的值，它们甚至不能保证停止振荡。它们可以不断改变。好的，是的。问题。那么，有没有具体的解释为什么TD不公正呢？是什么？为什么TD不公正？为什么不公正呢？是的，很好的问题。所以，问题是，你知道，为什么TD有偏见。TD是有偏差的，因为您插入了下一个状态值的估计器，这是错误的。这通常会导致一些偏见。您插入的估计器不是真实的下一状态的真实价值函数。这会导致一些偏见。所以，问题实际上出在引导部分。蒙特卡罗也对期望进行了抽样，并且它是公正的，至少在第一次访问的情况下是这样。这里的问题是，你插入了错误的未来折扣奖励金额。

好的。所以，这段话总结了算法之间的比较。选择算法时，重要的属性包括偏差、方差特征、数据效率和计算效率。需要在这些属性之间进行权衡。一般而言，在不同形式的算法中，蒙特卡洛是无偏的但通常有较高的方差，并且是一致的。而时间差分（TD）有一定的偏差，但比蒙特卡洛的方差要低得多。TD(时间差分)零通过表格表示能够收敛到真实值，但当我们使用函数逼近时，并不总是会收敛。在最后几分钟中，当我们考虑到批处理设置时，我们将没有机会了解这些方法之间的相互关系。正如在火星漫游者的案例中所看到的，再次进行比较。蒙特卡洛估计会等到集合结束，然后更新每个州的访问情况。

TD 算法仅在每个数据点上使用一次，因此只会更新最终状态的值。如果我们希望多次检查数据会发生什么呢？如果愿意增加计算开销，我们可以获得更准确的估计，提高样本利用率。这意味着我们希望更有效地利用数据来获得更好的估计。这种情况通常被称为批量或离线错误政策评估，我们已经收集了一些数据，并且愿意尽可能多地对其进行检查，以更好地评估所使用的策略。假设我们有一个由 k 个剧集组成的数据集，可以重复采样一个剧集。那么我们可以选择在整个剧集上应用蒙特卡洛方法或 TD 方法。在这种情况下会发生什么呢？在文中，Sutton 和 Barto 提到一个例子，假设只有两个状态，分别为状态 A 和 B，折扣因子 Gamma 等于 1，并且您有八个片段的经验。在这八个片段中，您可能会在第一个片段看到状态 A，奖励为 0，然后是状态 B，奖励为 0。

在场景 B 中，你观察到开始是一个抱歉，然后是另一个集合。你进行了六次观察，在第八个集合中又回到了 B。你观察到了一个零。首先，我们能否计算出在这种情况下 B 的价值函数是多少？所以，这个世界的模型看起来像是这样：从 A 到 B，B 有时候为 1，有时候为 0，然后我们总是终止。所以在所有八个集合中我们都看到了 B。在其中六个集合中我们观察到了 1，在其中两个集合中我们观察到了 0。因此，如果我们进行蒙特卡洛估计，对于 B 的评估是多少，B 的价值是多少？所以，利用这八个片段进行蒙特卡洛估计，我们可以根据需要多次对其进行检查。

在这里，讨论到了关于批量数据集和蒙特卡洛更新的话题。在这个情景下，需要根据数据集对B的V值进行估计。对于TD估计，重要的是获取S-A-R-S数据对，并进行更新。这需要使用Alpha乘以R，再加上Gamma乘以下一个状态的V值，然后减去之前的估计值。通过多次迭代这个过程，可以不断调整Alpha的值以获取更准确的估计，直到最终收敛。

有人猜测，对于时序差分（TD）来说，B的状态值函数（V）是什么。它也是四分之三。这对于TD而言是一样的，因为每次你处于状态B时，你总是会结束。所以实际上这就成了一个关于状态B的一步问题，因此对于TD来说，它认为B的状态值等于六除以八，也就是四分之三。因此在批量更新方面，这两种方法是一致的。如果你可以获得无限量的数据，那么B的状态值等于3/8，因为在这两种方法下也是如此。有人知道在蒙特卡洛下状态A的值函数会是多少吗？好的，在蒙特卡洛下状态A的值函数将会是0。为什么呢？因为你和我只有一条关于状态A的轨迹，而且它的奖励是零。您认为时序差分可能会发生什么呢？

它会是 0 还是非零？它是非零。
非零，为什么呢？因为你从 A 引导了价值。
你能正确引导吗？所以是的，只有当你处于 A 时，你碰巧在该轨迹中得到零。但这就是在 TD 中，你会说，好吧，你立即得到了奖励零加上 B 的 Gamma 乘以 V，而 B 的 V 是四分之三。所以这里 Gamma 等于 1。因此，您对 TD 下的估计为四分之三。在这种情况下，我们不会收敛到同一件事。
那么为什么会这样呢？嗯，所以这就是我们刚刚经历的事情，我们可以从这些概率的角度去思考。嗯，那么这里发生了什么？批量设置中的蒙特卡罗收敛到最小均方误差估计。因此，它可以最大限度地减少观察到的回报的损失。在这个例子中，A 的 V 等于 0。TD 零收敛于动态规划策略，并对动态和奖励模型进行最大似然估计。因此，这相当于…

如果您本质上只是通过计算给定 S a 来估计 S 素数的 P hat。因此，在给定 A 的情况下，去 B 的概率等于 1。因为你唯一一次在 A 上，你去了 B，然后 B 的奖励等于四分之三，而奖励因为 A 等于 0，然后你可以用它进行动态规划，然后你就可以得到这个值。因此，TD 正在收敛于这种最大似然 MDP 值估计，而蒙特卡罗只是收敛于均方误差。它忽略了——好吧，它不假设马尔可夫。所以它没有使用他们这个马尔可夫结构。问题。只是为了在上一张幻灯片上确认一下，如果我要多次查看数据，因为对于 TD 学习，A 的第一次迭代 V 将为零，对吧？因为 A 的 V [听不清] 只是假设。但过了一会儿 B 的 V 收敛到了四分之三？[重叠]。所以，在，在，在线设置中，嗯，如果你只看到一次，那么这个-那么对于该特定更新，A 的 V 将为零。

如果你重复做了很多次，那么系统会向一个方向收敛。这时你可能会想知道哪个方向更好。如果你的环境不是马尔可夫的，你可能不希望系统收敛到马尔可夫的状态，所以蒙特卡罗方法会更适合。但如果你的环境是马尔可夫的，那么使用时序差分方法会有很大优势。因为时序差分可以利用马尔可夫结构，即使你从状态A没有获得奖励，也可以利用你对状态B的数据来更好地估计价值。我建议你考虑如何计算这些模型，类似确定性等效这样的方法。你可以收集数据，计算值函数和奖励模型的估计，然后应用动态规划，这通常比其他方法更有效率。下次我们将讨论一些关于控制问题的内容。谢谢。
