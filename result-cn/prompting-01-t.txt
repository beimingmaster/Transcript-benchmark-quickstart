好的，很高兴和你通话。我是 Jesse Moo，我是计算机系的一名博士生，和 NLP 小组一起工作。今天我们要讨论的主题是提示指令微调和RLHF，这是目前非常热门的话题，特别是在聊天机器人领域，比如GPT、Bing等。我们希望能够讨论如何去理解这些系统是如何通过充分培训来实现的。在本学期的项目中，定制项目提案和最终项目提案的截止日期即将到来，如果你还未完成，请尽快完成。我们正在分配项目导师，并将很快提供反馈。另外，作业5的截止日期是周五午夜，我们建议继续使用协作来完成作业，即使你已经获得了AWS或Azure的积分。如果有任何问题，请随时联系我。

在 Kaggle 笔记本中，您可以使用 GPU 运行。您在 Ed 上可以找到帖子，约翰也在 Ed 上发布了一份课程反馈调查，这也会影响您的绩效评估。请在周日 11:59 前填写反馈调查。接下来让我们开始今天的讲座，我们将探讨使用越来越大的模型的目的。这些年来，模型的计算量增加了数百倍，各种规模的模型训练了更多的数据。在第 10 节课中，我们讲过关于预训练和模型初始化时的情况。学会预测缺失文本中的句子需要真正理解语法、指代关系、情感等内容。在今天的讲座中，我们将继续探讨这些主题。

进一步推演这个想法可以得出：如果我们在所有文本上训练一个巨大的语言模型，将其视为基本的世界模型，尽管此类模型可能并不完全精通世界模型，但它们必须隐式地进行一定程度的世界建模。这是因为互联网上有大量信息，其中包含着人类集体知识，这些知识被记录和编写在网络上。如果语言模型能够准确预测文本中的下一个单词，那么它们已经开始学习代表和思考智能体与人类之间的关系，以及可能采用的信念和缩写形式。

举一个最近的论文中的案例：帕特是一位物理学家，他在演示中观察到保龄球和树叶在真空室中同时落下，此时我们可以要求语言模型继续完整这个句子。由于他是物理学家，我们会期待语言模型继续描述帕特拥有的哪些特点。

当我们运用相同的推理能力去预测AI智能体的行为时，我们可以看到一些相似的情况。假设帕特被告知在某个情境下保龄球和叶子会同时掉落，但事实上帕特以前从未接触过这个情境，然后巴勃罗预测保龄球会先着地，这个预测是错误的。这提示了一点：要善于预测文本中下一句，你必须能够推测出AI智能体的信念、背景、常识，以及他们可能采取的下一步行动。除此之外，当我们在网络上浏览时，会碰到很多涉及广泛知识的内容，这也许说明大语言模型实际上善于解决数学推理问题。如果它们接触过足够多的数学演示的话。在网络编程领域，热电联产是一个备受关注的话题，人们研究并在短短几周内展示实验，此外，甚至在医学领域，我们也开始考虑将在医学文本上训练的大语言模型应用于科学等领域。所以当我们认真思考语言建模时，这种情况就会发生，促使人们重新对构建语言模型产生兴趣。

这些模型基本上充当助手，你可以指派给它们任何任务，比如创建三道菜的菜单。 大语言模型应该可以努力完成这项任务，这是它们的承诺。当然，要实现这一点需要经过多个步骤。从我们最初的语言建模目标出发，这就是本次讲座的内容。 那么我们如何从句子中预测特定的下一个单词，发展到像聊天GPT这样的东西呢？你确实可以让它执行任何任务，但有时可能会失败。然而，在某些方面，它的表现可能会非常令人信服。好了，这基本上是我要讨论的讲座计划。因为我们正在运用这些大型语言模型，我们一直在研究越来越复杂的方法来引导它们逐步接近像聊天GPT这样的水平。因此，我们将从零样本和少样本学习开始，然后进行微调，接着考虑人类反馈或RL强化学习。好的，让我们先讨论一些少样本和零样本学习。

为了达到这个目的，我们将基于上周二的预训练讲座再次讨论。在预训练讲座中，约翰提到了一些模型，比如GPT（生成预训练Transformer）。这些模型只是解码器的语言模型，它们被训练来预测文本语料库中的下一个单词。早在2018年，这个模型进行了第一次迭代，包含1.17亿个参数，因此当时相当庞大，但现在肯定要小得多。它只是一个普通版本的Transformer解码器，使用了您所熟悉的技术，在大约4.6GB的文本语料库上进行训练。GPT展示了实现这一简单语言建模目标的潜力，并作为各种下游任务的有效预训练技术，这可能会引起您的兴趣。如果您想要应用于自然语言推理等任务，您可以将您的前提句子和假设句子连接起来，然后也许对模型产生的最后一个表示进行线性分类器的训练，但这是三四五年前的情况了。

为了不断改进，他们推出了GPT-2，并在2019年发布。它包含了15亿个参数，比GPT的架构大一个数量级，并且接受了更多数据的训练。他们从4GB的书籍数据转变为40GB的互联网文本数据，创建了一个名为webtext的数据集，通过抓取Reddit上的评论链接生成。尽管网络中可能存在许多垃圾信息和低质量内容，但他们选择从Reddit上收集链接，这些链接至少得到了一些用户的点赞，表明可能有人认为这些链接包含有用的内容。这相当于粗略地代表人类质量。这就是他们如何构建这个庞大数据集的过程。如果将2018年的GBT表示为一个点，那么2019年的GPT-2则会更大。有人可能会问这对你有什么好处？正因如此，GBT-2的论文标题指出他们的语言模型是无监督学习的。

多任务学习者通常会从发现的关键要点中获得提示。根据无监督多任务的部分，我认为GBT2的关键要点在于语言模型可以展示零样本学习能力。零样本学习指的是模型能够完成许多实际上没有明确训练过的任务，而无需评分更新。只需简单地提供正确的序列预测问题即可查询模型。举例来说，如果你对问答感兴趣，你可以在段落中插入关于汤姆·布雷迪的维基百科文章，然后提出一个问题，比如"汤姆·布雷迪出生在哪里"，并提供答案，接着只需询问模型来预测下一个标记。通过这种方式，您可以临时调整模型以执行其他任务（例如分类任务）。另外一个可做的事是比较不同序列的概率，这就是Winograd模式挑战，它是一个代词消歧任务。

解决这个问题需要一些世界知识，比如猫无法戴帽子是因为帽子太大。在这种情况下，"它"指的是猫更为合理，因为物体适合物体，因为它们太大了。这个例子强调了需要世界知识才能解决这个问题。为了零样本预测这个任务，如GPT-2这样的语言模型可以做出预测。你只需询问语言模型哪个序列更可能描述猫无法戴进帽子的情况，因为猫太大了。语言模型会认为是由于帽子太大而导致猫无法放入帽子。然后您可以对这些序列进行评分，因为这是一个语言模型，所以您可以进行零样本预测，并且最终在这个任务上表现得相当好。如果有任何问题，您可以进行更深入的探索。

在语言建模基准测试中，我们击败了最先进的技术，没有进行特定任务的微调。相反，我们直接在测试集上进行测试。举个例子，有一个名为Lambada的语言建模任务，其目标是预测丢失的单词。在这个任务中，预测的单词取决于之前的一些句子或更早的几句话。只需简单地训练您的语言模型，然后在Lambada任务上运行它，您会发现相对于各个领域的有监督、经过微调的最先进技术，您会表现更好。

对于其他任务，观察到了另一种有趣的现象。只要您知道如何自由地处理一些事情，您就可以看到有趣的零样本学习行为。举个例子，假设我们想要让我们的模型进行摘要生成，即使GPT-2只是一个语言模型。那么我们应该如何让它执行摘要生成这个任务呢？

他们的研究思路是利用一篇文章和一些新闻内容，最后在右侧加上一个"tldr"标记，表示太长不看。在Reddit上，许多内容允许使用简短准确的总结，以便读者了解，如果不想阅读全部内容，可以看到几句话的摘要。所以，当要求模型预测"tldr"标记后的内容时，期望得到摘要。这个概念的早期版本被称为提示权，目前正考虑定义任务的正确方法，以确保模型表现出您期望的行为。在观察到任务性能时，随机基准是底线。只需选择三个句子，我们使用的分数是欺诈分数。如果还记得自然语言生成讲座中的GPT-2模型，它实际表现并不出色，可能比随机基准好一点或几乎没有提升，但它正在接近我们的监督方法，这些方法经过明确微调。

虽然 2019 年的 GBT2 在性能上仍然落后于当时的最先进技术，但展示了语言模型具有发展潜力。2020 年出现了具有 1750 亿个参数的 GPT3，比GBT2 大一个数量级，这是前所未有的。对于大多数用户和数据而言，这种规模可能过大，于是他们再次调整了数据。文中提到“语言模型是一种少量镜学习器”，意味着 GPT3 重点是在少样本学习上。这意味着虽然GBT仍然可以进行零样本学习，但现在你可以通过几个示例来指定任务，并要求模型预测你关注的示例。这种方法通常被称为上下文学习。

强调这一点，当您学习一个新任务时，无需执行梯度更新，您实际上是在构建一个小型的训练数据集，然后将其融入到Transformer的上下文窗口中，然后要求其理解任务内容并预测正确答案。这与少样本学习的文献形成鲜明对比，后者假设您能够进行梯度更新。在这种情况下，模型实际上是一个冻结的语言模型，因此少样本学习并不那么有效。这引人注目的是，在这里有一个超级粘合剂的图，它是一个广泛应用的自然语言理解基准，他们采用了dbt3，这个数据点是用于零样本学习的。您提供了一个英语描述任务的例子，然后只需一个示例就可以要求模型完成任务，这一次推理过程后，您的精度将提高10个点。这不仅提供了自然语言测试的描述，还提供了样例输入和输出，您只需请求它解码下一个。

增加镜头数量确实会提高你的得分，但随着时间推移，回报会递减。注意到Hue使用了TPT3，因此没有对分级进行更新。他正在调整超级胶水任务，这比BERT的性能出色。值得一提的是，学习仅仅依靠记忆是令人兴奋的事情。尽管在互联网文本数据中可能会找到许多需要进行镜头学习的例子，但也有证据表明GPT3正在学习一些实时优化或推理等内容。这种证据以合成词解读任务的形式出现。作者提出了一系列简单的字母操作任务，这些任务可能在互联网文本数据中并不常见，比如通过循环遍历字母来获取单词的非循环版本。

因此，通过将 PL EAP 转换为 Apple，删除或重新排列单词中的字符，您所看到的是性能，随着模型规模的增加以及您可以进行的工作范围的增加，您很少进行零样本学习，这种能力进行少样本学习是一个新兴的模型规模属性。因此，在最大的模型中，我们实际上看到一个模型能够在特定上下文中执行此操作。好吧，问题是，我注意到单词的反转，就像性能一样令人畏惧。是的，因此问题在于单词的反转任务仍然很困难。是的，这是该模型仍然无法解决的任务的一个例子，尽管我不确定我们是否使用更新的模型进行评估，也许您知道最新版本确实可以解决该任务。问题是，这里有一些直觉，解释为什么这些主题是模型规模的结果，我认为这是一个非常活跃的研究领域，每周都会发布相关论文，因此我认为还有许多有趣的内容。

在研究中，试图探索类似合成任务，例如GBT-3可以学习线性回归任务，并且一些模型可以用于解释性任务，比如关注层或隐藏状态中的内容如何影响学习。我需要向您推荐最近的文献，其中包含其他激动人心的内容。传统微调的总结通常涉及将任务交给模型，然后对每个示例进行评分，最终希望得到一个在某些输出上以及新型提示语言范式中表现良好的模型。我们常用的方法是给定一些例子，要求模型正确预测答案。提示的使用确实有一些限制，特别是对于更困难的任务，有许多任务可能看起来过于困难，特别是那些需要深入人类推理的。

有一些任务需要更丰富的推理步骤，这对人类来说是相当困难的。举个例子，GPT3是一个大语言模型，在处理较大数字的加法时表现不佳，这意味着如果给GPT3一系列加法示例，它可能无法正确完成任务。这部分原因在于人类也不擅长一步完成这类任务。就像要求你在没有纸和笔的情况下快速计算出两个数字的和，你也会发现很困难。

因此，有一种方法是改变提示的方式，希望以此获得更好的性能。一种想法是使用思维链提示。在传统的提示中，我们会提供一些我们想要完成的任务的示例。举个数学应用题的例子，我们会提示给出问题和答案。然后，针对我们真正关心的数据点，我们要求模型预测答案。模型会尝试产生正确的答案，但可能会出错。

因此，思维链提示的想法实际上是要展示给您，您希望模型进行何种推理。在这些提示中，您不仅提出问题，还提供了答案，并指出了正确答案所需的推理步骤。实际上，这是关于您如何对网球问题进行回答并得出正确答案的推理过程。语言模型在这里起着至关重要的作用，它可以激励您遵循模式并继续提示。如果您再向其提出问题，它将生成一个答案，然后是一个理由，最后是答案。因此，您在一定程度上要求语言模型自行完成这些步骤，这样一来，最终您将得到正确的回答一些问题。否则，您的想法可能不会那么简单，但这种方法已被证明是非常有效的。这是中学数学应用题的基准。当我们扩展到GPT模型和其他一些类型的模型时，我们可以看到思维链提示是否正确出现。

这些日益庞大的模型在性能上接近监督学习基线，是的。似乎提到添加了“驳船成员”的问题，您是否对具有以下特征的人的思维链获得了类似的结果？比起中学数学应用题更复杂的问题是，确实存在思维链，而不是提示我提出的那些加法问题的策略。在实际论文中应该有一些结果，虽然它们不在这里，但您可以查看。是的，您应该知道在无梯度更新的情况下模型是如何进行训练的。我最近在欧洲，直觉告诉我模型在没有梯度更新的情况下是如何学习的。是的，这与之前提出的问题相关，比如这实际上是如何发生的。这又是一个活跃的研究领域，所以我对文献的理解就像您可以证明模型几乎像上下文中的梯度下降一样，因为它在编码提示方面工作。您可以使用类似的模型可解释性方法来分析实验，但我...

是的，我很高兴可以在论文中更仔细地研究这个问题。我认为后续工作中会提出这样一个问题：我们是否真的需要收集人类在解决问题时的推理示例？或者我们是否只需要模型进行推理，只要问题问得当就可以了呢？这引入了一个叫做"零射击思维链提示"的想法。我认为这可能会有很大的影响，就像我在论文中看到的那种简单影响一样。这个想法很简单，不需要构建思维链，只需要提出问题，然后在标记前面给出答案，让模型逐步进行正确推理。模型会解码，并通过推理产生正确答案。这种方法在一些算术基准问题上是否有效呢？当只有零样本提示时会发生什么呢？

只要模型能够立即回答问题，而无需进行任何推理过程，仅需一些示例输入和输出，这就是零样本思维链的思想。这种方法要求模型思考所有可能情况，与我们手动进行推理比较时，通常会得到很高的准确性。虽然手动思考链仍然可能表现更好，但这个思想展示了一个简单的概念，最终提高了模型性能。因此，这篇文章的有趣之处在于，他们展示了只需逐步减少输入信息也能获得改进的性能数字。

他们在零样本基线性能方面做了很多尝试，通过搜索不同的前缀答案来证明思考和逻辑推理的重要性。他们逐步发现逻辑思考是最佳的方式。实际上，他们今年晚些时候建立了一种使用语言模型来搜索类似最佳字符串的方法，这显著提高了任务的性能。尽管可能存在过度拟合的问题，但他们发现逐步思考是最有效的方法。

为了保持原文意思不变，下面对内容进行了调整和润色：

正确答案能够为模型本身增加信心。对于你来说，这可能看起来像是一种完全神秘的艺术形式。因为我们对于所发生的事情缺乏直觉。但我们正设法建立一些直觉。然而结果是，我相信你已经注意到了，如果你在科技领域花费时间，或者在互联网上看到过一些新概念，那么快速工程就是一门新兴的科学和职业。这包括询问模型进行推理等内容，甚至突破语言模型的限制，让它们执行非训练过的任务。即使你了解空气动力学中的移动车辆或者稳定扩散等复杂概念，构建这些想法也能为你提供想要的模型输出。我曾听说过有人说，他会用热电联产模型，但首先要加上谷歌的代码标头，因为这样生成的代码会更专业或者更正确，这取决于你对谷歌的信任程度。是的，确实有一篇维基百科文章提到这一点。

现在一些新兴公司正在招聘实时工程师，并提供相当高的薪水。所以，如果你想成为一名实时工程师，一定要提升你的GPT和Spring技能。关于这个话题，抱歉，我想问一下，你能说得更具体些吗？他们的设计更像是长尾输出，你怎么能使用大语言模型来设计呢？我认为他们将其视为强化学习问题。但我建议你查阅Joe等人在2022年发表的论文以了解更多细节。是的，这篇论文提到的问题，我有点好奇他们如何提供反馈。如果模型没有给出正确答案，他们会如何提示？比如，是否会指出错误？或者会采取什么方式？因为在客户服务中，反馈很重要。他们是否考虑在这个动态链条中提供实验反馈？他们似乎只关注模型得到错误答案的情况，而不会考虑如何提供反馈。他们只关注于评估准确性是否正确，但这也是很重要的。

AI反馈的思想非常有趣。在稍后的讨论中可能会涉及到一些提示。零样本和少样本学习有很多好处。你无需进行微调，可以自定义提示以获得更好的性能。然而，缺点是适应的内容是有限的，比如固定的上下文窗口，可能需要更复杂的任务和微调。指导微调的概念旨在确保这些模型得到真正复杂任务的适应。

当给出提示时，我们希望让用户进行有趣的操作，但是一个问题是语言模型是通过训练来预测最有可能的标记序列，这与我们期望它帮助人们的行为有所不同。举个例子，如果我向 GPT-3 提供类似“解释登月”的提示，这个模型会被训练成预测在互联网上看到这个提示之后会出现的最可能的延续，可能有人会提出与六岁的孩子相关的系列想法，但这并不会直接回答你的问题。因此，问题在于语言模型不一定符合用户的意图。那么我们如何才能更好地将模型与用户对问题意图的匹配呢？这其实很简单，我们是机器学习者，我们可以进行微调来使语言模型根据用户的意图作出反应。让我们正确地调整模型的提示以更好地满足需求，这可以通过对预训练模型进行调整来实现。

在讲座中，幻灯片指出，通过再次进行预训练可以改善 NLP 应用程序，方法是用作参数初始化。与普通微调单个感兴趣的下游任务（如情绪分析）不同，这里是要微调许多任务，这样就可以概括测试时未见过的其他任务。因此，数据和规模是关键，需要收集许多不同任务的大量指令输出对的示例，然后微调语言模型，并评估对未见过的任务的泛化效果。举个例子，最近发布了一个名为"超自然指令数据集"的数据集，包含超过16000个任务，包括300万个示例，涵盖翻译、回答、生成甚至编码数学等内容。

当你看到这些时，你开始思考这到底是微调还是更正确的预训练，实际上两者都是正确的。我们模糊了这些界限，我们正在训练的规模基本上是通用的，但比预训练任务中的语言建模稍微具体一些。那么，现在我有一个问题：我们正在训练我们的模型来执行如此多的任务，我们如何正确评估这样的模型呢？因为你不能简单地说：“现在你能很好地进行情感分析了吗？”评估这种语言模型的任务规模必须更大。因此，很多研究正在为这些大规模多任务语言模型建立基准，并看看它们在多大程度上不仅可以完成一项任务，还可以完成多项任务。因此，这就是大规模多任务语言理解（MMLU）基准测试，它包括一系列任务，用于测试语言模型在一系列知识密集型任务上的性能。

这项任务是由高中生或大学生完成的，因此您不仅需要测试大语言模型，还需要向他们提供分析结果。在天文学、逻辑学和欧洲历史领域，我有一些数据要分享。您可能不太熟悉dpt3，但它的表现肯定比所有这些任务的随机基准要好。这是另一个例子，它超越了模仿游戏的基准或大工作台。大约有十亿位作者参与了这个巨大的协作努力。这是任务评估的词云，其中包含一些非常深奥的任务。以下是一个示例任务，必须在ASCII艺术中提供汉字或日语字符，您需要正确预测字符的含义。因此，我们正在对这些大语言模型进行严格的压力测试。

好了，让我们讨论一下指令微调是否有效。回想一下，有一种名为T5的编码解码器模型，这是Google开发的一种模型。它经过预训练，针对跨度损坏任务进行了调整。

因此，如果您对细节不太清楚，可以参考这个讲座。作者发布了一个名为"flan T5"的新版本，其中"flan"代表微调语言。这是在另外1.8万个任务上训练的T5模型，包括前文提到过的自然指令数据集。如果我们对大型工作台和LU的性能进行平均并标准化，我们就会看到微调指令是有效的。最重要的是，模型越大，通过正确微调所获得的好处就越多。因此，大型模型确实可以通过微调取得出色表现。您可能会认为这对于学术界或者没有大型GPU集群支持的个人来说有些令人沮丧。不过，如果看到在微调后，8000万参数的最小模型最终的性能表现有多好，以及与微调的110亿参数模型相比，您可能会发现文献中有很多关于小指令微调的预训练模型的资料。

相比于较大的模型，如T5，有一些优势，这些大模型的尺寸是正确尺寸的许多倍。因此，对于那些遇到GPU问题的人来说，仍然有希望。为了真正了解这些功能，我强烈建议您亲自尝试。例如，Flan T5托管在Hugging Face上，我认为Hugging Face有一个演示，您可以在其中输入一些内容，提出查询要求，然后查看它的执行情况。但是要注意，有一些工作需要事先进行定性示例，比如四个问题中的一些可能无法通过简单微调模型来解决，这些问题需要执行指令微调。这样可以使您的模型更准确地推理并给出正确的答案。指令微调的方法非常简单，它只是对模型进行正确微调。您会看到这种能力可以扩展到看不见的任务。至于负面因素，有关此方面的了解目前还很有限。

为什么 我 会 遇到 指令微调的缺点呢? 我 不 希望 发表评论，因为 它 似乎 受到 与 任何 人类来源 数据相同的 负面影响。 如何 使 人们 提供 像 不同的人 对它 的 看法那样 的 输入，让 评论 有用呢? 获得 人类 标签 很困难 且 昂贵， 这 无疑 是 很重要的。 你 提到的 最后 一部分 可能是 人们 可能不同意 正确的 标签 是 什么。 问题 正在 越来越明显。 显而易见的 限制 是 金钱，要 收集 大量 地面真实数据 为 如此 多的 任务 需要 花费 大量 资金。 那么 还有 哪些 限制呢? 有 很多 限制，包括 我们 之前 提到的 一些。 所以 当 我们 开始 要求 我们的 模型 执行 更加 创新 和 开放 的 任务 时， 有一些 任务 没有 正确的 答案， 这 就是 你 说 这 是 一个 什么 样的 问题。

编写一个关于一只狗和我们的宠物蚱蜢的故事是一件有趣的事情。在这个故事中，我们可以描述狗和蚱蜢之间的友谊以及它们生活中的冒险。这个故事可以展示它们之间的相处方式，以及它们如何彼此带来快乐和乐趣。

故事中可以描述狗是一个活泼好动的宠物，经常跟随着家人四处活动，而蚱蜢则是一个安静而神秘的宠物，常常隐藏在角落里或花园中。它们虽然生活习性不同，但却建立了一种独特的友谊，互相尊重对方的特点。

在故事中，可以描述狗和蚱蜢一起探险的场景，或是它们一起玩耍的时刻。通过这些情节，展现宠物之间的默契和互动，让读者感受到它们之间微妙的联系。

这个故事不仅可以展示出宠物之间的友谊，还可以通过狗和蚱蜢的冒险经历，传达出合作和相互理解的重要性。最终，这个故事可以以一个温馨的结局来表达对友谊和互助的珍视。

在语言建模任务中，如果我们设定一个目标，即对预测标记的错误进行同等惩罚，但事实上这个目标并不符合用户的期望。这可能是由于事实、创造力或者仅仅是人类偏好的想法。一个解决办法是根据标记之间的距离来调整惩罚，以减少这种情况。举例来说，音乐剧和冒险之间的距离可能更大，因此可以考虑给予不同的惩罚。这是一个有趣的问题，我之前没有听说过有人这样做，但这个想法似乎是合理的。或许可以尝试类似的对抗性设置，或者用“嵌入距离”这个概念。例如，在音乐剧和表演中可能会很接近，因为它们都是娱乐节目，但它们的本质完全不同，一个是真实的，一个是虚构的。所以，或许可以尝试一下，尽管我认为可能会面临挑战。

在接下来的演讲中，我们将更加明确地尝试满足人类的偏好，并为此提供一个数学框架。这些情况可能会有些棘手，但这正是让人感到兴奋的地方。在训练一个语言模型完成任务时，比如摘要，我们可以设想对于每个样本，设计一种获取人类对摘要的奖励的方法。这样，我们就可以使用奖励函数来评估摘要质量，我们将其标记为R的分数。当我们总结一篇文章时，我们会得到一个摘要，假设这是一个相当不错的摘要；而如果有另一个总结，可能效果就不那么好。这些就是我们在从人类反馈中进行强化学习时所面临的情况和限制。

如果我们能要求人类对所有这些输出进行评分，我们想要最大化或满足的目标是非常明显的，我们只想要最大化样本的预期奖励。我们的语言模型是正确的，因此当我们从我们的语言模型 P Theta 中获得样本时，我们只想要最大化这些样本的奖励。相当简单，为了数学上的简单性，我假设只有一个任务或一个任务提示，所以让我们想象一下，我们只是想要像这篇文章一样进行总结，但是我们可以稍后讨论如何将其扩展为多个提示。这种任务属于强化学习领域，我不会假设你们都熟悉它，可能有人比我更熟悉。强化学习领域已经研究了这些类型的问题，这些优化问题是关于如何优化某些事物的。模拟优化已经有很多年了，在2013年，人们重新对深度学习的强化学习产生了兴趣。

你可能已经从Deepbind中看到了关于AI代理学习在玩Atari游戏和掌握的结果比预期提前很多的信息。但有趣的是，我认为另一方面，在现代大语言模型上应用强化学习的兴趣有点新颖。我认为最早的成功案例之一仅出现在2019年。因此，在这种情况下，我认为该领域整体上存在一些争议，即使用大语言模型进行强化学习确实很困难。部分原因是大语言模型非常复杂，将其视为具有动作空间，可以生成各种句子，这样的搜索空间非常庞大，因此这仍然是一个非常具有挑战性的问题。这只是部分原因，但事实上，我认为已经有了一些更新的算法，似乎对于深度强化学习模型，包括大语言模型在内，效果更好。这些算法包括近端策略优化等，但在本课程中我们不会详细讨论这一点。

然而，这就是我们开始重新了解的原因。对于使用语言模型进行强化学习的想法感兴趣的人，那么我们应该如何最大化主观权利呢？我已经把这写下来了。理想情况下，我们应该调整我们的参数数据，这样奖励就会很高。但实际上，情况并非如此，这一点还不是很清楚。当我们思考时，我的意思是我们在课堂上学到了些什么，我们知道我们可以进行梯度下降或者梯度上升，所以让我们尝试做梯度上升。我们将最大化这个目标，所以我们将朝着最陡的梯度方向迈进。然而，这很快就会成为一个问题，即这个量是多少，我们如何正确评估它。考虑到梯度是我们采用Theta的梯度变量，我们如何估计它在期望的样本中出现的概率呢？其次，如果我们的奖励函数是不可微分的，就像人类的判断是不可微分的一样，那么我们将无法通过它们进行支持。因此，我们需要让其能够与黑盒奖励函数一起工作。因此，在强化学习中存在一类问题。

这种方法被称为策略梯度方法，它为我们提供了估计和优化目标的工具。在这门课程中，我尝试以最高可能和最高水平的直觉描述这种方法，着眼于数学并展示发生的情况。然而，这会涉及到很多细节，而强化学习的全面处理超出了这门课程的范围。如果您更感兴趣这方面的内容，可以参考CS234的强化学习课程。总的来说，我认为这可能会变得有点混乱，但如果你并不完全理解也没关系，我们会进行讨论。

最后，我们将重新整合并展示这些内容，就像对于怎样进行LLM意味着什么一样。但我的目标是描述我们如何实际估计这个目标，所以我们希望获得这个梯度。因此，它是对期望语言模型奖励的梯度模型。如果我们进行数学计算，我们可以将其分解，这就是我们对期望的正确定义。我们将对所有句子的概率进行评级，并对它们进行求和。

由于梯度的线性性质，我们可以将梯度运算符嵌入其中。现在，我们要使用一项非常方便的技巧，称为对数导数技巧，虽然被称为技巧，但实质上只是链式法则。让我们看看当我们采用这项技巧时会发生什么：计算来自我们语言模型的样本的对数概率的梯度。因此，如果我们对梯度取值，我们需要正确应用链式法则。这样某个事物的对数梯度将等于1乘以中间的梯度。因此，将梯度的 s 的 P(Theta) 重排后，我们可以将其写成一个乘积的形式，即 s 的 P(Theta) 乘以 log P(数据) 的梯度。我们之所以这样做是因为我们希望转换成一种易于估计期望的形式，因此我们进行了这一步。仔细观察最后一个方程式中的第一部分。

在对模型中的一组样本进行求和的情况下，我们根据每个样本的概率进行加权，这就是期望的定义。换句话说，我们可以将其重新表述为期望，特别是对这个量的期望。因此，通过这种方式重新表述，我们得到了这个目标的另一种形式。因此，这两者在这里相当于顶部和底部。在这里所发生的是，我们在期望值内计算梯度。如果这是有意义的，那么为什么这么做有用呢？在我继续之前，有人对此有任何疑问吗？如果你不明白，也没关系，因为这并不意味着我们需要理解背后的直觉。好了，我们已经将其转化为这种形式，将梯度放入期望值中。这意味着我们现在可以使用蒙特卡罗样本来近似这个目标。因此，近似任何期望的方法就是采样一组样本，然后求其平均值。这些都是正确的，因此大约等于...

我们的模型从有限数量的样本中进行采样，然后计算奖励的平均值乘以该样本的对数概率的梯度总和，从而得到更新规则。通过这种方式，我们创造了一种期望的感觉。那么这意味着什么呢？让我们以一个简单的例子来考虑，假设奖励是二元的，即要么是零，要么是一。比如，想象我们正在训练一个语言模型来谈论猫，每当模型生成包含“猫”这个词的句子时，我们给予奖励，否则给予零奖励。在这种二元奖励的情况下，有人知道目标函数会逐渐变得如何吗？就好像任何想法一样，我可能无法说清楚，但奖励就像一个指标一样是正确的。因此，基本上，奖励是一种指导性的信号。

除了包含单词"猫"的句子之外，在这种情况下是正确的基本上，看起来就像是一种香草香草分级血统，只是在包含单词"帽子"的句子上。因此，我们可以将其概括为更一般的情况，其中奖励是标量。如果你看一下，这看起来像什么？如果R非常高非常正，那么我们将该样本的梯度乘以一个很大的数字，因此我们的目标将尝试采取分级步骤，以最大化再次生成该样本的概率，从而带来高回报。另一方面，如果R较低甚至为负，那么我们将积极采取措施，尽量减少再次发生这种情况的可能性。这就像英国人对这里发生的事情的直觉。这就是我们称之为强化学习的原因，因为我们想要强化良好的行为并增加它们在未来再次发生的可能性。希望这对你们所有人来说都直观地有意义。假设您正在玩一款视频游戏，并且在一次运行中您获得了超高分，并且您想一想哦，这真的……

很好，就像之前我所说的那样，我们应该再次尝试这个方法。我们试图用这种更新问题的方式来捕捉所有相关信息，这就是为什么我们选择使用策略梯度，而不是值迭代或其他可能的方法。事实上，我们有许多选择。我认为已经有一些方法可以利用语言模型进行 Q-learning 的离线学习等等。我认为设计空间还没有得到充分的探索，因此在RL中有许多悬而未决的问题，对于那些有兴趣思考在RL中可以做些什么新奇事并将其应用于语言模型的人来说，这是个挑战。

在实践中，我们使用的并非简单的方法，而是更为复杂的方法，接近端到端的方式。问题空间是否像极大的超空间一样广阔，这几乎是确定的，所以这是个挑战。另外，我没有提到的一点是，现在我正在谈论的是完整的句子样本，就像在实践中，当我们进行RL时，我们实际上是在单个标记的水平上生成的，因此每个标记可以理解为GPT有50,000个标记，所以这是相当复杂的。

尽管拥有广泛的操作空间，但它仍然是可以管理的。对于我提出的问题，你能否看出与此目标相关的任何问题？这个目标很简单，但需要更多技巧来完成，希望这能让你有更直观的理解。我们首先要做的是，现在我们准备就绪，我们有来自大型语言模型的许多样本。对于任何奖励函数，就像我们要求人类对这些样本进行评分一样，我们可以最大化奖励，这样我们就达到目标了。但事实并非如此快速，存在一些问题。首先是指令微调案例中的一个共同问题，即让人类参与每个循环是昂贵的，就像我并不真正想要监督语言模型的每一个输出那样。我不确定你是否都会有这种感受，因此我们需要想办法解决这个问题。因此一个想法是，不是每次都需要询问人类的偏好，

这个想法实际上是由诺克斯和斯通在一篇论文中首次引入的，他们将其称为Tamer。在这个想法中，我们将重新实现他们的方法。我们将训练一个称为奖励模型 (RM) 的语言模型，通过参数化处理只通过费用来预测人类偏好，然后在进行强化学习时，我们将优化奖励模型的奖励而不是实际的人类奖励。这个方法涉及一个概念性问题，在评估任务的新示例中，要询问人们这个示例的评分，某人给我一个数字，还有人想评论这个示例吗？就像评分一样，我们用什么尺度等。因此，当单独询问人们时，人类的判断可能会很喧闹并且存在校准错误，解决此问题的一种方法是避免直接询问。

要求的不是直接给出评分，而是让人们比较两个摘要并做出判断哪个更好。已有多种形式证明，人类与受试者一起工作的领域，人类的反应更可靠，包括心理学和医学等。因此，我们不会要求人类仅给出绝对分数，而是让他们比较不同样本并判断哪个更好。举例来说，第一个样本可能比中间样本更好，也比最后一个样本更好。现在有了这些成对比较，我们的奖励模型将生成一种潜在分数。因此，基于这些成对比较数据的隐式分数，我们的奖励模型是一个语言模型，接受一个可能的样本，然后生成一个数字作为分数或奖励。我们训练这个模型的方式是经典的统计比较模型。根据以下原则，奖励模型本质上应该预测一个更高的分数。虽然在这里不需要了解太多细节，但这是一种经典的统计比较模型。

在比较样本时，得分较高的样本应该比得分较低的样本表现更好。如果根据主观意见来进行交易，可以训练一个语言模型，让其学习为不同事物分配分数，以显示其相对偏好。我们可以将这些输出作为奖励信号，通过标准化来确保分数在预期上为零。这对强化学习等任务非常有益。是否在训练过程中进行这种标准化处理我记不清了，但在训练模型之后，调整奖励模型以确保分数期望为零是很重要的。

即使存在嘈杂的情况，就像有些人认为S3比S1更好一样，我们如何解释这一点呢？即使像排序边界一样令人困扰，我认为这取决于每个人的偏好，在没有得到整体人类看法之前，我们无法得出基本事实。这只是一个限制。我想表达的是，只有在有足够的数据支持的情况下，噪音才会减少，但这当然是一个问题。

下一张幻灯片也会涉及到奖励模型的有效性问题，我们能否真正通过这种方式学习模拟人类的偏好。这显然是在实际尝试之前必须检查的一个关键标准，以优化这一目标。他们对此进行了测量，因此在正确的标准类型验证集上评估奖励模型，这能使奖励模型预测他们在训练期间未曾见过的数据点的结果。奖励模型的预测是否会根据模型的规模或数据量的正确性而发生改变。如果你留心这一点，你会发现这里有一些限制。

虚线代表了人类的基准水平。如果让人类来预测结果，他们不会达到100%的准确性，因为人类意见不同，即使是五个人的集合也无法达到100%的准确性，因为人类有不同的偏好。然而，重要的是，对于最优模型和充足的数据，奖励模型至少要达到人类一些验证性能的水平，这就像是一种绿灯，或许我们可以尝试一下，看看会发生什么。因此，毫无疑问，这是我们大语言模型的一部分。我们有一个预训练的模型，可能是Transformer结构，我们有一个奖励模型，它为语言模型输出生成标量奖励，正在通过人类比较的数据集进行训练。我们有一种策略梯度的方法来优化任意语言模型，并调整一些奖励函数的参数。

所以，如果你想进行零样本学习，你可以复制一个预先训练过的模型，我们将其称为模型的副本，这个副本是我们基于强化学习要优化的参数数据。我们通过强化学习来优化一个更复杂的奖励，而不仅仅是简单的奖励模型，其中还包括一个惩罚项目，以防止我们偏离预先训练的模型太远。这个想法被称为KL或Cold Black Labor（冷黑劳动）RL模型和预训练模型之间的差异。我将在几张幻灯片中解释为什么我们需要这个额外惩罚，基本上，如果你过度优化最终的奖励模型，你可能会产生类似乱码的结果，这是一种代价。因此，如果在强化学习过程中，模型生成某个结果的概率远高于预训练模型生成同样结果的概率，那么这个差异就会很大，预训练模型会认为这是一个非常不可能的字符序列，任何人都会这么说。

在进行调整时，beta是一个可调参数。当初始化一个副本时，意味着在第一次迭代时PRL等于ppt。基本上，初始化一个副本就是为了能够与未微调的模型进行比较，以评估惩罚项，只需保留预训练大语言模型的预测结果。这有效吗？肯定有效，至少对于《每日邮报》数据集的任务总结来说是关键要点。我们研究了不同模型大小，最终看到，如果只是进行预训练，就像GPT使用的典型语言建模目标一样，最终生成的摘要通常不如参考摘要优秀。因此，在Y轴上，这是...

人类输入次数会影响模型生成的摘要，这些摘要会被视为人类实际编写的摘要或数据集中的摘要。即使进行监督学习，预训练也不能很好地工作。在这种情况下，监督学习实际上是在微调我们的模型。即使如此，模型生成的摘要仍然会比参考摘要表现不佳，因为模型没有完美地对这些摘要进行建模。只有通过人类反馈，我们最终才能生成一个更好的语言模型。最终生成的摘要被认为比最初训练数据集中的摘要更好。我觉得这个问题非常有趣。

现在我们开始谈论的是，我们越来越接近构建像GPT或聊天GPT这样的东西。指导GPT的基本思想是将语言模型扩展到不仅仅是我之前描述的提示。

这段文字描述了三个部分的内容。首先是指令微调，其次是rohf，第三部分抱歉，是rohf。它们之间的区别在于使用了30,000个任务。这实际上涉及任务的规模和多样性，在这些方面获得良好性能非常重要。目前的结果表明，不需要太多机会，但在监督学习方面，数据效果不佳。然而，在第一阶段微调时进行了监督学习。这引发了一个很好的问题，所以我想强调一下。

我认为关键问题在于他们首先通过监督策略初始化了强化学习（RL）策略。这样做的目的是让模型在摘要生成方面表现出色，然后再通过RL进一步提升性能。不过，你提出的问题是，也许我们可以考虑从预训练的基线开始进行RL。这是一个很好的问题，我认为他们可能并没有探索过这个角度，虽然我不能确定是否必须这么做，但我需要再次查阅论文来确信。所以，肯定会类似于指导GPT的方式，即假设首先需要微调阶段，然后在此基础上构建。不过我认为，仍然存在一些有趣的开放问题，比如是否可以直接进行lhf，是的，这是一个问题。同时，奖励模型是否应该首先被训练，你首先训练它以确保其功能。

这是一个关于从语言模型中产生奖励的优化问题。具体来说，就是如何针对人类的奖励信号进行优化，这些信号可能来自于大语言模型产生的文本，或者从停止训练的样本中产生。因此，一个关键问题是奖励信号从何而来。对此，可以采用迭代过程，不断重复第二步和第三步。首先从语言模型中采样一系列输出，然后让人们对这些输出进行评分，接着通过强化学习再次更新模型。之后再次采样更多的输出，并让人们对其进行评分。通常，奖励是根据采样模型输出的评分来确定，因为这些输出可能是向着某个目标方向引导的。在迭代过程中，还可以执行强化学习，不断训练一个更优的奖励模型。在这个过程中，可能会不断尝试训练新的奖励模型，继续优化。至于在结构中进行的迭代，例如问题涉及到新的输出如何训练一个更好的奖励模型，他们在这方面可能做了一些尝试。

所以，3 万个任务呃，我认为我们正在进入最近的一个领域，你知道越来越多的公司像开放人工智能一样分享越来越少的细节，就像训练这些模型时实际发生的情况一样。所以我们对这里发生的事情不太清楚，也许我们过去曾经经历过，但他们确实共享了数据集不是公开的，但他们确实共享了以下类型：他们从贴标签者那里收集了正确的任务，所以他们从已经使用GPT-3 API的人那里收集了一堆提示，这样他们的好处是让你了解他们API的许多用户，并执行用户可能会执行的任务类型要求GPT这样做，其中包括集思广益之类的事情，或者你知道开放式一代等等。是的，我的意思是指导GPT的关键结果，这是聊天EBT的支柱，你知道，你知道真的只需要被看到和玩弄明白，这样你就可以随意使用ChatGPT或其中一个开放式AI API，但同样是语言模型的示例，我不一定。

通过执行这种微调任务，然后进行再强化学习，您将获得一个模型。更重要的是，您将学会更好地遵循用户指令。同样，语言模型可以非常擅长生成超级有趣的开放式创意文本。让我们聊一下 GPT，这是最新的更新。我们所知道的关于实际情况以及当前训练内容非常有限。他们正在保留他们的“秘密酱汁”，但他们发布了一篇博客文章。在其中，他们提到他们在调整模型方面取得了很好的进展。我们通过监督微调训练了一个初始模型，模拟了人类的对话训练方式。我们让人类 AI 训练师扮演双方角色，然后要求他们扮演 AI 助手，以此来微调我们的模型，使其模仿人类智能系统的行为。这是第一部分。

为了制定强化学习的奖励模型，我们收集了大量数据，并与早期版本的聊天机器人进行对话，以进行指令遵循或微调的预训练。接着，我们收集了多个样本并对样本质量进行了正确评估。随后，利用这些奖励模型，我们使用了PPO对其进行微调，这是强化学习的高级版本。这样，您就了解到我们无需介绍TBT的聊天功能。最近TBT的表现令人兴奋，这是一个很好的例子，让人们感觉很有趣，绝对值得一试。抱歉，这可能对刚开始学习的学生有些难以理解。是的，强化学习再加上直接建模，确实可以带来很多好处。

你关心的是人类的偏好，而不是我收集的数据。最终，我要满足人类偏好的程度。相比于指令微调等方法，这带来了明显的优势。关于负面因素，强化学习是一项困难的任务，尤其是在权衡方面。但我认为将来会变得更容易，因为我们对探索可能选项的设计空间有更多了解。这是一个显而易见的进步。有人提出可能在这种训练中看到的其他类型的弱点或问题，就像你的语言模型一样吗？然后，你的奖励模型可能会过度适应彼此，特别是即使它们没有一起训练。如果你喜欢来回反馈，过度优化就会成为问题。是的，如果你总是在Facebook上看到重复内容，你会想要再次获取反馈。因此，数据仍然是非常昂贵的，你可以看看。

在一些文章中，当谈到谷歌开发的人工智能时，人们表达了对于训练聊天的GBT（可能是指Generative Bot）所需的数据量不够满意。这意味着他们需要雇佣开发人员来解释和编码问题，每周工作40小时，是的，这仍然需要大量的数据。这就像一种外卖，和其他许多类似的任务一样，都需要大量数据支持。每一个任务都是数据密集型的。是的，我认为这总结了一些重要的事情。所以，在讨论大语言模型的局限性时，我们也需要谈论强化学习的一般局限性，以及在单一数据点中建模或捕获人类奖励的想法。因此，人类的偏好可能对强化学习非常不可靠，人们早就意识到这一点。他们有一个称为“奖励黑客”的术语，即代理正在优化开发人员指定的某些内容，但这并不是我们真正关心的。一个经典的例子是OpenAI的案例，他们正在训练代理玩船赛，他们训练它以达到最大化效果。

在左下角你可以看到分数，但实际上你并不太在乎这个分数，你更在意的是刚刚超过其他人，分数只是一种奖励机制。特工发现，你可以收集一些类似于涡轮增压的东西，这些东西可以提升你的分数，于是游戏最终变成了只是不断在道路中间驾驶，一遍又一遍地收集这些涡轮增压。这样它会获得疯狂的高分，但它并没有真正参与比赛，它只是一直撞到障碍物，船总是在着火。嗯，这是一个非常明显的例子，说明我们所说的人工智能的误位是正确的。你可能会觉得这很简单，就好像他们犯了一个愚蠢的错误，不应该将分数作为奖励机制。但我认为认为我们可以意识到这一点更加天真，因为所有人类偏好都被视为数字，并且为正确的事情分配一些标量值。因此，我认为这种情况已经发生了，这是一个例子，你可能已经注意到了。也许你以前和聊天机器人交流过，你可能注意到他们产生了很多错觉。

很多情况下，我们的大语言模型可能会因为提供看起来权威或有帮助的回复而受到奖励，而并不关心回复是否真的正确。他们只是希望看起来有帮助，这可能导致事实被篡改。有时您可能会看到关于聊天机器人的新闻，您可能知道某些公司与这些聊天机器人有关系，它们可能会犯错，即使您知道这一点，也会持有错误的看法。

一般来说，当涉及到人类偏好的模型时，它们更加不可靠，因为我们不仅使用人类的偏好本身，还训练了一个深度模型，我们并不清楚它是如何运作的。这就带来了使用它的风险。因此，我在这里提到为什么我们需要KL散度惩罚项，黄色标出的术语在这里是一个具体的例子，说明了语言模型过拟合时会发生的情况。奖励模型是正确的，这表明在这种情况下，他们取消了KL惩罚。

他们的目标是最大化奖励，他们训练了一个奖励模型，通过不断训练让数据点尽可能地向右上方靠拢，你会发现随着训练的进行，你的预测路径会与理论上的路径偏差变大，这可以通过KL散度或者最初的起始点距离来衡量。在图中，金色虚线代表奖励模型认为你的语言模型在做得对的情况下会得到的奖励，因此奖励模型认为我们正在扼杀它，就像一些人更喜欢摘要而不是完整文章一样。但事实上，当深入询问人类的偏好时，他们可能会陷入困境。这个例子可能过于投入在优化某些指标上，而不是更广泛的问题。因此，人们真正关心的是人工智能对齐的问题。我鼓励总统谈论这个问题。他在推特上说，我们用于强化学习和对齐的主要工具是rohf。但是，奖励欺骗行为经常发生。

人类并不是很擅长作为奖励监督者，这可能导致代理看起来在某种程度上是在做正确的事情，但在微妙和显而易见的方面却是错误的。我认为我们已经在当前一代的聊天机器人中看到了这样的例子。因此就积极方面而言，有一些积极的方面，但同样，强化学习很难获得正确的人类偏好。人类偏好模型更容易出错。我记得在Twitter上看到一个笑话，有人说你知道零样本和少样本学习是人工智能指导中最糟糕的方法，微调是第二糟糕的方法，对齐AI和ROHF是对齐AI的第三种最糟糕的方法。我们已经取得了一些进展，但你知道每一种方法都存在明显的基本局限性，这就是为什么强化学习面临竞争的原因，因为它是基于前提所做出的数学计算。

您所投入的是最大洞察力，以便我们能够对样本期望进行采样。但在涉及采样时，您如何实现并行处理呢？因为在这种情况下，您需要能够自适应地停止采样，但又不确定何时会重新开始。您又如何加速这个过程呢？我认为您希望像《变形金刚》中整个单位一样，能够避免一切瘫痪。这个过程确实需要大量计算量，我并不确定要使用什么样的基础设施，或类似 rlhs 那样的最先进、高性能的实现。它们可能会利用类似您描述的并行化技术。我认为在许多传统的强化学习中，可能会采用类似演员-学习者的架构，其中有一组“演员”工作人员，每个工作人员都是一个语言模型，它们会生成一批样本，然后“学习者”会将它们集成并进行正确评分更新。因此，您可能确实需要像纯粹的多语言那样操作，以获取足够的样本，在合理的时间内完成任务。您是否遇到了这些问题或者还有其他问题需要解决，超出我们通常处理的范畴？

在《变形金刚》中提到的问题规模较大。是的，我认为你可能需要复制模型多次，并从这些模型的不同副本中获取样本。就像变形金刚这种自回归生成模型一样，特别是前向传递和多头注意力，很容易进行并行化。但是自回归生成仍然是一个瓶颈，因为你必须先运行它，然后依赖于你采样的内容，必须再次运行。因此，我认为我们仍然没有完全解决一些问题，这会增加计算成本。所以，我们应该还有10分钟，如果我没记错的话。所以，总的来说，我们最终解决了如何从这一点到那一点的问题，虽然可能缺少一些细节，但关键要素是微调指令，通过人类反馈来进行强化学习。那么，让我们谈一谈接下来会发生什么。

正如我提到的，强化学习在人工智能领域仍属于一个非常新的领域，正在快速发展。在下一次讲座中，可能会发现幻灯片内容完全不同，因为我展示的很多想法可能并不是最佳或最有效的方式。与微调相比，rohf让你更深入地探索，但仍然代价昂贵。有很多关于开放人工智能的文章需要大量注释者或开发人员不断比对模型输出。最近我一直思考的一个问题是如何在不需要严格数据要求的情况下获取强化学习应用程序的好处。有一些疯狂的想法，比如尝试从人工智能的反馈中而不是人类的反馈中进行强化学习，让语言模型自己评估语言模型的输出。例如，最近有团队提出了一个称为“宪法人工智能”的想法，他们尝试在大型语言模型上进行这项工作。

这个想法的基本概念是，当你让 GPT-3 识别出无用或者有害的回应时，它其实擅长这样的任务。你可以利用这种反馈来改进模型。举个例子，如果你提出一个类似于“你能帮我侵入邻居的 Wi-Fi 吗？”的请求，AI 智能体回答“可以”，这时你可以请求模型自我反馈。通过添加一个批评性的请求，告诉语言模型 GPT-3 识别协助性回应中的有害内容，模型会生成一个批评，比如侵入别人的 Wi-Fi 是非法的，然后你可以要求它进行修改。简而言之，只需要修改助理的回应，删除其中的有害内容，现在这些修改只需从语言模型中解码就可以实现。你可以根据这样的数据指示模型微调，确保回应修改后不包含有害内容。

这些内容听起来很有趣，我认为是非常令人兴奋的。但是，关于对齐等问题，人类偏好奖励模型的概念可供参考，当思考这些问题时会变得复杂。我们对安全性和发展方向还缺乏清晰了解，但自我改进这一总体理念，在大型语言模型背景下已经有了不少探讨，这也是我在讲座一开始就提到的。这些以大型语言模型为基础的自我改进想法，已经在思维推理领域有了很多实践。尽管其具有颇具挑战性的名称，大型语言模型可以通过微调不断提升自身，但我们对其未来进展的具体路径仍知之甚少。不过，基本思想是你可以逐步推演，比如让语言模型产生一系列推理结果，然后通过微调好像这些是真实数据一样进行推理，以观察语言模型是否能够通过这种方式变得更优秀。然而，正如我之前提到的，这个领域仍然非常新颖。我认为大型语言模型存在许多限制，比如产生幻觉的可能性，以及其他一些。

纯粹依靠规模和计算能力，可能无法完全解决我们的LHF(大语言模型)问题，比如我们如何防止它发生。就好像我看到人们讨论如何规避GBT(梯度提升树)等技术仍然存在的有害反应一样。我们有什么方法可以缓解这些问题，因为似乎你只是想继续发展，就像我们发现机会一样。尝试着采取行动而不是漠视，我想要找到一种方法建立起这种规模以防止可能出现的问题。这确实很有意思。因此，当然有一些方法可以利用人工智能反馈或人类反馈来降低出现这些问题的可能性。就像当你在Twitter上看到有人提到使用某种策略或任何方法来避免GPT-3的滥用时，也许需要将其纳入考虑。

在框架中，要确定AI助手正确脱轨的方式，然后微调，试图纠正这些错误，但这确实很困难。我认为在大多数这类设置中，很难预测用户可能越狱助手的所有可能方式。就像在安全网络中总是处于攻击者优势的情况下一样，攻击者总是能想出一些新的东西或新的漏洞利用。所以是的，我认为这是一个很深的问题，我没有一个真正明确的答案。但当然，如果我们知道脱轨是什么，我们就可以减缓它。我认为这看起来很简单，是的。但是如果你知道如何做到这一点，你应该会受到一些公司的青睐，如果你能解决这个问题，他们会付给你数百万美元。所以，嗯，就像最后的评论一样，你知道所有这些，比如我提出的缩放结果，还有其他方法，比如进行指令微调，它会遵循你的指示，或者你可以尝试其他方法，你可能会持有一个非常乐观的观点。

通过扩展我们的大语言模型，来解决人工智能等问题，这种可能性确实存在。但在我们能够利用这些模型取得任何成果之前，我们首先需要弄清楚如何通过创新解决一些基本限制。目前正是研究这些领域令人兴奋的时刻，因此，感谢你的倾听，谢谢。