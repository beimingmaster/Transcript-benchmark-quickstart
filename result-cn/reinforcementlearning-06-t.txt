作业二现在已经发布了。我意识到，人们对深度学习的了解程度各不相同，有些人可能已经熟悉了解，参与过相关课程，或者广泛使用过深度学习技术。这个背景知识的广泛程度是不同的。

在本周末的课程中，我们将深入了解深度学习的背景知识。参与者不需要是深度学习专家，只需具备一些基本技能，就可以完成作业二，即使用函数逼近深度神经网络。

因此，如果你缺乏这方面的背景知识，我鼓励你参加本周的会议。今天我们会讨论一些关于深度学习的内容，但会涉及到非常基础的内容，而且更多地会聚焦于深度强化学习。

这些会议将是了解这些材料的好机会。我们也将在明天结束之前发布更多信息。关于这门课程的默认项目，你们可以选择是否要自行开展项目，或者选择其他方式。

在这里有一个项目的提醒。提案即将截止，还有一周多一点的时间。有没有其他问题要问的？是的。这些作业，是否只适用于TensorFlow？是在询问作业是否只适用于TensorFlow。我很确定一切都基于你使用TensorFlow。是的。请随时在Piazza上检查，但我肯定任何Oliver自动评分器都是为TensorFlow而设的。所以，即使您使用PyTorch，请以某种方式使用TensorFlow。我相信您也应该能获得Azure积分。如果您对不使用Piazza进行设置有疑问，请随时在Piazza频道里提问。上周我们发布了有关如何设置机器的教程。所以，如果有任何疑问，这是一个很好的起点。您可以查看教程，观看视频，或者在Piazza上联系我们。还有其他问题吗？好的。那么，我们可以开始了。今天我们要介绍的是关于深度学习。

在课堂上，我们一直在研究在没有提前了解奖励模型的动态情况下，如何做出决策。上周我们讨论了价值函数的近似，特别是线性价值函数的近似。今天我们将开始讨论其他形式的价值函数近似，特别是使用深度神经网络。为什么我们要这样做呢？因为我们希望能够处理高维的输入信号或观察结果，比如图像，以及复杂的客户、患者或学生信息，这意味着我们可能会有非常庞大的状态空间。虽然我们很少讨论庞大的动作空间，但我们需要考虑处理庞大的状态空间。

因此，当谈论这些概念时，我认为我们需要考虑模型的表示方式。这意味着我们可以使用动态模型或奖励模型，比如 T、T'、R，状态行动值 Q 或 V，甚至我们的决策策略，能够覆盖国家和行动之间的关系。我们的想法是，实际上我们可能永远不会再次遇到完全相同的状态。在现实世界中，你可能永远不会再次看到完全相同的情形，但我们希望能从历史经验中进行总结。因此，我们考虑，不再使用表格来表示值函数，而是采用通用函数逼近器，其中我们有一组参数 W。当我们考虑采用这种方法时，我们关注的是可微分的函数近似。可微分函数的好处在于我们可以利用数据来估计参数，然后使用梯度下降来尝试拟合函数，使其能够代表我们的 Q 函数或值函数。通过上次提到的方式，我们能够更有效地利用我们的数据和提高我们的系统性能。

在尝试拟合函数的适应度时，通常考虑使用均方误差来比较与真实值的差异。因此，我们可以定义损失函数j，然后使用梯度下降来寻找最优化的参数w。需要提醒一点，随机梯度下降非常有用，因为我们可以在逐步获取更多信息时逐步更新参数。这些信息可能以片段形式（噪音）或单个元组的形式（噪音）出现。提到元组时，通常指的是状态、动作和奖励的组合。另一个好处是，预期的随机梯度下降与完整的梯度更新是一样的。所以，回想一下我们之前讨论过的线性值函数近似。这意味着我们需要有一组特征来描述我们的环境，然后我们把这些特征输入到我们的状态中。这些状态反映了真实世界的一部分，然后我们根据这些特征输出结果。比如，这可能是我们机器人的激光传感器，可以告诉我们在所有方向上墙壁的距离。

我们讨论了一个事实，即每个世界都有多个别名版本，因为多个走廊可能看起来很相似。我们的价值函数现在是这些特征之间的点积，通过权重我们已经获取了关于世界的信息。我们的目标函数是均方误差，然后我们可以进行相同的权重更新。关键的困难在于我们不知道这是什么。这才是政策的真正价值。问题在于我们不知道政策的真正价值是什么，否则我们就不需要进行所有这些学习。因此，我们需要不同的方法来近似它。我们上次讨论的两种方法受到蒙特卡洛和TD学习的启发，我们可以插入完整集的回报值，即她的话的总和，或者我们可以投入自举回报。因此，我们正在进行引导。我们需要查看奖励的地方，

在这种情况下，我们使用线性值函数逼近器来计算下一个状态以及其对应的价值。这种方法让我们能够以简单的形式得到函数相对于权重 W 的导数。简单来说，这就是我们特征的乘积与预测误差。有时人们称之为预测错误，因为这是值之间的差异，或者我们用 GT 作为真实值。当然，实际上这只是价值的采样值，但是，嗯，这是真实价值和我们估计价值之间的差异。我们希望尽量减小这个差距。在这种情况下，我写下了这些方程，它们是用于线性值函数近似的。然而，线性值函数近似有一些局限性，即使它可能是最广泛研究的方法之一。因此，如果您拥有正确的特征集，并且之前有很多工作确定了这些正确的特征集是什么，它们通常能很好地发挥作用。实际上，当我们进入时，我之前提到过。

深度神经网络是一种复杂的特征提取方法，最后一层则是这些特征的线性组合。 在深度强化学习中，深度神经网络通常代表 Q 函数。 这是我们要考虑的主要类型。 因此，如果你有正确的特征集，线性值函数通常是非常有效的，但挑战在于找到什么是正确的特征集。 对于我们能否使用特征集来准确编写值函数以及容易收敛到这一点，有各种影响。 因此，一个可替代的方法是使用非常强大的函数逼近器。 在不需要的地方，必须使用直接表示特征。 一些方法是基于核的。你有没有听说过基于核的方法或k最近邻类型的方法？ 如果你在机器学习领域，应该听说过k最近邻，这些都是非参数方法，表示的规模通常随着数据点数量的增加而增长。

然后这些模型效果非常好，并具有一些很好的强化学习收敛特性。 问题在于，随着维度的增加，所需的数据点数量往往会增加。 换句话说，如果有180个特征，就需要将180维空间铺平，这往往会导致指数级增长。 因此，无论是在计算复杂性、内存需求还是样本数据复杂性方面，这都不太具吸引力。 相比线性值函数逼近器，这些模型实际上具有更强的收敛性能，但它们尚未被广泛应用。 每个人都在想办法解决这个问题，请告诉我你的名字以便我记住你。 对于为何会出现指数级增长，你能再重复一遍吗？ 是的，学生的问题是为何会出现指数级增长以及许多基于内核的逼近器或非参数模型。 直觉上，如果你想准确表示你的价值函数，嗯，你可能需要使用更多的数据点。

在表示一个点时，可以使用其周围的局部点，比如使用k最近邻方法。这意味着你需要让所有东西都接近的点的数量与维数成比例，类似于在ε球中。基本上，这相当于将空间进行网格化。如果你想让一条线上的所有点都接近，你可以在这里放一个点，在那里放一个点，以便让线上的所有点都有一个在ε距离内的邻居。如果你想将点放在一个正方形中，你需要四个点，这样所有的点都能够接近其中一个点。一般来说，随着维度的增加，你需要的点的数量会呈指数级增长。

尽管点的数量会急剧增长，但它们确实非常有用，因为它们可以确保我们之前讨论过的“平均者”，即查看线性值函数逼近器。当进行备份操作时，平均者不再一定是一个收缩操作符，这就是为什么在进行越来越多的备份时有时会出现崩溃的原因。关于平均者有一件非常酷的事情就是他们的名字。

当你使用这种类型的逼近器时，不会有扩展性的问题，这意味着当结合贝尔曼备份时，仍然能保证是一个收缩，这个性质真的很赞。因此，与许多其他逼近器相比，这些逼近器能够保证收敛。尽管在实践中很少见到它们，但我的哈佛同事Finale Doshi-Velez正在研究将它们应用于医疗保健等领域，以帮助进行患者归纳。所以虽然它们可能很有用，但通常无法很好地扩展。因此，今天我们要讨论的是深度神经网络，它们也具有非常灵活的表示，但我们希望它们能够更好地扩展。一般来说，在接下来的时间里，我们几乎没有理论上的保证，但在实践中它们通常表现非常出色，因此深度神经网络变成了强化学习和其他机器学习领域非常有用的工具。那么，深度神经网络是什么意思呢？在这种情况下，通常意味着我们想要构建一个由多个函数组成的函数逼近器。

我们会接收一个输入 x，并将其传入某个函数，这个函数会有一些权重。通常情况下，所有这些东西都可以被表示为向量。你会接收一些权重，将它们与输入 x 结合，然后将它们送入某个函数，最终会输出另一个可能也是向量的结果。然后这个结果会被送入另一个函数，加入更多权重。这个过程会重复多次，最终你会得到一个输出 y，类似于我们的 Q。然后，我们可以将其输入到某个损失函数 j。那么这意味着什么呢？这就意味着 y 等于隐藏神经元 hn 减去输入 x 的第一个神经元 h1。虽然我没有列出所有权重，但实际上会有一大堆权重，然后这就像以往一样，用到了损失函数。

您可以把这些看作是我们的 Q。然而，在无监督学习中经常会出现这样的情况，例如预测某个物体是否是猫，或者识别特定物体的图像，或者用于回归分析。那么，我们为什么要这样做呢？首先要明确的是，当您组合多个函数时，通过加法、减法和多项式，可以表示非常复杂的函数，以及通过组合函数所能完成的各种任务，这种表示功能非常强大。而编写这样的理由之一是，您可以利用链式法则尝试进行随机梯度下降。那么，这是如何运作的呢？这意味着我们可以写出那个 dj。所以，我们确实想要，您知道， dj 具有所有这些不同的参数。因此，我们可以写出 hn 的 dj 和 dwn 的 dhn，在任何地方都可以这样做。因此，h2 的 dj、

通过链式法则，您可以计算出h2关于dh的导数以及dw2关于dh2的导数。这意味着您可以沿着整个链一路传播，从损失函数相对于权重w的梯度开始，将信号传播回来更新所有权重。在学习深度神经网络时，需要手动完成这些操作，即反向传播。过去5到8年的重大创新之一是自动微分的出现，使得您无需手动推导出所有梯度，只需编写网络参数即可。

大量的参数需要调整，而您可以使用像 TensorFlow 这样的软件来帮助您处理所有这些细节。这使得这类工具变得更加实用，很多人使用深度神经网络，因为您可以建立非常复杂的网络，有多个网络层，并且无需手动计算梯度。这些 h 函数实际上是什么？通常来说，它们是线性和非线性变换的组合。它们必须是可微的，因为如果我们要使用梯度下降来训练它们，那么 h 必须是可微的。因此，通常的选择是要么是线性的，例如 hn = hn-1，要么是非线性的，我们可以认为 hn = some_function(hn-1)。如果是非线性的，我们通常称之为激活函数。由于时间有限，在课堂上我不会深入讨论神经网络的细节联系。

我们大脑内部的运作方式是令人鼓舞的，其中涉及人工神经网络。在大脑内部，人们认为存在某种非线性激活函数，即当信号通过某个阈值时，神经元就会发放电信号。因此，这种非线性激活函数可以是 sigmoid 函数或 ReLU 函数等。ReLU 函数尤其受欢迎。因此，我们可以选择不同的线性或非线性函数组合，然后我们需要一个损失函数。通常，我们使用均方误差作为损失函数，也可以选择对数似然。但我们需要一些方式来衡量我们实现目标的程度，以便更新权重。哎呀，抱歉打断一下，那么，这个 ReLU 函数不可微的，对吗？其实是可微的，就像，你可以将它拆分成可微的部分，最近它比 sigmoid 函数更受欢迎，尽管我认为它们有交叉之处。它在某些点上是不可微的吗？是的。

我理解你的困惑，当神经网络的某些部分变得平坦时，梯度会如何影响。如果是平坦的部分，梯度会趋近于零，这就导致最终整体梯度也会变为零。这就是我所说的。问题在于实现ReLU激活函数时，有很多地方是平坦的。如果梯度消失，也就意味着信息流失。一般情况下，在课堂上我们可能不会深入讨论这个问题，但随着神经网络层数的增加，这个问题就变得尤为重要。有时候，可能会发现早期层几乎没有信号传回。虽然我目前不打算详细讨论这些内容，但会在未来的会议中一起探讨。了解这一点很重要，我们也很乐意给出一些建议。尽管如此，即使是在平坦区域，依然可以拥有非零梯度。为什么要这样做呢？因为这样可以使用更复杂的表示形式。另一个原因是，如果至少有一个隐藏层…

嗯，如果你有足够数量的节点，可以将其视为一种足够复杂的特征和功能组合。节点是通用函数逼近器，可以用深度神经网络表示任何函数，这点非常重要。使用足够表达力的函数逼近器，就不会有容量问题。例如，如果使用线性值函数逼近器，有时特征可能太有限而无法准确表达值函数状态。通用函数逼近器的特性在于，如果深度神经网络足够丰富，就不会出现这种情况。

当然，您也可以尝试构建一个具有非常丰富特征的线性值函数逼近器，使其等效。因此，另一个好处是相比于浅层网络，您可能可以使用指数级更少的节点或参数，这意味着不需要那么多的组合。

嗯，让我们用更流畅的语言来表达相同的概念。我很高兴能够在线下和您讨论这个话题，或者我们也可以选择在广场上交谈。最后想提的一点是，您可以使用随机梯度下降来学习参数。深度神经网络可以在五秒钟内几乎完成训练。接下来，我们要讨论的是卷积神经网络。这里我将进行一个简要的介绍，因为对于完成作业而言，您无需深入了解细节，只需了解卷积神经网络是一种高度表现力的函数逼近器。那么为什么我们要关心卷积神经网络呢？它们在计算机视觉中被广泛应用，尤其当我们对机器人和其他类型的智能体在现实世界中进行交互感兴趣时。在这种情况下，视觉是我们最主要的感官之一。因此，我们可能希望在我们的人工智能智能体或机器人中使用类似类型的输入。假设您有一张爱因斯坦的图像，其中包含许多不同的像素。

假设你有一个 1,000 x 1,000（百万像素）的图像。因此，x 和 y 都是 1,000 x 1,000。这意味着你有 10 的 6 次方像素。因此，这通常被称为前馈深度神经网络。在这种情况下，所有这些像素将作为下一层的输入。你可能会希望有许多不同的节点从这些像素中接收输入，这样你就会有很多权重。因此，你可能会有 10 的 6 次方个权重，我们假设有很多并行功能的神经网络。因此，输入不仅仅是一行，而是可能有 x 进入 h1，h2，h3，h4，然后以某种复杂的方式进入其他函数。这样，你可以并行计算多个函数。所以，你可以想象对于你的图像数据，你会有很多输入。

一个函数计算图像的某些特征，另一个函数计算图像的其他方面，然后以各种复杂的方式将它们组合在一起。 这就意味着对于第一个函数，可能会有一大堆n个不同的函数来计算图像，涉及到10的6次方个参数。因此，如果我们将这些权重和x相乘，将涉及到10的6次方个参数来处理所有的x。如果我们想要同时处理不同类型的权重，将会面对非常庞大的参数数量，尽管现在我们有很多数据，但仍然需要处理海量参数。这可能会忽略我们在视觉处理中常用的一些技巧。如果我们考虑为多个隐藏单元重复执行此操作，我们将需要大量的参数。为了避免这种时空复杂性，以及忽略图像结构的情况，卷积神经网络尝试通过一种特殊形式的深度神经网络来处理图像的特性。

因此，特别地，图像通常具有结构，就像我们的大脑认为图像也具有空间和频率上的独特特征一样。在卷积神经网络中，会涉及特定类型的运算符，类似于我们之前提到的函数h1和hn，这些函数可以是线性的或非线性的。卷积神经网络学习这些函数的特定结构，以便尝试提取从图像中可能想要获取的属性。其中关键的一点是进行大量的权重共享，以减少参数数量。因此，不是每个参数都包含所有像素，而是最终会得到一些相同的局部参数，然后将它们应用到图像的不同部分来提取特征。这样的处理旨在尝试提取我们认为对图像分类和理解是重要的特征。

预测诸如面部是否包含在图像中等任务中，寻找有效特征是非常重要的，同样对于理解 Q 函数也是如此。关键思想之一是采用一个过滤器或感受野的概念，即使用一些隐藏单元来处理先前输入的函数。这意味着我们只考虑图像的一个子集，而不是整个图像，从而只需处理图像的一部分。我们可以将其比喩为打补丁，选择占据图像的某些部分，例如左上角或中心，来计算特定区域的属性。这个过程通常被称为过滤器，在整个图像上应用一组权重，步幅则表示在每次移动多少个像素的小补丁上进行操作。此外，还有一种叫做零填充的技术，它用于在每个输入层周围添加零，以帮助确定输出结果。

因此，在这种情况下，如果您有一个 28 x 28 的输入，并且有一个 5 x 5 的小块，您将在整个图像上滑动。那么最终将得到一个 24 x 24 层，因为基本上您只需要把它移动一点。每次移动，您都会取这 25 个。这就是一个 5 x 5 的区域，你将有 25 个输入 x，并且要用一些权重对它们进行点积，这会给你一个输出。所以，在这种情况下，我们需要 25 个权重。有一点需要注意的是，我们不是将整个 x 输入传递，而是将 x 输入的不同部分传递到不同的神经元，您可以将它们视为不同功能。另一个重要的概念是我们将为所有部分设置相同的权重。因此，在使用这些权重时，您可以将它们看做是在尝试从图像的子块中提取特征。例如，可能会有某些优势或者检测到某些特征。

这部分图像是否有类似水平边缘的特征？我试过调整权重，将这部分特征应用到整个图像上，看看是否存在这样的特征。所以现在所有权重都相同，只需要在整个图像上移动它们。我可能只使用了 25 个权重，而不是之前的 10 到 6 个，然后将它们应用于图像的不同部分。在输入后进入隐藏层时，是的，我也对图像进行了下采样。为什么要这样做呢？我们认为大脑经常会这样操作，试图获取各种不同类型的特征。在深度学习出现之前，很多计算机视觉工作试图构建这些特殊类型的特征，比如边缘特征等，认为这些特征能够捕捉图像的重要属性，同时也能对翻译等任务保持稳定。因为我们相信，无论是在固定视线的情况下，还是稍微转动头部后，所看到的世界都是差不多的。

我通常看到的特征在我向左或向右移动时通常是相同的。世界上存在一些非常显著的方面，与检测面孔的存在以及确定我的价值函数相关。因此，我们希望提取一些能够代表这种变化的特征。这也意味着不仅仅是计算，您可以做到这一点。您将在整个特征集上应用相同的权重，在整个图像上进行操作，然后可以对多种不同类型的特征执行此操作。因此，关于这个问题有一个很好的讨论，从231页开始更深入，你们中的一些人可能已经接触过。有一个很好的动画，它展示了，假设您有输入，可以将其视为图像，然后可以应用不同的滤波器，尝试检测不同的特征，然后将它们移动到图像周围，看看该特征是否存在于任何地方。因此，您可以使用多种不同类型的滤波器来实现这一点。您可以将其视为尝试寻找某物是否是这样的，或者某物是水平的还是垂直的，具有不同类型的边缘。

这些内容实际上描述了提取不同特征的方法。在卷积神经网络（CNN）中，另一个非常重要的部分就是池化层。这些层通常被用来对图像进行下采样，可以通过最大池化等操作来检测特定特征的存在，也可以使用平均值等方式来压缩信息。在许多情况下，我们会从一个高维输入开始，比如图像，然后经过处理最终得到低维输出，比如标量值（如Q值）。因此，通常最后一层是全连接层。这个过程可以被视为计算出新的特征表示，从高维到低维的转换。通过这样的方式，我们可以计算出图像的新特征表示，最后通过一些全连接层进行处理。

就像进行线性回归一样，我们可以使用它来输出预测或标量。再说一遍，我知道对于一些人来说，这只是一个快速的浅层回顾。对于其他人来说，显然不是，这将是一个快速的介绍，但我们不要求您了解所有这些细节。强调再次，只要参加会议即可，如果您有任何疑问，请随时与我们联系。好的。因此，这些类型的表达（如深度神经网络和卷积神经网络）在深度强化学习中被广泛使用。大约在2014年左右，在研讨会上，David Silver开始谈论如何在Atari中使用这些近似值。那么为什么会令人惊讶呢？只是我在这里闲逛。对我个人而言，在1994年左右，我们就开始使用了带有深度神经网络的TD双陆棋。他们使用了神经网络。我记得有一位深入研究过这个领域的人，我的想法就像是世界级的西洋双陆棋玩家一样。所以，这发生在很早的时候。

然后在大约1995年到1998年左右，我们得出了这样的结果，即使用函数逼近结合离线策略控制并加上自举可能会导致问题，可能无法收敛。因此，我们之前讨论过这个问题。一般来说，一旦我们开始使用线性函数逼近器进行函数逼近，在结合策略控制和引导时，这意味着我们正在进行TD学习或Q学习。在函数逼近器中，这样做可能会导致出现具有挑战性的三元组，这通常意味着我们无法保证收敛。即使我们能够保证收敛，最终的解决方案可能也不是理想的。因此，尽管早期取得了一些成功，但在上世纪90年代中期出现了这些结果，我们试图更好地理解这一点，这表明情况可能非常复杂，其中风险除了理论结果之外，还存在其他挑战。

简单的测试用例很容易出问题。这并不仅仅是理论上的可能，实际上已经发生过失败的案例。因此，在相当长的一段时间里，社区曾一度远离深度神经网络。人们对使用它们非常谨慎，因为即使在简单情况下，函数逼近也有可能变得非常糟糕。从理论上讲，人们可以证明这种糟糕情况可能会发生，因此在相当长的一段时间里人们对它的关注较少。然后，在2000年代中期，深度神经网络开始兴起，并持续至今。因此，深度神经网络变得非常庞大，在视觉等领域取得了巨大成功，在其他领域中，也见证了大量数据和计算资源的应用。我们取得了非常显著的成果。因此，也许很自然的是，在大约2014年，DeepMind将这些成果与Atari结合起来，取得了一些真正惊人的成功。我认为它确实带来了改变。

人们如何使用复杂函数逼近、仪器和强化学习来理解故事，有时可能无法收敛。虽然事情可能出现问题，实践中情况确实有时会变得非常糟糕，但尽管如此，我们并不总是完全理解它们为什么总是有效，实际上在很多时候我们都能够推出不错的策略。然而，我们经常不确定这些策略是否是最佳的。通常我们知道它们不是最优的，因为我们意识到人们仍有提升的空间，但这并不意味着这些策略就一定不好。因此，我们看到人们对深度强化学习再次产生兴趣。

深度学习是否已经解决了在90年代中期出现的问题，或许只是通过增加计算能力和数据收集能力来应对失败并尝试不同方法，类似于重复组合尝试直到成功。这些问题是否真的已经被解决，仍是一个需要考虑的问题。

我们是在克服困难还是仅仅暂时解决了问题呢？问题在于，我们如何能够根本解决我在90年代末遇到的一些问题，或者说我们是在用粗暴的方法去解决它。我认为在1995年到1998年期间，收敛方面出现了一些问题，现在有一些更真实的随机梯度算法，第11章介绍了这些算法，这些算法能够保证收敛。但它们可能无法确保收敛到最优策略，因此仍然需要大量的工作来理解函数逼近器、策略控制和引导。我认为在今后的讲座中，我们将看到一些算法思想，它们有助于避免一些收敛问题。人们在2013年和2014年就已经意识到这一点，因此他们试图思考，“这些问题会在什么时候出现，我们该如何避免这些情况呢？”究竟是什么原因导致了这些情况呢？至少在算法上，我们可以尝试解决一些人们经常谈论的稳定性问题，以确保深度神经网络似乎不会开始拥有趋向于无穷大的权重。

在经验上，拥有更稳定性能是很重要的。是的，这对我来说也很重要。那么，具体到雅达利案例，您是否避免了这个问题呢？如果您尝试通过政策控制来解决呢？我只是不确定。实际上，在您进行的深度神经网络实验中，他们并没有更新性能以符合苹果公司的政策。问题是，如果我理解正确的话，在雅达利的案例中，他们在政策上做了更多的改动，或许我们能够实现更加稳定。他们正在使用深度学习，具体来说是Deep-Q Learning。因此它可能仍然非常不稳定，但他们会采取一些措施来调整网络更新的频率，以使其更加稳定。这是一个很重要的问题。我们将在这里探讨它是如何运作的。还有谁？好的，太棒了。所以，我们很快将看到一个突破性的案例，看看他们所做的工作。现在我们再次谈到使用深度神经网络来表示价值函数。下周我们会讨论使用深度神经网络来表示策略。

那么我们要做什么呢？我们将再次关注我们的体重。我们将使用相同的逼近器。现在，我们将应用深度神经网络。在这种情况下，我们要使用 Q 函数，因为我们想要进行控制。所以，在这种情况下，我们要进行控制。所以，我们需要学习行动的价值。需要澄清的是，雅达利游戏通常并不具有真正的高维动作空间，它是离散的。通常在 4 到 18 之间，具体取决于游戏。它的维度相当低，相当小。因此，虽然状态空间很大，因为它是像素图像，但动作空间非常小。

提醒一下，对于 Q 学习，我们看到的是，对于我们的权重，Q 学习是这样的。我们必须有我们函数的导数。这不一定是线性的，但我们更新权重的方式是通过 TD 备份这种方式，我们有这个目标。

好的，但是我们现在希望在下一个状态、动作和权重上取得最大化。需要注意的是，在这个等式中，所有右侧的权重都是相同的。因此，我们使用相同的权重来表示当前价值，并用相同的权重来估计未来价值和导数。我们可能会看到另一种替代方案。他们的想法是，我们可以使用这种类型的函数逼近器。这些深度函数逼近器是专为 Atari 游戏设计的。他们选择了 Atari 的一部分。我记得，至少我认为丹尼斯和大卫在很久以前共同创办了一家电子游戏公司。如果我没记错的话，那是在大卫回到研究生院之前。他们对此都很感兴趣，因为游戏通常对人们的学习有挑战。他们认为游戏是一种很好的智力展示方式，并且认为我们可以通过访问并发表一篇论文。我忘了确切时间，可能是在2011年或2013年，讨论了 Atari 游戏和模拟器对强化学习是一个有趣的挑战。

在这种情况下，系统会采用完整的图像来决定下一步动作，这些动作通常在4到18个之间。奖励可以是各种形式，一般来说我们会使用分数作为奖励机制。在这种情况下，系统会考虑特定的输入状态，之前我们讨论过这是否符合马尔可夫性质。在这些游戏中，速度是一个重要因素，所以系统不仅需要考虑当前图像，还需要查看之前的四个帧，以便捕捉速度和位置的变化，比如观察球等信息。有时候这还不足够，举个例子，比如雅达利游戏，不确定有多少人玩过雅达利游戏。

嗯，针对最后四幅图像可能还不够，或者对于某些游戏类型可能还不足，是的。微生物完全正确。就像《蒙特祖玛的复仇》一样，我们经常需要获得一把钥匙，然后你必须触及那把钥匙。它可能在屏幕上可见，也可能被存储在库存中的某处。因此，您必须记得您拥有它，这样您在以后做出正确的决定时就不会错过信息。否则，您可能会错过一些信息。因此，在很多游戏和任务中，甚至最后四张图也不能提供所需的信息。但这与逼真程度无关，它比呈现整个历史容易得多。所以他们从这里开始。在这种情况下，有80个操纵杆按钮的位置，在某些游戏中可能需要使用这些位置，也可能不需要。奖励可能是分数的变化。需要注意的是，这可能会很有帮助，也可能没有帮助，这取决于具体游戏。

因此，在一些游戏中，您可能需要花费非常非常长的时间才能到达可能会改变得分的地方。在这种情况下，您可能只会得到很少的奖励，而在其他情况下，您可能会获得很大的奖励。这样一来，学习应该做什么就会变得容易得多。他们在2015年的自然期刊上做了一些重要的事情，他们在所有游戏中都使用相同的架构和超参数。现在需要明确的是，他们为每个游戏学习了不同的Q函数和不同的策略。但他们的观点是，他们不必为每个游戏使用完全不同的架构，并且单独进行完全不同的超参数调整才能使其正常运行。这种通用的架构和设置足以让他们能够学会做出所有游戏正确的决策。我认为该论文的另一个很好的贡献是我们将尝试获得一种通用的算法和设置，远远超出我们在强化学习领域正常看到的三个示例。只需努力确保在所有50场比赛中取得好成绩即可。同样，每个智能体都将从头开始学习50场比赛，因为它们将使用相同的基本参数。

他们使用了相同的超参数和相同的神经网络来实现这一点，因此相同的函数逼近器会起作用。值得高兴的是，我认为这实际上是所需的自然结果。他们也发布了源代码，这样你就可以尝试一下这个模型。那么，他们是怎么做到的呢？他们使用了值函数逼近器，代表Q函数。他们通过最小化由随机梯度下降引起的均方损失来训练模型。但是，我们知道这可能与价值函数逼近器有所不同。其中的两个问题是什么呢？嗯，这些问题包括样本之间的相关性，这意味着如果有s、a、r、s素数、a素数、r素数、双素数。如果想象一下我们的价值函数或回报与S素数的价值函数和回报之间是否独立，答案是否定的。事实上，就像你可能期望的那样，它们与部分高度相关，这取决于S素数的概率。如果这是一个确定性系统，

它们之间唯一的区别就是R。 这意味着它们高度相关，当我们进行更新时，这不是独立同分布样本，存在很多相关性。此外，还存在非固定目标的问题。这是什么意思呢？就是当你尝试进行监督学习并训练你的价值函数预测器时，并不总是有相同的v pi预言机来告诉你真正的价值是什么。随着时间推移，这种情况会发生变化，因为你正在尝试用Q-learning来估计价值，并且你的策略会发生变化，因此存在大量的非平稳性。因此，当你试图适应你的功能时，你没有固定的目标，因为它可能在每一步中都在不断变化。所以你改变你的政策，你改变你的权重，接着你再次改变你的政策。因此，在融合这些方面，事情可能会变得非常困难，这也许并不奇怪。因此，DQN，深度Q学习的方法是通过经验重放和固定的Q目标来解决这些问题。经验重放，如果您听说过这个，或者之前了解过DQN，我们所做的就是不断地漫游数据。我们之前已经讨论过在TD学习中如何处理其标准方法。

仅使用数据点，我们实际上指的是SAR、S素数元组之一。在TD学习或Q学习的最简单方式中，您只使用一次数据点然后将其丢弃。这对于数据存储来说很有效，但对于性能来说不够理想。因此，我们提出了仅保留一部分有限的先前经验缓冲区，并基本上重新执行Q学习更新。请记住，这里的Q学习更新如下所示：我们会更新权重，这相当于获取元组并一次更新权重。这类似于一次随机梯度下降更新。因此，您可以从经验中抽样，然后在重放缓冲区中计算当前Q函数的目标值，接着进行随机梯度下降。请注意，因为您的Q函数会随着时间变化，每次更新同一数据点时，可能会有不同的目标值，因为Q函数已经针对该数据点发生了变化。这是很有用的，因为基本上意味着您可以重新利用这些数据点。

数据的重复使用是很有帮助的，即使我们在每一轮中更新权重，目标值也会发生变化。这意味着我们可以多次使用数据，这通常是非常有帮助的。虽然这个问题是关于是否在表示中保留更多帧，实际上并不是。保留更多帧会导致更复杂的状态表示，但你仍然可以在状态操作或单词之间仅使用一次，并且仍可以抛弃数据。比如，从状态s1执行操作a1得到奖励r1转移到状态s2，然后继续这个过程，每次更新当前位置，比如从s3执行a3得到r3转移到s4，你就会知道当前处于第四个状态。

就像我突然假装，哦等等， 我要假装我回到了状态s1，执行了动作a1，得到了奖励r1，然后转移到了状态s2，我需要再次更新我的权重。这次更新的原因与之前不同，因为我已经进行了第二次和第三次的更新。因此，我的Q函数总体来说会和之前不同。这将导致不同的权重更新，即使是相同的数据点，也会带来不同的权重更新。一般来说，我们之前讨论过的一个概念是，如果你使用TD学习来收敛，这意味着你会像永无止境地检查你的数据一样。至少在表格情况下，这相当于你已经学习了MDP模型，理解了奖励模型中的转移动态，然后刚好使用它进行MDP规划。这就是TD学习的收敛点，如果你无限次地重复查看数据，最终会收敛到好像你已经学习了建模、 动态模型、奖励模型，并对其进行了规划，这很酷。所以，这让我们更接近这个目标。但我们不想一直这样做，因为涉及到计算权衡，特别是在游戏中。

在计算和积累更多经验之间存在直接的权衡。 这个权衡非常有趣，因为你需要权衡是更多考虑、规划和使用旧数据，还是去收集更多经验。我们可以之后再深入讨论。请先提问并告诉我你的名字。经验丰富的重放缓冲区有一个固定的大小。请澄清一下，这些样本是否在固定时间后被新样本替换？或者有一种特定的方法来选择存储在缓冲区中的样本吗？那是一个好问题，这可能是一个固定大小的缓冲区。那么，你是如何选择其中内容的呢？是否使用最新的样本？另外，你如何添加和删除内容？这是个非常有趣的问题。不同的人可能有不同的做法。通常情况下，会选择最新的缓冲区，比如最后一百万个样本，这样可以重点关注我们将要讨论的样本数量。但你也可以做出不同的选择，同时还会涉及到哪些内容应该被排除。另外，这也取决于你的问题是否是不稳定的。

现实世界的变化如同客户群体的变迁一样不稳定，对吧？我正试图在新数据点的持续体验和重新标记之间取得适当的平衡。我们可以考虑类似剥削与探索的策略吗？嗯，本质上就像随机概率一样，只是在进行重新标记的决定时。问题是我们如何做出选择，比如获取新数据和重播多少数据等等，我们能否将其作为探索和利用之间的权衡。我觉得这个问题通常没有得到充分研究，但人们使用了许多不同的启发式方法。通常来说，人们会固定某种比例来更新根据经验重播和引入新样本的程度。所以，总的来说，目前确实是一种启发式的权衡。当然，可以尝试以最佳方式解决这个问题，但需要进行计算。这让我们涉及到了非常有趣的元计算和元认知问题。但是，如果你考虑到你的智能体正在思考如何优先考虑自身的计算资源，这是一个非常有趣的问题。这正是我们一直在努力解决的问题。好的。所以，DQN 做的第二件事情是，首先保留旧数据，其次是使用固定的 Q 目标。

这意味着什么呢？为了提高稳定性，这里所指的稳定性是指我们不希望权重爆炸并变得无穷大，这可能在线性值函数中发生。我们将修复多次更新中使用的目标权重。这里所说的目标计算是指奖励加上 S prime 的 Gamma V。这实际上是 w 的一个函数，我们将在多轮中修正我们在 S prime 中使用的值。因此，我们不会总是进行更新 - 无论最新的价值是什么，我们只是在一段时间内修正它，这基本上就像让它变得更加稳定。总的来说，这是对 V* 的近似。因此，你想让预言机在每次到达 S prime 时告诉你，或者当你采取行动进入 S prime 时，你希望预言机准确给出什么是真正的价值。如果没有这样做，它可能会在每一步中发生变化，因为你可能会更新权重。

这意味着我们不应该一直使用相同的权重来计算素数，而是要固定一段时间，可能是10步，可能是100步，来稳定目标，即尝试将损失降至最低。因此，我们仍然有一个网络，但我们会维护两组不同的权重。一组会用于减法操作，因为这些是旧的权重集，我们暂时不更新它们，它们是我们用来计算目标值的。所以，当我们需要计算素数的值时，我们会使用这些值，同时我们有另外一些权重用于更新。在计算目标值时，我们可以再次从经验重播缓冲区中的数据集中采样和经验元组，使用负值来计算目标值，然后使用随机梯度下降来更新网络权重。所以，这是与减法一起使用，同时也是与当前权重一起使用的。对吗？

所以，嗯，我想从直观上解释两个问题，第一个是为什么这样做有帮助，第二个是为什么这能使系统更稳定，然后，除了稳定性外，这样做还有其他好处吗？这就是两个问题。从直观上讲，为什么这对提高稳定性有帮助呢？嗯，这是一个很好的问题。另外，除了增强稳定性外，还有其他好处吗？那么，直观上来看，为什么这对提高稳定性有帮助呢？对于稳定性来说，这是很有帮助的，因为基本上可以减少目标中的噪音。如果回想一下蒙特卡罗，嗯，在那里并不像我们在使用GT作为引导目标那样使用目标。所以，在蒙特卡罗中，我们使用GT，我之前告诉过你它的好处是作为V函数的无偏估计。但是缺点是方差很大，因为你必须在回合结束前总结奖励。嗯，所以，如果在尝试对它们进行回归时情况是高方差的，它会更加嘈杂，嗯，你可能会受到梯度的影响。想象一下我们采取一个极端的措施，如果我们想要，嗯，为了稳定性，你总是可以让你的目标等于一个常数。比如，您可以始终让其等于零，如果您一直保持目标不变，

当您训练权重以最小化误差为常数函数时，权重将变得稳定。因为您始终具有相同的目标值，您一直在尝试预测，最终会得知将权重设置为零就足够了。这只是为了减少干扰，以达到我们所追求的目标。如果将这看作监督学习问题，我们有输入 x 和输出 y。在强化学习中的挑战是，我们的输出 y 是变化的。如果使得输出保持不变，那么拟合过程会更容易。除非有其他益处，我认为大部分情况下并不会有。这也会降低信息传播速度，因为您正在使用一组过时的权重来表示状态值。因此，您可能会误估某个状态的价值，因为您尚未使用新信息对其进行更新。对吧？假设我们想要一个近似器。深度神经网络有什么与众不同之处吗？这是个很好的问题，它是否特定于深度神经网络，或者我们可以与线性值函数近似或任何值函数一起使用？

这些概念都可以适用于任何值函数近似器。是的，这并没有具体指向某个东西。实际上，这只是关于稳定性问题，对体验重播而言也是如此。通过经验回放，信息可以更有效地传递，这能提升稳定性。所以，并不是深度神经网络的独特之处。他们更关注的是这些复杂函数逼近器的稳定性。是的，我同意。您是否每次都更新减号，或者是什么[听不清]。这是个好问题。那么，那个——迪——戴尔？ 甸。 甸。对于Dian提出的问题，我们确实更新过w减号。而且我们也可以定期更新w减号。因此，在一个固定的时间表中，比如每50集，或者每n集或每n步，您会进行一些类似于每次更新w减去dw的操作。对吧？我在考虑，鉴于我们知道这适用于梯度下降，并且您没有使用与梯度下降相同的结构，您正在使用创建...

不同的选项减去潜在值函数的梯度。这怎么可能呢，就像不是真正的梯度下降一样，所有这些假设都来自于这些Q学习。你的问题是，这在梯度下降方面真的有效吗？我的意思是，这是一个很好的问题，因为这些Q学习并不是真正的梯度下降方法。它们只是近似方法，考虑到这一点，它们通常表现得非常好。一些最近的算法，第11章有很好的讨论，比如GTD或梯度时间差分学习才更接近真正的梯度下降算法。这些还只是近似值，因此不能保证收敛。希望这可以帮到你，但我们不能作出保证。对吗？所以在实践中，人们是否有一些更新权重的循环模式以计算梯度？是的，他的问题是，实际上在实践中通常存在某种循环模式，即您更新w减去的频率。是的，通常存在特定的模式或者是一个超参数选择，决定您更新它的速度和频率。

嗯，这段内容似乎在讨论强化学习中的一些概念和技术。其中涉及到一些术语和概念的讨论，可能对于非专业领域的读者理解起来比较困难。我会对内容进行整理，使其更易于理解：

这段内容主要讨论了强化学习中的一些概念和算法。在进行信息传播时，需要权衡恶化和可能不太稳定的情况。当参数 n 的取值不同时，会影响到算法的表现，当 n=1时，类似于标准的 TD 学习；当 n 趋近无穷大时，则意味着不对参数进行更新。因此，存在一种中间的连续体来平衡这一过程。

在讨论参数的初始化时，提到了平均值和方差的作用。对于参数 w，合适的初始化是非常重要的，例如初始化 w 减去两个 w 或初始化 w 减去其他值。这会影响到参数的收敛和性能表现。

在继续讨论深度 Q 网络（DQN）的工作原理时，提到了它使用经验重放和固定 Q 目标这两个创新方法。通过将转换存储在重播缓冲区中，DQN 实现了更稳定和高效的学习过程。

整体来说，这段内容主要涉及到强化学习算法中的一些关键概念和技术，包括参数更新、初始化和经验重放等内容。在深入研究和应用时，这些概念会对算法的性能和效果产生影响。

在重新播放（回放）内存中，一般情况下会使用来自D的随机小批量样本。因此，通常会以小批量的方式进行采样，而不是逐个进行采样。因此，可能是样本1-或其他任何参数。基于这些情况，你可以进行梯度下降。使用这些旧目标计算Q学习，并优化Q网络与Q学习目标之间的均方误差，采用随机梯度下降，这里没有提到的是我们通常要进行ε-贪婪探索。因此，你也需要一些时间来了解如何进行ε-贪婪探索。因此，并没有在原始论文中进行过复杂的探索。这就是现状。你可以执行多个不同的卷积。他们有图像，然后添加了一些完全连接的层，为每个动作输出一个Q值。让我提一下，对于那些之前没有了解过的人。好的一点是他们，所以，你将看到突破，这是雅达利游戏，他们展示了代理正在进行的操作的表现。请记住，代理只是从这些像素中学习如何做到这一点。

因此，在2014年左右展示这一点是非常不寻常的。刚开始学习这种策略时，并没有做出非常正确的决定。随着时间的推移和观看更多剧集，开始学会如何做出更好的选择。有趣的是，随着获取更多数据，AI开始做出更好的决策。人们对此特别喜欢的原因之一是，您可以通过奖励函数进行学习。在这种情况下，AI显示，如果要最大化预期奖励，最好的方法就是开一个洞并从中尽快反弹[听不清]。所以，这表明如果要求AI代理最大化奖励，它将学会在给定足够数据的情况下采取正确的方法来最大化奖励。这真的很酷，因为它有能力发现人们在某些情况下可能不会追求最大化奖励。

初学者可能需要一些时间来学习游戏策略。然而，一旦掌握，他们可以展现出惊人的表现，并且能够像人类一样游玩多种游戏。对于一些行为上看似混乱的情况，如对大量动作的不确定性或频繁移动，通常是由于AI智能体在探索游戏规则，尤其是在涉及成本的移动时。这种行为对智能体来说可能是合理的，因为在没有积极或消极奖励的情况下，它难以区分不同动作之间的价值。引入牵引层是否有意义可能取决于具体情况。

关于拉力层？也许有一个。我——我记不清楚完整的曲线神经网络架构了，嗯。问题是里面是否包含了拉力层。我认为可能会有——可能会有的。它们必须从图像中不断学习。但是它们有完备的结构。这是一个很好的问题。所以，接下来，你可以看到的是，嗯，它们在许多不同的雅达利游戏上实现了人类水平的表现。大约有50场比赛。嗯，为了澄清一下这点。当他们提到人类水平的表现时，意味着逐渐逼近人类水平。所以，在训练他们的智能体之后，呃，他们并不是在谈论他们或者他们的智能体花了多长时间来学习，就像你们在作业二中会发现的那样，这可能需要大量的经验。嗯，呃，需要花费很多时间来学习如何做出出色的表现。但即便如此，在许多情况下，这对于游戏可能是合理的。所以，它们在某些领域表现出色。在一些领域，它们表现很差。嗯，人们对这类需要大量探索的游戏很感兴趣，这类游戏通常被称为硬探索游戏。我们可能会在稍后的课程中讨论更多关于探索的内容。那么，关键是什么呢？我-我喜欢，呃，这篇论文有很多非常有趣的内容，其中一个非常棒的地方就是他们进行了很好的解释性研究。

让我们来了解一下重要的功能以及查看这些数据时的情况。显然，重要的功能是重播。在这种情况下，线性网络的性能优于深度网络。使用固定的Q值意味着看起来是一个目标保持不变。当你从十中获得一点三后重播，结果突然变成241。因此，在使用数据点后不要将其丢弃，而是要重复使用数据。若将重播和固定Q结合起来，你将会得到改善，至少在某些游戏中通过重播获得突破。在其他情况下，一旦使用更复杂的函数逼近器，你将获得显著的改善。总的来看，重播是非常重要的，它提供了更好地利用数据的方式。是的，在这种情况下，看起来你可能想要使用重播和固定Q值与线性模型，而在这里，使用深度模型可能是个错误。

您同意参考上述表格吗？现在问题是，“嗯，也许我们可以尝试使用线性的——” 我认为我应该很清楚。 所以，这就是全部内容——接下来的所有内容——接下来的四个都很深。 所以，它们没有进行线性加重播。 但是当然您可以尝试线性加重播，并且看起来您可能在这方面做得很不错，这可能取决于您使用的功能。 在过去几年里，已经做了一些很酷的工作，呃，也在考虑，嗯，是否可以将这两者结合起来。 所以，我们做了一些工作，嗯，使用贝叶斯最后一层，使用贝叶斯线性回归，这对于处理不确定性非常有用。其他人刚完成了线性回归，其中的想法是你 - 你有点，嗯，呃，深度神经网络达到某个点，然后你做一种直接线性回归，以准确地拟合权重在最后一层。 这样可能更有效，但仍然需要复杂的表示。 好的。 因此，从那时起，人们对这一领域产生了巨大的兴趣。 嗯，啊，所以，再次，和我自己对强化学习感兴趣，我们过去常常谈论强化学习，大约有40个人会出现，但其中大多数您都会认识到。

然后，随后发生了真正的变化。我记得大概是在2016年，在纽约参加强化学习演讲，房间里有400名听众。今年在一场主要的机器学习会议中，关于自然语言处理的会议上，仅用大约8分钟就爆满了。那次会议有8000人参加，人们对深度学习非常感兴趣，因此深度学习研讨会安排了一个可容纳2000人的大礼堂。这表明人们对这项工作感到非常兴奋，我认为这主要归功于深思熟虑的想法，以及大卫·西尔弗和其他人一直在进行的工作，他们证明了这是可行的。我们很快就会看到一些直接的改进，比如双倍DQN、优先重播和决斗DQN。虽然有很多相关论文，但这些是在DQN基础上的一些早期重要改进。双倍DQN有点像双Q学习，我们在之前的章节中简要介绍过。我们讨论的是最大化偏差，也就是说，对状态动作值的最大估计可能是真实最大值的有偏估计。

所以，简单来说，双 Q 学习的概念是同时维护两个不同的 Q 网络。我们可以使用ε-贪婪策略来选择行动，通过对这两个 Q 网络取平均来做出决策，然后根据奖励观察，将其中一个 Q 网络的值作为另一个的目标。这样，以50%的概率我们会更新其中一个网络，然后通过另一个网络来选择行动，以尝试将行动选择与价值估计分离，以应对最大化偏差问题。另外的50%概率下，我们会更新第二个Q网络，并从另一个网络中选择下一个操作。这个方法是为了解决我们如何选择行动与我们对行动价值的估计之间的偏差问题。

提到两个不同的网络或者两组不同的权重可能非常有帮助。在深度Q网络（DQN）中，这一想法可以应用于当前的Q网络以及一个旧的网络，用于评估动作。这样，您可以使用当前网络进行动作选择，然后利用另一个网络（另一组网络权重）来评估该动作的价值。这个改变类似于我们已经在目标网络和网络权重上做的事情。事实证明，在许多情况下，这种方法为Atari游戏带来了巨大的好处。这种方法通常非常有用，并且可以在某种程度上立即显著提高性能，相当于少量编码的增强。这就是双Q学习的一个变种。

另一个重要的技术是优先重播。回到火星漫游者的例子，在火星漫游者中，我们处理一个非常小的领域，我们只谈论通过一个包含七个状态的表格进行处理。

我们正在讨论一种政策，总是选择行动a1，其结果通常是向左移动。假设我们从状态s3开始，采取行动a1后得到零奖励，进入s2，再次采取a1后留在s2一轮，最终到达s1，然后终止。第一次访问蒙特卡洛估计为1110000，α为1的TD估计如下。我们在讨论中提到了TD仅使用每个数据点一次，不会将信息传播回来这一事实。因此，在这种情况下，TD学习的唯一更新是当我们到达状态s1时，采取行动a1，获得奖励1，然后终止，因此只更新状态s1的值。现在让我们想象一下你需要做的事情——考虑在这种情况下，您的重播备份会是什么样子。你将会有s3, a1, 0, s2, s2, a1, 0, s2, s2, a1...

假设您有一个重播备份，标记为0，状态为s1，动作为a1，奖励为1，然后终止。这是您的一个重播备份的情况。现在假设您可以进行两次重播备份。这样，您有四种可能的重播备份。您可以选择相同的备份两次，然后要求您选择重播备份以某种方式改进价值函数。我建议您考虑一下，或者与邻居讨论应该选择哪一个，为什么选择这样做以及选择的顺序是否有区别。也许选择哪一个并不重要，无论您做出什么选择，最终都会得到相同的价值函数。但是，是否有一种选择方式比较好？如果有的话，为什么？您会以什么顺序进行这些更新呢？希望您有时间思考一下这个问题。

这段文本可能是从一个技术会议或讨论中的内容，内容较为冗长且缺乏上下文，主要涉及到关于选择价值函数的讨论。下面对其进行整理和翻译：

首先，这个问题很重要吗？我想先问大家这个问题。如果你认为根据你得到的价值函数进行选择很关键，请投票。这是很正确的。所以，就结果来看，选哪两个是绝对重要的，无论选择哪两个，价值函数都不会相同。好的，现在再投票。那么我想问，我们应该先做哪个？我们应该更新吗？是先做四个吗？四是回放缓冲区中的最后一个。我们应该先做三个吗？是先做两个吗？好的，好的。所以，3号有它，谁能解释一下为什么吗？是的，我认为，你必须根据第一步和第二步中已有的信息进行反向传播。对的，是的。所以这位同学说得对。那么，如果你选择备份三，那备份三是什么？对，S2，A1，0，S1。所以，如果你做备份，就是0加上S素数的伽马V，S1。这是其中之一。这意味着现在你要进行备份。

所以现在，你的 S2 的 V 将等于 1。你可以备份这些信息。是的。我并没有非常具体地说明，这里要做的正确的事情是，我的主要意图是强调这会带来很大的不同，并且对顺序非常重要。接下来我们应该做什么？如果我们应该这样做，请举手；如果需要再次举手三次；若需要做两个动作，请举手；若需要这样做，请举手。是的。有人可以解释为什么吗？是的，在后面。这与我上次听到的是一样的。是的。所以，如果你想要达到蒙特卡洛估计。在这里你要执行 S3, a1, 0, S2 这将会将你的 S3 的 V 更新为 1。这样你的价值函数就会与蒙特卡洛的完全相同。因此，这一点非常重要。重要的是做的顺序，这是关键。如果你执行了S3，a1，0，…

S2中的S3是不会改变的。所以订购的方式会带来很大的不同。除了考虑之前已经提出的内容外，我们还需要思考在重放缓冲区中应该存放什么样的数据。不仅要考虑应该将什么样的数据放入重播缓冲区，而且我们对这些数据进行采样的顺序对收敛速度可能会有很大的影响。特别是，几年前有一些非常有趣的研究，形式上来看就是关于顺序的重要性。2016年有一篇论文尝试研究什么是最佳的顺序。假设你有一个可以准确计算的预测器。虽然在实际计算中这可能很困难，一般情况下我们做不到，但设想一下，如果预测器能够分析并选择出正确的顺序，那将会怎样。他们发现，在这种情况下，对于像这样的小链条示例，收敛速度可能呈指数级的提升，这是非常了不起的。这意味着什么呢？在价值函数收敛到正确值之前，您需要执行的更新次数可能会明显减小。如果更新得当，您可以让预测器精确告知您样本元组的顺序。

这听起来很酷。嗯，虽然你有能力变得更好，但你无法这样做，因为你不会涉及这些方面。在某些情况下，要准确定义预测顺序可能需要大量计算，成本很高，甚至是不可能的。但是，这表明我们可能需要谨慎选择操作的顺序，因此，他们的直觉是我们可以根据元组的深度Q网络（DQN）错误来确定重演元组的优先级。所以，在这种情况下，DQN错误就是我们的时间差学习误差。这就是当前的差异。这基本上是我们的预测错误。所以这是我们的预测误差，我将其称为时间差（TD）误差，因为它不完全是由于我们正在选择最大值。因此，这是我们预测的时间差误差减去当前误差。例如，如果有一个非常大的错误，我们将更倾向于更新。每次更新时调整这个静态数量，你将其设为新元组为零和一种方法。他们有两种不同的方法。

这种方法实际上是基于优先级排序，对它们进行α次方处理，然后标准化，以确定选择该组的概率。因此在考虑更多因素时会优先考虑权重。如果使用旧方法冻结不会阻碍信息传播吗？如果你首先使用旧方法，我们无法再次将其传播，因为价值变为零。这是一个很好的观点，如果你修复了w减号，然后查看之前的案例，你就无法继续传播，因为尚未更新，这是正确的。所以，解决问题时传播信息会存在紧张关系，这是一个人们必须注意的关注点，虽然没有固定的方法，但确切的时间表是一个超参数。

为了解答你的问题，让我们来梳理一下内容。在修复问题时，数据样本的顺序确实非常重要。尽管在之前的采样过程中顺序不重要，但在数据重播时，排序依然有影响。当权重在缓冲区重播期间发生变化时，可能需要更新权重，比如每50步更新一次。在这种情况下，对于重播缓冲区中的数据点的顺序考虑是很重要的。一旦权重变化，需要考虑如何处理这些数据点的顺序，这是一个重要问题。关于选择的方法，让我进一步澄清一下，如果将alpha值设置为零，那么在选择现有元组时的规则是什么？在这种情况下，Pi代表的是我们的DQN错误。通过将Alpha设置为零，我们在统一选择、无优先级和完全选择一个之间进行折衷。

嗯，就像如果 alpha 无穷大那么就会选择 DQN 误差最高的一个。 所以这是一种权衡，是一种随机。 好的。 所以，嗯，他们将这一点与我选择这三个[听不清]的原因结合起来，它们是彼此叠加的一层。 因此，优先重播与我认为这是优先重播加 D，嗯， 双 DQN 与仅双 DQN。 大多数时候，嗯，这是零，它们在下面都是相同的，意味着平坦，香草 DQN，双 DQN 更好。 以上意味着优先重播更好。 大多数情况下，优先重播的时间更好，这里有一些超级参数可以使用，但大多数时候它是有用的。 考虑一下我们的秩序可能很重要，这当然是有用的。好的。我们剩下的时间不多了，所以我要简短地讲一下，以便您了解这一点。 嗯，ICML 2016最好的论文之一是dueling。 这个想法是，如果你想在世界上做出决定，他们正在处理一些更好或更差的状态，他们只会有更高的价值或更低的价值，但是真正想做的是找出在特定状态下正确的行动是什么。 嗯，所以，你想让我们理解的是，

这个优势函数的作用是用来比较采取特定行动与遵循当前政策相比是好还是差。与关注估计状态价值不同，它关心的是各个行动之间的优势大小。因此，研究者正在研究这个优势函数。与深度 Q 网络（DQN）不同，他们不直接输出所有的动作值 Q。相反，他们首先估计状态的值，然后估计优势函数，即状态 s 下采取动作 a1 和 a2 相对于该状态值的优势。这种拆分是一种架构选择，通过学习将这些值组合起来形成 Q 值。这有助于我们更关注准确估计动作的优劣，即哪个动作更好或更差。关于这种方法是否可能被识别有一些涉及隐私的问题，但我现在没有时间来深入讨论。总体而言，这种方法是不可识别的。我很乐意与您讨论更多，因为这是非常重要的。

它们只是强迫人们对某种默认假设做出某种附属功能的指定。 根据经验，这通常非常有帮助。 因此，与我们刚刚看到的具有优先重放的双 Deep Q-Network (DQN) 相比，它已经比双w-DQN更出色，甚至比普通DQN更出色。 这又为您带来了另一项实质性的性能改进。 所以基本上这些都是经过两年后推出的DQN的三个不同的版本，相比于完全取代普通DQN，在性能上取得了一些显著的进步。 对于作业二，您将实施DQ，而不是其他方法，但他们也鼓励尝试其他方法。 值得一提的是，这些是一些旨在在《ATARI》上提供更好性能的主要初步改进。 嗯，我会留下这个。 我们时间不多了。 请随意查看最后几张幻灯片，了解John Schulman。

强烈鼓励完成作业后按顺序进行线性案例，确保Q学习有效。然后将其部署到ATARI上，即使是像我们正在开发的《Pong》这样的小游戏，也是非常耗时的。因此，建议重新理解和深入了解Q学习方法是否有效，然后等待12小时，查看在《Pong》上是否有学习到东西。这就是为什么我们在任务中采取这种方式的原因。此外，还有其他一些实用的技巧，可以参考一下，然后我们就到星期四了。谢谢。