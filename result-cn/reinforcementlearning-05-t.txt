早上好，让我们开始吧。今天是作业一的截止日期，作业二将在今天发布。作业二将涉及函数逼近和强化学习。我们今天将介绍这些内容，下周我们将继续深入学习。深度学习并非先决条件，但我们会在本周晚些时候发布有关TensorFlow的教程。下周的会议将提供更多深度学习背景知识。您无需成为专家，但需要足够了解以完成作业和函数逼近。我们假设您对梯度下降和求导等内容非常熟悉。TensorFlow等软件包可以为您自动处理，但您应该了解整个过程。在我们继续模拟之前，有任何后勤问题吗？好的，让我们开始吧。如您所见，我的声音有些问题。

当前恢复工作正在进行，但我们将继续关注进展情况，如果遇到困难，就会采取控制措施。在这一段时间里，我们一直在讨论思考和学习，特别是在连续的决策情境中评估政策，并做出相应的决策。这一切都是在未知环境中进行的，没有提前给定动态模型或奖励模型。今天我们要开始探讨的是价值函数逼近。我想知道，有人之前见过这个概念吗？有些人可能有所了解。当提到价值函数逼近时，指的是我们通常讨论的领域，在这个领域中，会有一组有限的状态和动作，我们可以通过编写一个表格来跟踪状态的价值或状态动作对的价值，或者创建数据表来明确描述模型、奖励模型和动态模型。

但是许多现实世界的问题都有巨大的状态和行动空间。例如，像雅达利游戏这样的问题可能不是严格意义上的现实世界问题，但肯定是具有挑战性的问题。当我们讨论状态空间时，实际上是指一组像素。这一空间巨大到无法用一张表格来表示。因此，在这些情况下，我们需要超越表格表示形式，真正考虑泛化问题。我们希望能够做出决定并学会做出正确的决定，需要能够从之前的经验中进行概括，这样即使遇到之前从未见过的状态动作对，也能够做出正确的决定，这就是需要概括的意义。因此，我们今天要讨论的是从价值函数近似开始，用于预测，然后控制。

在这种情况下，我们要开始讨论的关键思想是，使用参数化函数来表示状态动作值，或者说值函数。 因此，我们可以将值函数看作是一个接受输入状态并输出值的函数，而不是像在表中查找数值那样，而是由一些参数进行确定。 这个函数可能是一个深度神经网络，也可能是一个多项式，总之它可以是各种不同的函数形式来近似值，但关键在于我们有一组参数，使我们能够确定任何输入状态的值。 就像我们之前看到的那样，我们会不断思考状态价值函数和状态动作价值函数之间的关系。 现在的关键是我们有这些参数，通常我们会用w来表示这些参数。 因此，您可以将w视为一个向量，这个向量可以是深度神经网络的参数，也可以是更简单的形式。

在AI领域，为了解决存储和计算量大的问题，我们需要考虑使用紧凑的表示形式来学习，而不是为每个状态-动作对都保存学习信息。这种方法可以帮助我们对庞大的状态空间和动作空间找到有效的策略，而不需要存储大量数据。通过这种方式，我们可以减少内存和计算量的需求，甚至可能减少需要的训练数据量。因此，紧凑的表示形式对于提高效率和性能非常重要。

嗯，可能会有一些非常糟糕的情况，这将是对真实情况的非常糟糕的近似。在不需要大量数据、计算和内存的情况下可能会很不错，但它们可能不适合在政策方面表现出色。这些选择定义了你无法表示策略和价值函数的空间。因此，会存在一种偏差-方差权衡，并引入一个函数近似的平衡。从某种意义上说，如果你有一个非常简单的表示，你不需要太多数据来学习适应它，但它也不会很好地适应复杂的价值函数或策略。

举个简单的例子，我们可以假设我们的AI智能体始终处于相同的状态。所有视频游戏的帧总是相同的，这是一种高度压缩的表示。虽然我们只有一个状态，但这不会使我们学会在不同情况下做出不同决策，比如游戏的不同部分。因此，这种表示方式不会让我们获得高回报。

在选择表示能力时，通常需要在所需的表示能力、内存、计算和数据之间进行权衡。有时候，我们可以选择非常紧凑的表示形式，仍然能够准确地代表所需属性，以做出正确的决策。然而，我们需要考虑到存在权衡，通常我们并不知道什么样的表示能力足够才能取得高回报。这种权衡可以看作是偏差-方差权衡的正交考虑。当选择非常有限的表示能力时，我们可能会遇到这种权衡。

你将永远有偏见，因为无法准确表示真正的函数。在很长一段时间内，小表示会导致较小的方差。因此，并未很好地转移到相关特征。在机器学习中，要考虑结构风险最小化，并思考模型容量选择与数据量之间的权衡，以最小化测试误差。因此，需要权衡泛化能力和表达能力。因此，一个重要的问题是我们将使用什么函数来进行近似？虽然有许多选择，但大多数可能性在强化学习中已经尝试过。在监督学习中，几乎可以尝试任何事情，例如将函数逼近当作值函数，可以使用神经网络、深度决策树、最近邻或小波基等多种方法。

在这堂课上，我们主要关注可微分的内容。出于多种原因，这些内容非常重要，因为它们通常具有良好的平滑优化属性，更容易进行优化。这也是我们关注这些内容的原因之一。然而，并不是所有情况下都是正确的选择。举个例子，对于熟悉决策树的人来说，你可能会考虑决策树在何处代表您的价值函数或策略。决策树通常具有高度可解释性，可以帮助所有人更加简单地理解。在某些情况下，树形结构可能更有帮助，比如用作医生的决策支持系统。在这种情况下，使用深度神经网络来解释为何选择对患者施行特定治疗可能不太有效。

如果你使用决策树，通常它们具有很高的可解释性。取决于所使用的特征，一般来说，决策树非常易于理解，这对于帮助推断非常有帮助。因此，您经常使用的函数逼近器取决于您之后将如何使用它。最近关于可解释深度神经网络方面也有一些令人兴奋的研究，您可以在深度神经网络上面安装一个更简单的函数逼近器。您可以先拟合深度神经网络，然后尝试拟合决策树，以达到兼顾表达能力和可解释性的效果。值得考虑的是您所研究的应用类型，因为不同的应用场景可能需要不同的函数逼近器。当前在强化学习中最流行的两类函数逼近器可能是线性值函数逼近和深度神经网络。我们首先从线性值函数逼近器开始，有两个原因：一是它可能是强化学习中最彻底研究的函数逼近器，但是

直到最近几年，深度神经网络被视为计算复杂特征集并在许多情况下进行线性函数近似的手段。这为后续工作奠定了良好基础。因此，我们将快速回顾梯度下降算法，因为接下来的几天里我们将大量使用它。我们考虑任意通用函数J，它是参数向量W的可微函数。我们希望找到参数W，使目标函数最小化，尽管还未定义目标函数，但我们很快会给出。我们需要计算J对W的梯度，即对参数向量W中的每个参数分别求导的过程。

因此，这是尝试优化函数的梯度下降方法。首先，计算目标函数J关于参数向量W的导数或梯度，然后沿着梯度的方向调整参数向量。一般情况下，我们假设权重是一个向量，我们用当前的权重值减去学习率乘以目标函数的导数来更新权重。因此，我们计算函数的导数，然后根据一个步长来更新我们的参数权重。然后我们继续这个过程。那么，通过多次迭代，我们能保证找到局部最优解吗？是的，假设是的。虽然学习率可能有一些限制条件，但是通过足够多的迭代次数，我们确实能够达到局部最优解。

嗯，我不确定这是否是本地的讲话方式。因此，让我们从城邦的角度来思考这个问题。就强化学习而言，关键在于思考我们要收敛到何种程度，以及是否需要收敛。我会在整堂课上更多地探讨这个问题。因此，我们将尝试一种本地方法，逐步调整值函数的参数表示，以获得更好的近似值。是的，让我们考虑一下如果我们试图进行政策评估，这将如何应用。再次强调，政策评估是某人为你提供一个政策的评估。他们向你提供了一个映射，描述了在一个状态下采取什么样的行动，这可能是随机的。因此，这个映射可以被视为从状态到动作概率分布的映射。有人向你提供了一份保单，而你需要做的是弄清这份保单的价值。通过遵循这个政策，你预计会得到多少折扣奖励？因此，让我们假设一下，嗯，

我们可以通过查询特定状态，Oracle会返回策略的真实值。 如果我们想知道从房间的某个位置出发，根据某个特定政策走向门口时的预期折扣回报总额，Oracle可以告诉我们，比如平均需要走30步。 这样，Oracle可以返回状态和价值对，帮助我们找到最佳的状态值表示。 假设我们拥有所有这些数据，我们的目标是拟合一个函数，用参数化函数准确表示所有数据。 在这种情况下，我们可以使用随机梯度下降来最小化我们预测值与真实值之间的损失，以找到状态值对的最佳表示。

这些给定的真实数值，我们的目标是使用一个函数逼近器来拟合这些数据。因此，这个过程与监督学习非常相似。通常我们会使用均方损失，稍后我们会再讨论这一点。在这种情况下，均方损失用来衡量真实值与模型预测值之间的差异，其中模型的预测值由参数向量参数化。我们将使用梯度下降来优化模型。因此，我们需要计算目标函数的导数，然后采取一个步长来更新参数。在这里，我们会使用随机梯度下降，这意味着我们只需对部分样本计算梯度。换句话说，当我们对目标函数进行求导时，我们会得到类似这样的结果。接下来我们要做的是...

我会将原文内容进行重新整理和翻译，让它更易于理解：

**原始文本**:
我将使用它作为更新权重的简写， 我将在评估的方向上采取一个小步长 单点。 所以现在，没有任何期望，这只是为了一点。 [噪音] 所以这是随机梯度下降， 我们不尝试计算我们将要计算的梯度的平均值 - 我们只是尝试对这个梯度进行采样， 在特定状态下进行评估。 我现在告诉你的是，有人给了我们 这些状态对和真实的价值函数。 因此，您只需取其中一对， 计算该点的梯度，然后 更新您的波函数并执行多次。 好处是 预期的随机梯度下降与完全梯度更新相同。 嗯，所以这在收敛方面具有很好的特性。 是的，请先报个名。 嗯，所以只是为了确认，呃， 如果你说，如果新元是一个单一的国家，为什么预期是针对政策而不是针对一组国家 ？

**重写文本**:
在更新权重时，我会采取细微调整来评估方向。现在没有预期结果，只是试探性的一步。这就是随机梯度下降的方法，我们不会计算要计算的梯度的平均值，而是在特定状态下对梯度进行抽样评估。在这里，我们已经收到了一些状态对以及真实的价值函数。因此，只需选择其中一对，计算该点的梯度，然后更新您的函数并重复多次。这种方法的好处是预期的随机梯度下降与完全梯度更新相同，具有良好的收敛特性。请先自我介绍。另外，为了确认一下，如果新元素是一个独立国家，为什么预期是针对政策而不是一组国家呢？

在这种政策情况下，您遇到的是状态分布，意思是您在该策略下获得的状态和值的预期分布。这是非常重要的一点，稍后会再次提到。我们无法访问预言机来获取任何状态的真实价值函数。因此，我们需要讨论如何进行无模型函数逼近，以在没有模型的情况下进行预测评估。如果回顾我们之前讨论过的基于蒙特卡罗或TD学习的方法，我们可以在线自适应地学习价值函数，以表示特定策略的价值。我们是通过数据来实现这一点的。

我们现在要做的事情完全相同，只是每当我们进行更新步骤时，不仅需要使用新数据更新我们的估计器，还需要对函数进行逼近。因此，我们不再像增量更新关于状态值的表条目那样，每当获得新数据时，都需要重新逼近我们的函数。所以，当我们开始这样做时，就需要选择一个特征向量来表示状态。让我解释一下这意味着什么。想象一下，我们在考虑一个机器人，这个机器人可以拥有很多令人惊叹的传感器，但让我们假设它是老式的，只有一个激光测距仪。在过去，很多激光测距仪基本上都是180度的，因此你只能得到沿着这180度遇到的第一个障碍物的距离。所以，也许这个距离是两英尺，相当于1.

在这里，我们有一个机器人，它装备了一个激光测距仪传感器，用来测量墙壁距离。这种特征表示方式存在什么问题呢？在走廊中，不同部分会有相同的距离特征，而且这种表示方式可能缺乏标记信息。例如，走廊的墙壁可能会被描述为“左右各两英尺远，前方没有”，但这种描述方法在特征表示时会导致很多部分无法区分。

这是一个特征表示的示例，可能没有被明确标记出来，但是却是合理的。比如，当你在走廊中央行走时，别人可能看不出你想要继续前进，这就是特征表示的一个例子。特征表示的选择至关重要。在深度学习课程中，你可能已经听说过这些，但这是在深度学习之前的概念。通常，在进行特征工程时需要大量的工作，以确定正确的方法来描述状态空间，以便做出预测或决策。深度神经网络的优势之一是可以推迟特征选择问题，允许使用真实高维度的传感器输入，减少手动调整。手动调音是什么意思呢？在这种情况下，你可以使用原始特征，比如在每个180度方向上的距离，或者想象一些更高级的抽象特征，比如尝试识别角点。因此，你可能已经对这些原始数据进行了一些预处理。

在做出决策时，与标记相关的功能可能存在一些问题。 如果选择了错误的标记集合，可能无法做出正确的决策。 嗯，首先要说名字。 嗯，你能详细说明一下为什么这不是标记吗？ 嗯，这个[噪音]啊有点像180度。 是吗？ 是的。 那么问题是我能详细说明为什么这不是标记吗？ 嗯，如果机器人有一个 180 度的角度，假设你有像长走廊这样的结构，第一楼对应一楼，第二楼对应 二楼，有许多门。 所以，如果你有一个机器人正在穿过这个走廊，它依靠激光测距仪来测量到所有物体的距离，你就无法通过这些读数来区分自己是在一楼还是二楼，因为即时传感器读数会是一样的。 事实上，根据这些读数，你甚至无法确定自己在走廊的哪个位置。 是的？[听不清] 嗯，所以我们可以总结说，如果我们有部分混叠，那么...

噢，你在说这不符合马尔可夫？这是个很好的问题。是的，如果我们有一些部分重叠，那么它就不是马尔可夫了。我的意思是，通过涵盖历史记录来改变要标记的状态表示，这样每个单独的观察都会被别名，但整个状态表示不会被别名。但一般来说，是的，如果您有一个状态表示，其中有，嗯，别名不是标记。也许你仍然可以很好地利用这种表示，或者你可能不会，但了解一个人所应用的技术是很重要的。好问题。好的。所以，让我们考虑使用线性值函数近似来实现这一点。那么，线性值函数近似是什么意思呢？这意味着我们只需要一组权重，然后用一组特征来生成它。这表示，也许这是我180度传感器的读数，然后我只需为这180个特征中的每一个分配一个权重。我们可以重新利用它来表示值函数，或者也可以对状态动作值函数执行此操作。嗯，那些已经在考虑。

提到状态动作价值函数的人可能会注意到，一旦开始考虑Q值，至少有两种方法可以实现。一种是为每个动作设定一个独立的权重向量，另一种是将动作视为一种附加的特征，本质上是多种不同选择中的一种。不同形式的共享也是可能的。但是，现在我们只是在考虑估计特定策略的价值。因此，我们只考虑值，会记住W是一个向量，X是另一个向量。现在，X和S将为我们提供该状态的特征。所以，真实世界的状态可能是机器人的位置，而你得到的特征可能是180个读数。因此，我们再次关注均方误差，我们的目标函数就是均方误差。我们关注我们预测的值与实际值之间的差异。这将指导我们的权重更新，也就是说，我们想要通过乘以该函数的导数和学习率来更新我们的权重。那么，在线性值函数近似的情况下，这会是什么样子呢？【噪音】因此，我们希望的是利用...

我们知道实际上要计算 J 的导数，需要将 X 乘以 W。在这种情况下，我们得到的是 W 减去 ΔW 等于 S 乘以（P π的1.5次方α）减去 S 乘以 W，因为相对于 W 而言，X 乘以 W 的导数为 X。这是所有状态的期望值还是特定状态的期望值呢？好问题，请再提醒我一次你的名字。是的。所以问题是，这是所有状态的期望值还是特定状态的期望值？当我们更新 W 时，我们会根据特定状态对其进行评估。因此，我们会针对每个状态执行这个操作，通常采用随机梯度下降的算法。因此，我们会在每个状态都这样做。在这里，你可以考虑的期望值实际上是从该策略中采样得到的状态分布。因此，如果您要在真实的马尔可夫决策过程中执行该策略，您将面临各种状态。

我们很快会更深入地讨论这个分布是什么样子的，但现在我们的目标是尽量减少我们在这一政策下可能遇到的状态分布误差。这些问题都问得很好。现在让我们看一下这个表格，它展示了我们在TD学习中熟悉的步长。然后我们有一个预测误差，即值函数的真实价值与我们估计的价值之间的差异，再加上一个特征值。因此，线性值函数近似的一个优势是，这些更新形成了一个很直观的概念，即你对特征加权的真实价值的估计有多大的偏差。对吗？这里涉及到一个数学问题，这样你就可以得到Vπ帽子上的负数[听不清]。那么，那里是否应该是负数过多，而外部也应该是负数[听不清]？我们将把它推向任何一个方向，然后逐步改进。

因此，重点在于要警惕它们可能造成的负面影响。是的，你可以把这些负值传递到这里，通常α是一个常数，因此你可以取其相反数让其为正或负。通常，你希望，嗯，如果你最小化这一项，啊，你会从权重中减去它，但你需要谨慎，这取决于你如何定义你的α，确保你在梯度下降 - 梯度步骤中朝着正确的方向前进。好的，这是一个很好的问题。那么，我们应该如何做到这一点呢？再次提醒，我们实际上无法访问真值函数呢。嗯，所以我们实际上不知道，所以在这个公式中，对吧？它假设这是真的，就像 Oracle 已经为您提供了该策略下的状态值一样，但我们当然无法访问该值。嗯，因此我们要做的就是采用与我们在表格学习中看到的相同类型的思想，嗯，现在使用值函数的近似。因此，预期回报或从时间步 t 到剧集结束的奖励总和是。

在时间步 t 上，当前状态的真实预期回报是一个无偏的噪声样本。因此，我们可以尝试使用蒙特卡洛值函数逼近，就像我们对状态-动作对的回报集合进行监督学习一样。这里我们用 G_t 替代真实值，这是对真实价值的估计。虽然我们不知道真实值是多少，但是我们知道蒙特卡洛返回是一个无偏估计，因此我们将用它代替。那么这意味着什么呢？这意味着我们正在进行线性值函数逼近吗？这意味着在等待更新时，我们有一个 G 在此处。因此，我们会采取动作，在这种情况下是国家。我们将获得这个集合中的奖励总和。需要再次强调，这仅适用于情景设置，就像通常使用蒙特卡洛一样。然后我们进行微分，这里是 x，我们的特征，因为我们使用线性值函数逼近，然后在最后一行，我只是将我们的 V 估计器准确地插入其中。所以，我们将我们的回报与当前的估计量进行比较。

然后将其乘以我们的特征。和往常一样，我们遇到的问题是 G 可能是一个非常嘈杂的回报估计。是的，请先说名字。

我们能像以前一样区分第一次和每次吗？有点。我们是否会像以前一样区分首次访问和每次访问？很好的问题。我们，嗯，区分第一次访问和每次访问吗？是的。同样的区别也适用于蒙特卡罗，请记住之前的应用。

所以，我在这里，我正在展示它的首次访问变体，但你也可以，也可以每次访问。它将具有与以前相同的优点和局限性。每次访问都是有偏差的，渐进地它是一致的。

好的，那么权重是什么样的？在这种情况下，我们会说权重等于旧权重加上 s 的 Alpha 乘以 G_t 减去 s 乘以 w 的 v，请记住，对于该状态，这只是 x 乘以 w，乘以 s 的 x。

这部分内容涉及到类似于蒙特卡罗政策评估的方法。在这里，我们并没有值函数，只有一组权重，初始设为零向量。我们首先对一个情节进行采样，根据指定的策略进行采样直到情节结束，然后我们逐步执行该情节。如果这是首次访问某个状态，我们计算从该状态到情节结束的回报，然后更新权重。您是否考虑了学习率以及奖励机制？在这方面，是否在意负面和正面影响？在这里，“阿尔法”指的是学习率，是一个选择。通常我们会尝试最小化我们的目标函数，即减少权重，并需要再次进行。

我们如何选择 Alpha 时间需要谨慎对待，并且已经对我们在每个状态下进行的评估进行了考虑。在第六步时要特别注意，是重新审视因素还是只是添加通知？这是个好问题。在情景强化学习中， Gamma 的作用总是需要考虑的。在情景性任务中，总是将Gamma设置为1，因为在这种情况下，不需要考虑折扣因子。所以，在偶发情况下，设定Gamma为1是合理的，因为当您需要考虑值函数时会使用小于1的Gamma值。

这个概念总是确保是有界的，并且在这里考虑到 Gamma 也是完全可以的。我有几个关于同一点的问题，关于这个 G。当我们这样操作时，似乎我们会对具有奖励的 G 进行不同长度的采样。但是，在没有平稳性和更多方差的情况下，闭合它们的分布吗？具体问题是存在一个情况，您获得的回报将是不同长度的总和。然而事实并非如此。因此，你总是在尝试估计在这种状态下的价值，这本身就是在这一政策下。在偶发情况下，您可能会在轨迹的早期或晚期遇到该状态，且您的价值将根据您是否遇到早期或晚期以及其中一个回报进行平均。因此没有问题，我们假设所有的轨迹都是有界的，它们必须是有限的。所以必然存在概率，即您的轨迹必须结束。

如果这真的是如此，那么，嗯，你的奖励总是有限的，你总是可以对其进行平均，这是很好的。有时你可能会遇到，在训练的早期阶段有许多奖励的状态，而其他时候你可能会在训练结束时遇到的奖励较少，[噪音]嗯，兴趣值在于对所有这些情况的期望。[噪音] 对吧？[噪音] 嗯，在这段话的澄清中，实质上是指上行链路，您正在更新该集合中的每个小视频片段的近似值。所以，[听不清] 这不仅仅是一次，而是随着训练的进行。您不只是一次更新权重，对吧？因此，您会查看该集合中遇到的所有状态，并针对每个状态更新权重向量。[噪音] 这相当于生成所有的片段并尝试以逐个片段的方式提供它们 - [听不清] 好了，如果我们在批处理设置中执行这个操作，那么如果您生成所有数据然后尝试去适应它。这是一种逐步的方法来完成这一点，嗯。

现在最终我们聊到了同一个事情。是关于Gamma的。现在我们开始逐渐折扣J减去T，就像在前进一样。在这种情况下，我们不需要考虑Gamma。通常来说，在这种情况下，并不会有伽玛的影响。但保持事情准确是有用的。现在，让我们考虑一个特定的例子。当我们逼近函数与决策相结合时，并在线进行增量更新时，情况可能开始恶化。这意味着我们可能无法收敛到我们想要的表示最优值函数的位置。所以，有一个很好的例子可以说明这个问题。

在20世纪90年代初开始思考函数逼近时，贝尔德提出了一个例子，用以展示在组合中进行函数逼近时所面临的一些挑战，特别是在控制和决策方面。现在我们要介绍这个例子，在进行MC政策评估时，我们将多次在课堂上看到它。这个例子究竟说明了什么呢？在这个例子中，会涉及到两个动作，其中a_1是一条直线，这些动作将确定性地进入一个状态，我将其称为S7状态。这个状态是S1、S2、S3、S4、S5、S6的组合。你可以在气泡内部看到它们的特征值是什么。所以，记住我说过我们会有一个状态，然后我们可以将其描述为一组特征。那么S1是怎样的呢？看起来像二、二、三、四、五、六、七。所以，重量一是二，重量八是一。那么S2又是怎样的呢？S2看起来像是从零到一、

二、三、四、五，S3看起来是这样的。然后逐步推移，直到我们达到S7，如下所示。好的，所以S7看起来与其他产品有点不同。这些状态的特征表示就是这样。请注意，它看起来类似于表格表示。实际上，特征比状态多一个。因此，这里只有七个状态，但有八个特征。这是完全可能的，对吧？就像您的特征表示可能多于实际存在的状态数量。因此，我们有动作a_1，动作a_1总是以确定性方式将我们从任何状态带到状态S7。然后我们有动作a_2，以此类推。动作a_2的作用是……

以1/6的概率，您将被分配到状态Si，其中i为1到6，表示您基本上是均匀地分布在前六个州中的某一个。在这种情况下，您只有两个选择：要么确定性地进入状态S7，要么以相同的概率再次进入前六个状态中的某一个。这是一个非常简单的控制问题，因为奖励为零，没有任何偏好或指导。因此，该系统的价值函数在所有状态下都为零，因为没有奖励可供利用。然而，有时我们可能会遇到一些问题。在继续讨论之前，让我们首先考虑蒙特卡罗更新算法会做什么。此外，我们还需要考虑一种情况，即在状态S7有一些额外的微小概率导致我们最终到达一个终点状态。因此，正如我们之前所说，我们以0.

在这个案例中，我们有一个模型，以概率999停留在状态S7，以概率0.99停留在状态S7，以概率0.01终止。这是为了进行蒙特卡洛案例而轻微修改的一个实例，以便我们可以思考剧情结局。当处于状态1到状态6时，可以选择转移到S7，也可以选择继续留在原来的状态。处于S7时，可以选择转移到状态1到状态6，也可以选择终止。假设我们从状态S1开始，采取动作a1，将确定地转移到状态S7。在此之前，奖励被设定为零。我们进入S7，采取动作a1，得到零奖励，然后采取动作a1，得到零奖励，最后终止。这就是整个情节。现在，让我们考虑蒙特卡罗更新。对于状态S1，蒙特卡罗更新将会是什么？

在零。零。所以，回报也是零。嗯，x是什么？我应该告诉你。因此，让我们首先将所有的权重初始化为一。那么，我们对状态S1的价值函数的初始估计是多少？[听不清]有多少？所以，所有的权重都是一。状态S1表示为200013。没错。好的。所以，这正好等于我们的，啊，X乘以W。好的。那么，我们的更新是什么样的，当然我必须告诉你alpha是什么。因此，假设alpha等于0.5。那么，我们的权重将会是——权重的变化将等于0.5乘以0减去3乘以X的特征向量。我们的X特征向量是20001。所以，这意味着我们将得到负1.5。

在进行数学计算时，我们可以计算 20001 乘以负 3 再乘以负 1.5。一二三四五六。这样做可以为我们提供每个权重的更新，但只会提供在特定状态下非零权重的更新，即第一个权重和第八个权重。因此，如果我们想要得到新的权重，我们可以让新权重等于原权重加上增量。这样，我们的新表示将是负二、一、二、三、四、五、六减去 0.5。这就是第一个状态的蒙特卡罗更新。然后，您需要对该集中的每个状态执行类似的操作。例如，当您第一次看到它并按照之前定义的算法进行操作时，您将执行这些步骤。接着，我们来看下一个状态 S7，情况也会类似，回报是零，但值会稍有不同，因此在这种情况下，我们会得到不同的值，实际上是 3。

这取决于您是否已经更新了w，因此您的值已经不同了。因此，我们按州而不是按集来计算SGD。问题是我们是否为每个集或每个州执行SGD？我们按州这样做。在上一张幻灯片中，在每个状态之前都有一个“每次遇到”，这是否意味着对于每个状态，对于该集中的每个第一次访问。是的。就在这个特定的框架内——如果是这样，那么您就会进入一个新的情节，即S7。关于通过第一次访问的问题，我们基本上沿着该情节进行，类似于我们之前对蒙特卡罗所做的事情，第一次遇到状态时，我们使用其返回值来更新权重。当我们对每个独特的状态以及我们第一次看到它的那一集都这样做时，然后在这一切之后我们会得到一个新的情节。所以，这就是会发生的事情。您可以看到变化可能相当大，因为我们正在比较价值函数的完全返回。这当然取决于我们的阿尔法。

阿尔法在这里指代一种可以随着时间变化的数值。通常，我们希望 alpha 随时间变化以便收敛。这里提到了使用线性值函数逼近器进行蒙特卡罗更新的示例。那么，一个自然的问题是，这样做是否合理？我们能否确保最终会收敛到正确的结果？这里所指的正确结果是指什么呢？考虑到线性值函数逼近器的限制，我们是否能保证最终达到类似于该逼近器中最优解的状态呢？在进行这一步之前，我们先讨论一下状态的分布以及它对结果的影响。回想一下之前的课程中我们讨论过马尔可夫过程、马尔可夫奖励过程和马尔可夫决策过程之间的关系。我们谈到，一旦您定义了一个具体的策略，那么你的马尔可夫决策过程实际上就等同于一个马尔可夫奖励过程。可以将其想象成一条链，其中下一个状态由您的动态模型确定，而您只能根据您的策略来选择动作。

因此，当您执行由具有特定策略的马尔可夫决策过程（MDP）定义的马尔可夫链时，最终会收敛到状态上的概率分布。这种收敛的分布被称为稳态分布，即在平均情况下，您处于状态一的时间与状态二的时间百分比是多少。这些百分比总和必须为一，因为这是一个概率分布。在任何时刻您总是处于某个状态，并且满足平衡方程。所以，它描述的是之前状态的概率分布。让我总结一下，是的，我明白了。让我换个角度来看待这个问题，可能会更容易理解一些。就像S素数的d等于S的总和a一样。我们目前只是在做求和，以便确定我们可以采取的随机策略。因此，我们会考虑在当前状态下可以采取的所有行动。

接下来我们考虑在下一个状态中可以过渡到哪里。在各个状态之间有一定的分布。我们考虑可以采取的所有行动，以从当前状态过渡到可能的每个状态。这给出了一个关于素数状态 S 的新分布，这两个分布必须相同。因此，当马尔可夫链运行足够长时间时，通常被认为具有混合属性。平衡方程最终会成立，这意味着在经过完全混合后，在前一个时间步的状态分布必须与下一个时间步的状态分布完全相同。这只是告诉我们在任何特定时间步长内，处于特定状态的概率是多少。然而，这并没有告诉我们这个过程需要多长时间才能发生。这在很大程度上取决于系统的基本动态。因此，可能需要数百万步才能达到平稳分布，或者可能会很快混合，这取决于转换矩阵的属性。

在这堂课上不会讨论这些问题。 重要的是要明白你不能只是等待 100 步，然后就一定会处于取决于问题的平稳分布中。 是的，有证据证明了吗？有证据证明这种蒙特卡罗方法的混合时间有一些已知的界限吗？我从来没有听说过。也许有一些吧。 噪音通常是一个棘手的问题，因为你不知道需要多长时间才能达到平稳分布。 大约一个月前，有一篇很棒的论文发表在某处，讨论了在考虑政策评估时如何进行，我们晚些时候会详细讨论这个话题。 不是考虑超步、比率，或者是否采取某种行动和某种政策，而是考虑不同策略间的平稳分布及其差异。 问题是，通常不清楚需要多长时间和数据是否达到平稳分布。 所以，如果有简单的测试来验证这一点，那就太棒了。 但实际上这是非常困难的。 是的。

抱歉，我无法理解您所说的内容。但是确实如您所言的情况。是的，嗯，为什么不是这样呢？是啊，所以问题是...当你给出一个很冗长的前奏，让人觉得事情可能不会收敛，但最后一切看起来都很好时，我会做出一个很长的“什么”。我们要去那个酒吧。是的，我们要讨论的事实是，在政策制定中我们只是进行政策评估。一切都会好起来的。只有当我们进入控制案例时，我们才会使用一项政策的数据来估计另一项政策的价值，在这个例子和许多其他例子中，事情开始顺利进行。因此，我们将使用它作为一个运行示例，但是现在您没有理由相信这是病态的。好的，这就是平稳分布。然后，收敛保证与此相关。好的，所以我们要做的就是找到线性值函数逼近器相对于平稳分布的均方误差。为什么这是合理的呢？嗯，因为您可能更关心您经常访问的状态下的函数近似误差。有一种状态真的非常罕见，

在定义总体均方误差时，可能会出现更大的错误是可以接受的。我们希望它基于平稳分布。这里所说的是预测误差的均方值。它比较了我们的预测与真实值，并加权考虑了状态分布。假设我们使用的是线性值函数的近似器。需要指出的是，这涉及到历史原因，涉及到约翰·齐齐克利斯。

有些内容看起来有点混乱，我来帮你整理一下。此文涉及到的内容似乎是关于机器学习中的权重调整和线性回归的讨论。

在机器学习中，当你有大量数据可用时，通过多次调整权重，你可以收敛到最优权重的可能性。尽管在实际情况下不会完全为零，因为价值函数无法完全用带有权重的线性函数来近似，但它会尽最大可能找到最佳解决方案。基本上，它是尝试用最好的线性回归来拟合你的数据。

这种方法对于健全性检查很有帮助，让你达到最佳结果。虽然一些人可能倾向于渐进的方法，比如在客户推荐系统中，随着时间推移，获取数据并更新评估器。但在某些情况下，你可能只能立即处理大量数据。这时候，你可以采用批处理，即蒙特卡罗值函数逼近器，它通过大量来自策略的序列来进行估算。

我们可以通过分析方法来找到最佳的逼近器。这样，我们的估计器将成为真实预期回报的无偏样本。在这种情况下，我们的数据集 N 是一个线性回归问题。我们将使用这个无偏样本来估计真实价值函数。我们的目标是找到能够最小化均方误差的权重。你需要对权重进行求导，并将导数设置为零，这就是线性回归。通过分析，您可以解决这个问题。在某些情况下，您可以分析性地进行政策评估。在这种情况下，您也可以对线性值函数逼近器进行分析。需要再次注意，这是Monte Carlo方法，没有做出马尔可夫假设，只是使用全额回报。因此，在非马尔可夫环境中也是有效的。当然。可以讨论这种方法与我们使用其他导出方法进行政策评估之间的差异。通常情况下，这种方法的计算成本较高。

X 可能是一个非常大的矩阵，甚至可能无法直接表示出来。在未来的表示或处理中，可能需要对其进行矩阵逆操作。然而，由于可能存在巨大的特征向量和数以百万计甚至数十亿的样本，这样的操作可能并不现实。例如，Facebook 也无法直接处理这么大的数据量。即使可以逐步进行处理，仍然需要不断地重新调整所有数据，这将耗费大量内存和计算资源。对于小规模数据而言，这可能是可行的，但也取决于是否已经拥有所有数据。

可以尝试进行批处理来帮助收敛，以避免辐射估计出现剧烈波动。因此，处理大型数据集时可以选择全量增量处理、全量批量处理或混合批量处理。在处理大规模数据时，合理使用批处理将能更好地平衡内存和计算资源的消耗。

在深度学习中，进行一定量的分析工作比处理少量数据更为重要，尤其是当涉及到时间差异学习时。在时间差异学习中，我们同时使用引导和采样，不同于蒙特卡罗仅使用采样来近似期望。时间差异学习也使用引导的原因是我们不需要等到整个剧集结束，而是通过当前的价值函数来引导和结合估计的未来预期回报。在这种情况下，我们使用了引导的思想。目标通常是奖励加上折扣率乘以下一个状态的值。抽样则是为了...

接下来是对时间差异学习的介绍，过渡会实现更流畅。

我们进行抽样以接近期望结果，并没有考虑所有S素数的概率，而是对它们进行求和前将内容整理成表格。现在我们不想再这样做。在开始讨论之前，让我们回顾一下三种形式，即我们即将获得的近似形式。接下来，我们将进行函数逼近、引导和采样，但同时还坚持政策评估。这意味着我们正在从政策中收集数据，试图估计其价值。在这种情况下，遵循政策会使事情变得更容易，似乎更直观一些。这与监督学习相似，在监督学习中，通常假设数据独立同分布，或者比这更复杂。然而，在这种情况下，我们的数据更接近，因为我们只有一个策略。

当我们开始改变政策时，并不意味着局势不稳定。现在，我们有三种近似方法：函数逼近、引导采样，但大多数情况下，我们的政策仍会趋于稳定。那么，这是什么意思呢？我们可以再次考虑类似于监督学习的方法。我们希望只有我们的状态，然后通过“Oracle”来告诉我们价值是多少，并适应我们的函数，而不是直接拥有Oracle。这时，我们会使用RTD来估计。因此，我们将使用我们的词加上Gamma乘以下一个状态的近似值。这将形成我们对真实价值的估计。接着，我们会找到权重以最小化该设置中的均方误差。因此，如果我们这样做，在线性情况下，我们可以写出这个TD目标。简单提一下，我经常会用到“TD零”这个术语。虽然我们还没在本课上讨论过它，但实际上有许多不同的TD变体，通常被称为TD Gamma。

因此，如果您正在阅读这本书，可能会有点困惑，所以我只想澄清一下，我们正在使用 TD(0) 变体，这可能是最受欢迎的，并且还有很多其他变种。为简单起见，我们现在只关注 TD(0)。所以，这就是 TD 目标。这是我们当前的估计，然后我们求导数。在这种情况下，这意味着我们最终将插入当前状态和下一个状态的线性值函数逼近器，并查看由特征向量加权的差异。因此，它看起来应该与蒙特卡洛更新几乎相同，除了现在我们正在引导这一事实。因此，这不是之前在特定剧集中看到的回报 G，现在我们正在引导，得到了立即奖励加上使用的奖励折扣总和的估计。值函数逼近器进行估计。因此，这就是策略评估算法的 TD 学习线性值函数近似的样子。我们将再次初始化我们的权重向量。

我们将对一个元组进行采样，然后更新我们的权重。因此，现在我们可以在每个元组之后更新权重，就像我们在TD学习中看到的那样。在这里，我们可以看到，我们只需插入我们的特定估计减去旧估计的时间X。让我们看看在Baird示例中这是什么样子。所以，我们再次拥有与之前相同的状态特征表示。第一个状态是200001。我们仍然有零来表示任何其他位置。让我们将学习率alpha设置为0.5。现在我们处于这种情况，或者可以说不存在最终状态，因为强化学习可以处理持续的在线学习。因此，我们假设在动作A下S7始终保持S7。因此，A1是实线，A2是虚线。我们将权重初始化为1111。然后让我们看看这个元组。

就像我们前面讨论的第一个元组一样，假设我们目前处于状态一。我们选择了动作 A1，当我们转移到状态S7时，所获得的奖励是零。那么，为什么不花点时间计算一下更新后的新权重呢？或许可以将其与蒙特卡洛案例进行比较，看看它们发生了多大变化。同时，随时和邻居交流。让我们放大一下这个情况，这样记住S7就更容易了。好的，他们搬走了很多东西还是只是少部分？

在我们处理蒙特卡罗时间差分方法时，关注的是权重的变化。 根据一些观察，权重的变化比较小。 这是正确的。 因此，对于状态S的价值乘以权重W，结果仍然是3。 对于状态S的价值，这个值是7。 让我们查一下这是什么。 这也将是3。 但现在我们要计算的是，ΔW等于α乘以（0加上0.9乘以3减去3）。 所以，这将等于α乘以-0.3。 所以，请记住之前实际上是-3，所以这是一个更大的更新。 因此，当我们将这个更新添加到我们的新权重中时，我们会调整我们的权重，但是调整比之前看到的要小得多。 这应该不会太令人惊讶，这与我们在蒙特卡罗更新和TD学习中观察到的一致。

TD学习是指仅更新一些小的局部变化，比如一个状态动作或下一个最高状态的单词。与蒙特卡洛方法不同，它是根据每个完整的情节回合来更新的，而不是一步步引导。因此，实际上是从状态开始进行回归。初始状态S1的回报值为零，但在更新过程中我们可以将其估计值设定为接近3，比如2.7。这样，当我们将权重向这个方向移动时，与在蒙特卡罗方法中观察到的差异会小得多，类似于没有函数逼近器时的情况。即使在这种情况下，TD学习的理论特性也相当不错。因此，当使用TD学习时，权重会根据零估计值收敛，这些权重可能不如蒙特卡罗方法那么准确，但它们在一个常数因子内。所以，它们将以一个比例减去伽玛值的最小值。尽管它们不及蒙特卡罗方法，但也相当不错。

折扣因子和可能的函数逼近器在好处方面有所不同。为了澄清我们的理解，我列出了以下两个结果。有人认为蒙特卡罗策略评估器收敛到线性值函数逼近器下可能的最小均方误差，而TD 零 收敛到该最小误差的一半减去 Gamma 的范围内。如果您可以选择任何线性价值函数逼近器，那么这个最小误差是多少呢？它在代表您的政策的真实价值方面能有多好呢？让我们再花一分钟时间，这是一个很好的与邻居讨论的话题。如果价值函数逼近器是表格表示，那么蒙特卡罗和TD的均方误差是多少？我们保证收敛到最优解，即π的V的真实值是否最优，如果您有任何问题，请随时提问。

在表达时，您是否意味着限制了系统的表现能力？比如，如果我说上周的会议是以表格形式呈现的，我指的是什么呢？我的意思是每个情况都有一个特征，就像一个独热编码。因此，这就类似于我们之前课程中见过的相同表达形式，针对每种情况，您都有一个表格进行数值查找。对吗？您能解释一下吗——而且，您的名字呢？您可以解释一下桶的主要组成成分是什么吗？就像，如果别人想要了解的话。啊，这是个好问题。所以，“TD0”，我们在讨论的一切都与TD和TD0有关。我们现在讨论的都是关于TD0的。我选择这种描述是因为TD有多个版本。如果您查看其他资料，有时也会出现“TDLambda”。所以我只是确保清楚明了。这样，无论您阅读哪个资源，您也会知道应该了解的是哪个版本的TD。好的。好的，给你第一个问题。

当我们使用表格来表示时，我们能准确地表达政策的价值吗？是的，我们可以。对于世界上的每个国家，你可以获取不同的表格来代表不同的策略的价值。虽然实际上不可能为每个国家提供这样的表格，但你可以准确地代表政策的价值。如何实现呢？你可以简单地运行该策略对每个州。通过蒙特卡罗返回一个平均值，这将为您提供该州的真实价值。因此，您可以通过在每个表中表示预期的折扣回报总和来达到这一目的。换句话说，这意味着误差为0，因为您的表格足以代表这个价值。

让我们出发吧。所以我们看到的是期望值，对吧，你的函数之间的差异实际上达到了 0，但就像任何图表都会有一点不同。所以，我-我预期是0，但在任何意外情况下，它都是不同的。在这种情况下，如果你有一个表示表格，并且这是有限的，因此有无限量的数据等等，那么这将是对于每个状态来说这都是 0。所以对于每个状态来说这都等于 0。如果您使用表格表示，您将收敛到每个状态的正确值。那是因为，如果你认为拥有无限量的数据并且运行你的策略，你知道，无限次，那么对于每个状态，你都有从该状态开始的无限数量的轨迹，你可以在表中单独记下该值。所以它将是 0。这意味着如果您使用表格表示，蒙特卡洛估计器的均方值误差为 0。因为它是 0，所以它与

TD 的均方值估计量除了一个地方，与蒙特卡罗的均方值估计量完全相同。这个地方恰好等于蒙特卡罗的均方值估计量乘以一减去 Gamma。因此，这个值也等于 0。所以，如果以表格的形式表示，只需将其连接回该值，嗯，并不是-这些方法都没有时间差异。有问题吗？首先是你的名字。我？是的。嗯，我在想一比一负 Gamma 常数是从哪里来的？是的，问题是：一比一负 Gamma 常数是从哪里来的？嗯，考虑到时间，我不会多说。我鼓励你阅读齐齐克利斯的论文。嗯，直观上，这里存在一个误传误传的错误，因为我们正在引导学习，所以如果你有一个函数，这个结论试图强调的是，如果你的函数逼近器没有误差，那么蒙特卡罗和 TD 之间就没有区别，因为它们在平方误差方面是相同的。

w的最小值为0。因此，无论您使用TD还是蒙特卡罗，都会得到正确的结果。但如果不是这样，比如如果无法准确表示价值函数，将会出现错误，并且这些错误会不断累积。这种积累是因为您正在更新数值，将错误传播回去，而蒙特卡罗则不会受到这种影响。总的来说，均方误差已经涵盖了状态分布，但是根据您关注的政策会有所不同。在特定政策下，我们只看到固定分布。您尝试过其他的吗？这表明您正在固定分布下进行观察，在此政策下，您将到达这些状态。这是一个正确的选择，因为这些状态是根据这一政策即将到达的状态。如果您开始考虑控制，您可能需要其他方法，比如如果您打算更改政策。

好的，让我们简单介绍一下这个话题。哪个更快？哪个更好？据我所知，这还没有得到确切的解答。如果您找到了相关文献，我会很乐意了解更多。通常情况下，自举法（Bootstrapping）比较好，在实践中，自举法通常有助于提升效果。让我们继续进行控制。这会非常类似。所以，我们不再只表示值函数，而是表示状态-动作值函数，这是我们在从策略评估转向控制时经常看到的情况。现在我们要做的是将策略评估与值函数逼近器结合在一起，并执行类似贪心策略改进的操作。这时开始变得不稳定。在这种情况下，我们该怎么做？通常涉及函数逼近、自举，也经常涉及采样。但实际上，真正的问题似乎出现在策略学习阶段。但是考虑到我们以前曾有一个很好的平稳分布或者收敛到状态的平稳分布时，我们不再这么做，因为我们将使用。

随着时间的推移，我们改变控制策略，从而改变所遇到的状态分布。设定一个称为“致命三合会”的标准。如果愿意，开始结合函数逼近、引导和离策略学习，事情会变得有些复杂，可能导致收敛困难或无法收敛到良好的结果。在开始讨论之前，先从程序角度考虑一下。

我们使用参数化为W的Q函数，可以再次进行随机梯度下降，几乎类似之前的方法。随机梯度下降可以对梯度进行采样，针对特定的状态动作对进行更新。通过一组线性状态动作权重来表示我们的Q函数，这意味着我们将对状态和动作进行编码，就像机器人左转时看到的那样。这是这两者的结合。

一旦有了这个，我们就会在 Q 上面有一个权重向量。这样，我们就不会为每一个动作都有一个单独的权重向量，而是会尝试将状态和动作整合到一起。然后，我们可以基于此执行随机梯度下降。那么，针对蒙特卡洛方法，它是如何工作的呢？看起来与之前几乎完全相同。我们将再次使用回报。现在，我们要定义特定状态-动作对的回报。对于第一次访问，当我们第一次到达某个状态-动作对时，我们会查看回报，即直到该情节结束时的奖励总和，并将其作为我们的目标。将其用作对我们的预测模型、真实 Q 函数的估计，然后对其进行更新。在 SARSA 中，我们将使用 TD 目标，因此我们会查看即时奖励加上我们遇到的下一个状态的折现乘以 Q 以及我们选择的动作。因此，我们将再次更新它。

接下来讨论的是关于 Q 学习的内容，它与之前的 Q 学习非常相似，不同之处在于我们将使用函数逼近器。在这里，我们将插入一个函数，即对状态和动作的函数进行近似。所有的计算都是线性的，我们只是在执行不同形式的引导并比较不同动作状态的值，以确保选择最大值。尽管我浏览得比较快，但这部分基本上与我们更仔细讨论的第一部分非常相似。目前为止，我们一直在使用 Q 函数。为什么这一点会变得复杂或棘手呢？因为带有值函数近似的时间差分学习并不是按梯度下降的方式进行更新的。今天我没有时间详细探讨这一点，但第 11 章中有很好的解释。因此，对于那些对此感兴趣的朋友，Umberto的第 11 章是一个很好的资源，我们也可以在线获取相关讲义。概括地说，我们现在正在讨论一种非常类似的方法。

在进行交替或近似样本贝尔曼备份以及常规所谓的投影步骤时，我们试图将我们的价值函数回归到能够表示该函数的空间中。直观地讲，为什么这可能成为一个问题呢？因为我们展示的贝尔曼运算符是一个收缩。就像我们在动态规划中所做的，如果您执行贝尔曼更新 - 即备份操作，您会收敛到一个固定点。但是当您使用值函数近似器时，它可能是一个膨胀。膨胀是什么意思呢？那么，什么是收缩呢？提醒一下，收缩是指什么？让我们看看操作符，具有收缩性。如果您应用这个运算符，这类似于贝尔曼方程的运算符。我想说，如果您将它应用于两个不同的价值函数，那么这两个函数之间的距离会感觉就像一个最大范数或者类似的东西小于或等于之前的距离。这意味着当您应用此运算符时，旧的价值函数和新的价值函数之间的距离不断减小，并最终达到一个固定点。现在的问题在于我们不再这样做了。

这段内容似乎是关于值函数逼近器以及投影算子的讨论。作者提到了对值函数进行投影运算后可能会出现值函数不再在逼近器空间内的情况，需要将其调整回原空间。此时，投影算子本身可能会是一个扩展。作者提到了Jeff Gordon在1995年的一篇关于线性值函数逼近器扩展的论文，探讨了贝尔曼备份和距离扩大的问题。接着作者提到了从控制案例的角度考虑值函数逼近器的政策设置。

在评估第一个政策时，您一直遵循实线。因此，您总是选择A1。在您的行为数据中，您选择A2的时间占了七分之六，选择A1的时间占了七分之一。伽玛值为9.9。您的任务是生成大量数据。因此，您生成数据，这些数据基于您的行为政策。因此，有一些非常酷的工作涉及如何处理您获得的数据与您想要评估的数据之间的不匹配。让我们设想，我们不涉及任何我认为很酷的事情，而是只做一些非常简单的事情，即丢弃所有不匹配的数据。因此，想象一下，如果A不等于S的Pi，那么您将丢弃数据。

您生成了所有这些数据点。那么，这些数据点指的是什么？我们以 SAR 为首要。因此，您收集了所有这些元组。如果实际操作与您要评估的策略不同，但您只采取 A1，那么只需要丢弃该元组，无需更新。因此，现在您剩下的所有数据都符合您的政策。那么，让我们设想一下，您尝试使用这些数据进行 TD 学习。问题是，您可能会发散。这是什么意思呢？这意味着，您的权重可能会爆炸增长。为什么会发生这种情况非常有趣。嗯，主要的直觉是您的数据分布与在目标策略下获得的数据不同。特别是，如果你要运行策略 Pi 排出会发生什么？假设您从状态 S1 开始。您选择动作 A1。这会使您进入第七状态，但您会在第七状态停留很长时间，因为这是一个确定性状态。因此，您会一直在 S1 和 S7 之间来回。举例来说，即使您这样做了，也许您...

有些可能是不确定的，但如果你遇到这种情况并且有多个情节，你希望拥有少量关于某个情节 S 的数据，同时又有大量关于情节 S7 的数据。然而，在你得到的数据中，由于行动 A2 需要大量时间，它会不断地将你带回到 S1 到 S6 中的一个情节。这导致你拥有的数据呈现出非常不同的分布。你所访问的状态分布与你获取的数据、以及在政策 Pi 下访问的状态之间存在很大的不同。这就是问题所在。如果你不考虑这种不匹配，那么这些值可能会有所不同。尽管它们在一定程度上是兼容的，但从某种意义上说，如果您根据所需的策略采取了操作，那么您会受到状态操作对的影响。当您使用 Q 学习时，这种问题也可能会出现。通常随着时间的推移，您会更新此策略。因此，简而言之，总结一下这一点。在表格情况下，一切都会收敛，这看起来很美好。在线性情况下，...

在AI领域，有一些概念，比如Q学习和函数逼近，是在不断被探讨和研究的。对于这些概念，人们致力于寻找算法能够在非线性近似的情况下做出收敛保证。虽然有些算法在这方面已经取得了进展，但依然存在很多挑战和工作需要继续努力。因此，对于这些问题的研究至关重要，不仅要考虑算法是否收敛，还要考虑收敛结果的质量，避免收敛到不理想的近似值。

事实并非如此，你的权重并没有爆炸，但这只是一个非常糟糕的近似器，关键在于你的目标函数和特征表示。在我们结束之前，我认为萨顿和巴托提到的这个观点非常有见地。可以将其想象成一个平面，你可以在其中表示所有的线性值函数逼近器。当你进行贝尔曼更新或者进行TD备份时，你将得到一个可能无法在平面中表示的价值函数，然后你可以将其投影回来。这种方法允许你量化不同形式的误差，从而找到最佳的近似器。虽然我们今天看到了一种类似于最小均方误差的近似方法，它类似于贝尔曼误差，但并非是唯一的选择，也不一定是最佳的选择。在实际问题中，误差最小的那个可能并不是性能最佳的。

Shane 讲解的内容涵盖了萨顿和巴托的 11 点要点。如果你想深入了解更多细节，那么你需要在线性价值函数逼近器上实现这些，并进行策略评估和控制。你需要了解在策略评估情况下事物是否收敛，以及何时解决方案达到零误差和非零误差。此外，你应该对可能出现的问题有定性的了解，因为其中一些解决方案可能并不总是收敛，这涉及函数逼近的引导和所有策略学习的结合。这些知识足以让我们着手完成这周发布的作业二。接下来的一周我们将开始深化讨论。谢谢。
