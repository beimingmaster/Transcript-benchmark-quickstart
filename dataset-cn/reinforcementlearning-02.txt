Stanford CS234: Reinforcement Learning | Winter 2019 | Lecture 2 - Given a Model of the World - YouTube
https://www.youtube.com/watch?v=E3f2Camj0Is

Transcript:
(00:04) 好的。 所以，上次我们开始谈论 强化学习所涉及的一般概述， 嗯，我们引入了模型、值和策略的概念[噪声] 。 [噪音] 嗯，所以现在就让你的大脑清醒一下， 这三件事是什么，这很好。 有人能立刻记住 强化学习背景下的价值、模型或政策是什么吗？ [噪音] 嗯，所以政策是一组行动， 呃，代理应该在工作中采取 [噪音]。 [噪音] 完全正确。 因此，策略的定义是从 您所处的状态到要采取的操作的映射。 这可能是一个好政策，也可能是一个坏政策。 我们评估的方式 是根据其[噪音]预期奖励折扣总和。 有人记得模型是什么吗？ 是的？ 模型就像是，呃， 世界的代表，以及世界如何随着特工的事故而变化。 [噪音] 是的。 是的，所以通常我们会想到一个模型，该模型结合了奖励模型， 或者决策，呃，或者动态模型， [噪音]，它指定了对当前状态的响应，以及，呃，
(01:09) 世界可能如何改变的行动 ， 可以是随机模型或确定性模型。 [噪音] 嗯，奖励模型指定了 代理在特定动作中采取某种状态所获得的预期奖励是什么。 [噪音] 所以我们今天要讨论的是，嗯， 思考一下，如果你知道世界的模型， 那么，你知道，嗯， 如果你在特定状态下采取行动会发生什么， 或者会发生什么 下一个状态的分布可能是如果你[噪音]采取行动， [噪音]嗯，我们应该如何做出决定。 那么，我们该如何做规划问题呢？ 所以，今天我们不谈学习。 我们只是要讨论弄清楚什么是正确的事情要做的问题， [噪音]当你的行为可能会产生延迟的后果时， 这意味着你可能必须牺牲 即时奖励才能最大化长期奖励。 [噪音] 正如我们刚才所说，嗯， 我们通常会考虑的模型是 动态和奖励函数的统计或数学模型。 嗯，政策是一个函数，它将学生的每个， 呃，呃这些代理的状态映射到行动， 而价值函数作为奖励的预期贴现总和，嗯， 来自于一个状态，嗯，
(02:15) 和/或一个行动， [噪音]，然后遵循特定的政策。 [噪音] 所以我们今天要做的是， 嗯，建立马尔可夫过程， 嗯，直到马尔可夫决策过程。 我认为这个构建 是一个不错的构建，因为它可以让人们 思考在你 可能无法控制世界但世界可能仍在以某种方式演变的情况下会发生什么。 [噪音] 嗯，想想对于 一个被动体验世界的代理来说，这些过程中可能会得到什么回报。 嗯，然后我们可以开始考虑控制问题，即智能体应该如何 选择在世界中采取行动，以最大化其预期奖励折扣总和。 [噪音] 所以，我们今天要关注的，呃， 以及大多数其他课程的重点是马尔可夫决策过程， 嗯，我们考虑一个与世界交互的代理。 因此，代理可以采取行动， 通常用 [噪音]表示，这些行动以某种方式影响世界的状态， 然后代理收到一个状态和奖励。 所以上次我们谈到了这样一个事实：这实际上可能 是一种观察，而不是一种状态。
(03:20) 但是，当我们考虑马尔可夫世界时， 我们会[噪音]想到一个代理， 只关注当前状态，嗯， 所以最近的观察，就像，你知道， 无论机器人是否 激光测距仪说，在 它的左边或右边有墙， 而不是考虑所 采取的行动序列和收到的观察的先前历史的完整序列。 [噪音] 嗯，正如我们上次谈到的， 但你总是可以结合 [噪音] 完整的历史来制作一些马尔可夫， 嗯，[噪音] 但今天的大部分时间， 我们都会考虑，有点，直接的 传感器。 如果不清楚，请随时联系。 [噪音] 那么，马尔可夫过程是什么意思？ 马尔可夫过程是说， 智能体用来做出决策的状态 是历史的充分[噪声]统计量。 [NOISE] 这意味着为了预测 下一个时间步的未来状态分布。 这里我们使用 t 来表示时间步长。 [噪音] 给定我们当前的状态 s_t， 以及采取的动作 a_t， [噪音] 这又是这个动作， [噪音] 嗯，这相当于，
(04:25) 如果我们实际上记住了整个历史， 其中 历史回忆将是 之前所有行动和奖励的顺序。 接下来说明我们到目前为止所看到的情况。 [噪音] 从本质上讲，它让我们可以说， 鉴于 当前的一些总体统计数据，未来独立于过去。 [噪音] 因此，当我们考虑马尔可夫过程或马尔可夫链时， 我们还没有想到存在任何控制。 没有任何动作。 嗯，但这个想法是， 你可能有一个随着时间的推移而演变的随机过程。 [噪音] 嗯，所以无论我是否投资股票市场， 股票市场都会随着时间而变化。 你可以将其视为马尔可夫过程， [噪音]嗯，所以我可以只是被动 地观察特定股票的股票市场如何 随时间变化。 [噪声] 嗯，马尔可夫链 就是随机状态的序列， 其中过渡动力学满足马尔可夫特性。 正式来说，马尔可夫过程的定义是，
(05:29) 嗯，一组有限或可能无限的状态。 并且您有一个动态模型，它 指定给定前一个状态的下一个状态的概率。 [噪音] 没有奖励，也 没有任何行动。 嗯，如果你有一组有限的状态， 你可以把它写成一个矩阵。 只是一个转换矩阵，表示 您正从某个状态开始。 您可以达到的下一个状态的概率分布是多少？ [噪音] 那么，如果我们回到上次讨论的火星漫游者的例子。 [噪音] 嗯，在这个火星漫游车的小例子中， 我们想到了火星漫游车在火星上着陆 ，可能有不同类型的着陆点， 嗯，所以也许我们的火星漫游车从这里开始。 然后，它可以向左或向右，嗯，呃， 在不同的动作下，或者我们可以将这些动作视为 a_1 或 a_2， 它试图在世界上采取行动。 [噪音] 嗯，在这种情况下， 呃，过渡动态，它没有， 我们实际上还没有采取行动， 我们只是认为它是，有点， 也许它已经有某种方式， 它正在移动 在世界上，电机只是在工作。 [噪音] 所以在这种情况下， 过渡动态看起来像这样，
(06:32) 例如，你 可以这样读，你可以说，好吧， 我从特定状态 s_1 开始的概率，嗯， 然后，我可以在下一个时间步长为 0.4 时过渡到下一个状态。 [噪音] 我有 0.6 的机会在下一个时间步保持相同的状态。 是的？ [噪音] 嗯，哪个维度代表开始状态？ 嗯，所以，这是一个很好的问题。 哪个维度、哪个、哪个状态是起始状态？ [噪音] 我没有在这里具体说明。 嗯，呃，一般来说，当我们考虑马尔可夫链时， 我们会考虑查看它们的稳态分布。 因此，如果运行时间足够长，它们的平稳分布将[噪声] 收敛到状态上的某些分布， [噪声]独立于起始状态 。 哦，抱歉，我想问， 在那个矩阵上， 哪个维度代表 - 哦，你的意思是，你现在在哪里？ 是的。 因此，在这种特殊情况下， 你可以将其视为，嗯， 如果你从状态开始， [噪音]呃，让我确保我做对了。 在这种情况下，[噪音]在那里回答， 所以如果你从这里开始， 嗯，所以这是你在状态 s_1 的初始开始，
(07:38) 然后你用它的点积， 我可能有-让我 看看我在混合方面是否正确。 它要么在一侧，要么在另一侧， 然后，我可能已经转换了它。 嗯，我想你必须为这里的[噪音]另一侧这样做。 是的，会被翻转。 所以，你就会有你的初始状态。 所以 1, 0, 0, 0, 0, 0, 1, 2, 3, 4, 5, 6，然后乘以 P， 这将给你下一个状态 分布 s'。 是的？ [噪音] 嗯，嗯，那么计算的概率是多少， 比如奖励，我猜是 基于从状态 1 到状态 2 [噪音] 的奖励的概率？ 很好的问题，所以，你知道， 这个转换概率之一是 考虑[噪音]，这与他们的话有关，在这种情况下， 我们只是考虑马尔可夫链， 所以还没有奖励，也没有任何行动。 [噪音] 嗯，这只是指定进程的某种状态 。 所以就好像你， 假设你的代理，嗯， 对其电机进行了一些配置。 [噪音]你不知道那是什么， 它被安置在火星上，然后它就开始移动。 这就是说， 这是如果
(08:42) 该代理在状态下启动的转换概率，我可以这样写。 因此，如果它以状态 [NOISE] s_1 开始，那么它保持在状态 s_1 的概率为 0.6。 因此， 由于该机器人的电机配置不同，您在下一个时间步骤中仍处于该状态的概率为 0.6。 [听不清] 世界运转。 这表明， 这就是世界的运作方式。 这是一个很好的问题。 所以我们现在假设，嗯， 这个马尔可夫过程是你所处的世界的一种状态， 有一些， 你所处的环境被描述为马尔可夫过程， 这描述了 该过程的动态。 我们不是在谈论你如何估计这些。 这真的就好像，这就是那个世界的运作方式。 这，这，这， 这就是假小火星车的世界。 [噪音] 我们对此有任何疑问吗？ 是的？ 呃，[噪音]当你乘以 P 时，血清一云作用需要 转置[噪音]，我们可以看到的所有[噪音]都是[听不清] 是的。 是的。 [听不清] [噪音] 让我写下并纠正矢量符号。 会是这样的。
(09:48) 一、一、二、三、四、五、六。 也就是说，这将是您可能处于的示例起始状态。 所以，这可能是你的初始状态。 初始状态，这意味着您的代理最初处于状态 S 1。 好的。 然后，如果您想知道它可能在下一个状态上的位置， 您可以将其乘以转换模型 P，具体 取决于符号以及是否采用此转换模型的转置， 它将在左侧或右侧。 从上下文来看它应该是显而易见的，但如果不清楚，请 随时询问我们。 那么这会说明什么呢？ 这就是说，如果你采用 这个向量的矩阵乘法，这只是 说你从状态 s_1 开始，那会是什么样子？ 之后它会说你处于状态 s_1 的概率仍然是 0.6， 你处于状态 s_2 的概率可能是 0.4。 这将是您的新（状态分布）。
(10:51) 我认为应该换位思考。 但这只是一个它，它指定了 您将进入的下一个州的分布。 您可能对此有任何疑问吗？ 好的。 好的。 所以，这只是指定 世界如何随着时间的推移而运行的转换模型，而这只是我， 我用矩阵表示法将其编写为紧凑的。 但如果考虑起来更容易，那么就 可以根据 给定前一个状态的下一个状态的概率来考虑它。 因此，您可以枚举这些， 如果状态数量恰好是有限的，则可以将其写成矩阵形式。 因此，如果您想考虑 在这种情况下代理随着时间的推移可能会发生什么，或者过程可能是什么样子， 您可以只对情节进行采样。 所以，假设你的初始起始状态是 S 4， 然后你可以说，好吧， 我可以把它写成一个单热向量。 我将其乘以我的概率。 这给了我一些 我可能处于的下一个状态的概率分布，并且世界将采样其中一个状态。 因此，您的代理不能同时处于多个状态。 因此，例如，如果我们查看状态 s_1，
(11:56) 它有 0.6 的机会在 s_1 中弃权，或者有 0.4 的机会进行转换。 因此，世界将为您采样这两种结果之一，它可能是状态 s_1。 所以在这种情况下，我们有与 s_4 类似的动态。 从 s_4 到状态 s_3 的概率为 0.4。 进入状态 s_4 的概率为 0.4， 或者停留在同一位置的概率为 0.2。 因此，如果我们要对代理随着时间的推移可能发生的情况进行采样， 您可以从 s_4 开始，然后可能会过渡到 s_5。 也许他们会去 s_6、 s_7、s_7、s_7。 因此，您只需从该转换矩阵中采样即可生成特定的轨迹。 所以，就像这个世界，你知道 世界的动态是什么，然后大自然会选择其中一个结果。 这就像从某种概率分布中抽样。 有人对此有疑问吗？ 好的。 所以，这只是给你一个特定的情节。 我们将对剧集感兴趣，因为稍后我们将 考虑这些剧集的奖励，以及 如何比较我们在这些剧集中可能获得的奖励，但就目前而言， 这只是一个过程。
(13:02) 这只是为您提供一系列状态。 那么，接下来我们要添加奖励。 所以，这只是一个马尔可夫链。 那么现在什么是马尔可夫奖励过程？ 同样，我们还没有像以前一样采取行动。 但现在我们还有一个奖励函数。 所以，我们仍然有一个像以前一样的动力学模型。 现在我们有一个奖励函数，它表示， 如果你处于特定状态，那么 你从该状态中获得的预期奖励是多少？ 我们现在还可以有一个折扣因子，它允许我们在 即时奖励与功能奖励之间进行权衡，或者让我们思考我们权衡多少。 因此，再次像以前一样， 如果我们有有限数量的状态，在 这种情况下，R 可以用矩阵表示法表示，它 只是一个向量，因为它只是我们处于每个状态时获得的预期奖励。 因此，如果我们查看火星漫游者 MRP， 那么我们可以说，成为 s_1 的奖励等于 1。 成为 s_7 的奖励等于 10，其他奖励为零。 是的。
(14:07) 这些词总是与你所处的状态相关吗？ 我想你上次谈到它也有一个选择。 那么，我们为什么不在这里考虑这一点呢？ 很好的问题。 我是说，我上次提到过， 马尔可夫决策过程的奖励可以是状态、 行动中的状态或下一个状态的状态行动的函数。 目前我们仍处于马尔可夫奖励流程中，因此没有采取任何行动。 因此，在这种情况下， 您定义奖励的方式要么 是当前状态，要么是状态和下一个状态。 因此，一旦我们开始考虑奖励， 我们就可以开始考虑回报和预期回报。 因此，首先让我们定义什么是地平线。 地平线只是一个情节中的时间步数。 所以，这有点像代理的作用时间有多长，或者 这个过程持续了多长时间，而且可能是无限的。 因此，如果它不是无限的， 那么我们将其称为有限马尔可夫决策过程。 上次我们简要讨论过这些。 嗯，但我们经常会想到这样的情况， 嗯，一个特工可能会永远行动，或者这个过程可能会永远持续下去。 它没有终止的可能。
(15:12) 今天股市上涨。 明天就起来了 我们预计它会持续很长时间。 我们不一定会尝试在短时间内对其进行评估。 人们可能想考虑在 很长一段时间内对其进行评估。 所以，我们已经做到了这一点。 回报的定义只是 从当前时间步长到某个范围内获得的奖励的贴现总和，并且该范围可能是无限的。 所以，回报只是说， 如果我从时间步 T 开始， 我得到的直接奖励是多少，然后我可能会过渡到 一个新状态，然后我用 Gamma 来衡量回报奖励。 然后我再次进行转换，并用伽玛平方来衡量该值，等等。 那么价值函数的定义就是期望回报。 如果过程是确定性的，那么 这两件事将是相同的。 但一般来说，如果过程是随机的，它们就会有所不同。 所以，我所说的确定性的意思是，如果你总是进入相同的下一个状态， 无论你从一个状态开始，
(16:17) 如果你只能进入一个下一个状态， 呃，那么期望就相当于一个 返回。 但在一般情况下，我们将对 这些随机决策过程感兴趣，这 意味着平均值将与特定运行不同。 因此，对于这方面的一个例子， 让我首先谈谈折扣因子，然后我会举一个例子。 折扣因素有点棘手。 它们都有一定的动机，并且在一定程度上是为了数学上的方便而使用的。 因此，我们稍后会看到数学上的好处之一，呃， 折扣因子在数学上的好处是，我们可以确定， 只要这里的奖励函数是有界的，预期贴现收益总和的价值函数就是有界的。 呃，根据经验，人们常常表现得好像存在折扣因素。 通常， 我们对未来奖励的权重低于 当前奖励。 企业也经常这样做。 如果 Gamma 等于 0，则 您只关心即时奖励。 所以，你这个代理人的行为是短视的。 它没有考虑以后可能发生的事情的未来。 如果伽玛等于一，
(17:20) 那么这意味着你未来的奖励 与当前的奖励对你来说完全一样有益。 现在，需要注意一件事， 如果您只是为了数学方便而使用折扣因子，嗯， 如果您的范围始终保证是有限的，那么 从数学方便的角度来看，使用等于 1 的 gamma 是可以的 。 有人对折扣因素有任何疑问吗？ 是的。 我的问题是，伽玛的折扣因子是否总是必须以 几何方式进步，或者我们这样做有什么原因吗？ 这是一个很好的问题。 你知道， 我们在这里定义的是，使用 通过这种指数几何方式进展的伽玛是必要的。 这是一个不错的选择，最终具有非常好的数学特性。 在那里，人们可以尝试使用其他参与者当然是 最常见的一种，稍后我们将看到 为什么它具有一些非常好的数学属性。 还有其他问题吗？ 好的。 那么，有哪些例子呢？
(18:24) 嗯，如果我们回到我们的火星漫游者这里，我们现在有了奖励的定义， 嗯，样本回报是多少？ 因此，假设我们从状态 s_4 开始，然后过渡到 s_5、 s_6、s_7，并且只有四步返回。 所以，这意味着我们的，嗯， 我们的过程只持续四个时间步骤，然后它可能会重置。 那么，为什么这样的事情可能是合理的呢？ 嗯，特别是当我们开始做出决策时，嗯，你知道， 也许客户与网站的互动平均有两到三次。 嗯，通常你知道有限长度的时间是有限的，当然 在很多情况下，地平线自然是有界的。 因此，在这种情况下，您知道在我们从 s_4 开始的这种情况下可能会发生什么。 根据定义，s_4、s_5、s_6 的奖励均为零。 嗯，然后在时间步 s_7 上，我们得到的奖励为 10。 但这必须受到折扣因子的影响，这里是 1/2。 因此，它是 1/2 的 3 次方。
(19:28) 因此，该特定剧集的样本回报仅为 1.25。 [噪音]当然，我们可以为任何特定的，嗯，剧集定义这一点， 并且这些剧集通常可能会经历不同的状态， 即使它们以相同的初始状态开始， 因为我们有一个随机转换模型。 因此，在这种情况下，代理可能只是停留在 s_4、 s_4、s_5、s_4 中，并且不会获得任何奖励。 在其他情况下， 嗯，它可能会一直向左移动。 因此，如果我们考虑期望值函数是什么， 它将涉及对许多函数进行平均。 当我们对所有这些进行平均时，嗯， 然后我们可以开始在不同的时间步长获得不同的奖励。 那么，我们如何计算呢？ 嗯，现在你可以做的一件事是 你可以通过模拟来估计它，这有点受我之前展示的启发。 所以，你可以，嗯， 假设一个初始起始状态分布，嗯，
(20:36) 它可以只是一个起始状态或 多个起始状态，你可以直接推出你的流程。 所以，现在我们假设我们有 一个转换模型转换矩阵和一个奖励模型。 嗯，您可以像 我们在前几个时间步骤中展示的那样将其推出。 你可以这样做很多很多很多次。 然后平均。 这将渐近 收敛到价值函数，因为价值函数只是， 嗯，预期回报。 因此，您可以通过模拟做一件事，嗯， 您可以使用数学界限来 表示您需要进行多少次模拟 才能使您的经验平均值接近真实的预期值。 准确度大致下降到 N 的平方根分之一，其中 N 是您已完成的转出次数。 所以，它只是告诉你，你知道， 如果你想弄清楚你的马尔可夫奖励过程的价值是多少， 嗯，你可以做模拟，这会给你一个价值的估计。 这样做的好处 是不需要假设马尔可夫结构。
(21:40) 实际上根本没有利用这是马尔可夫奖励过程这一事实。 这只是一种估计回报总额的方法——总结奖励。 因此，从某种意义上说，这都很好，嗯， 如果您在根据某些数据估计的过程中使用它， 或者您假设事情是，呃，嗯， 您知道这是动态模型 但这也是根据 数据估计的，它可能是错误的，嗯， 那么这可以给你一种，嗯， 如果你真的可以在世界上推广，那么你可以得到这些 关于该过程真正如何运作的很好的估计。 但它并没有利用这样一个事实：如果世界真的是马尔可夫， 嗯，我们可以做额外的结构以获得更好的估计。 那么，更好的估计是什么意思呢？ 我的意思是，如果我们想要，嗯， 获得某种更好的意义，一种计算上更便宜的， 嗯，估计价值的方法，那就是一个过程。 因此，马尔可夫结构允许我们做的事情是， 鉴于现在，未来独立于过去， 它允许我们分解价值函数。 因此，标记前向过程的价值函数只是 智能体从当前状态获得的即时奖励
(22:46) 加上由折扣因子加权的未来奖励的折扣总和乘以 - 并且我们将未来单词的折扣总和表示为 我们 可以用 V, V(s') 来表示。 所以，我们可以说，无论你现在处于什么状态， 你都会立即得到指示，然后你就会 过渡到某种状态。 嗯，然后你将得到 你最终根据我们的折扣因子折扣的任何州的价值。 因此，如果我们处于有限状态 MRP，我们可以使用矩阵符号来表达。 因此，我们可以说作为向量的价值函数等于 奖励加 gamma 乘以转换模型乘以 V。再次注意，在这种情况下，由于我们定义转换模型的方式， 嗯，那么值 这里的函数将 转换模型定义为给定前一个状态的下一个 [NOISE] 状态，并将 其乘以那里的值函数。 因此，在这种情况下，我们可以仅使用矩阵符号来表达它。
(23:54) 嗯，好的事情是，一旦我们完成了， 我们就可以分析求解价值函数。 所以，记住所有这些都是已知的。 所以，这是已知的。 这是众所周知的。 我们要做的是计算 V(S) 是什么。 所以，在这种情况下我们能做的就是把它移到另一边。 所以，你可以做 V 减去 gamma PV 等于 R，或者我们可以 说单位矩阵减去折扣因子乘以 P。这些都是矩阵。 因此，这是单位矩阵乘以 V 等于 R，这意味着 V 正好等于该矩阵的逆矩阵乘以 R。Um。 那么，如果其中一个转换可以回到自身， 那么尝试用 V(s) 来表达 V(s) 岂不是会变成一个循环吗？ 嗯，问题是是否可以有自循环？ 嗯，在这种情况下，这可能是某种循环器定义的[噪音]。
(24:59) 嗯，在这种情况下，因为我们正在考虑无限范围的过程， 价值函数是固定的，嗯， 如果你包含自循环就很好了。 因此，如果您 可能会转换回相同状态的某些状态没有问题，那就很好。 您确实需要明确定义该矩阵。 你可以采取它，你可以采取它的反面。 嗯，但对于大多数进程来说是这样。 嗯，所以，如果我们想直接解决这个问题，嗯， 这很好，它是解析的，嗯， 但它需要矩阵逆。 如果你有 N 个状态，那么假设你有 N 个状态，通常 大约在 N 平方和 N 立方之间，具体 取决于你使用的矩阵求逆。 是的。 呃， 这个矩阵是否真的有可能没有逆矩阵，或者 像列总和为一或其他什么属性那样使它不可能？ 问题是这是否有可能没有逆元？ 嗯，这是一个很好的问题。
(26:03) 嗯，我认为这基本上不可能没有逆。 我正在尝试思考在某些情况下是否可以违反这一点。 嗯，如果是的话，抱歉，请继续。 好的。 [噪音] 是的。 所以，我认为有几个， 嗯，如果有一个 - 如果这最终成为零矩阵， 嗯，这取决于事物的定义方式。 嗯，但我会仔细检查，然后在广场上发送一张便条。 是的。 好吧，实际上我认为关于过渡矩阵的最大方面[听不清] 让我仔细检查一下，这样我就不会说任何 不正确的事情，然后我会在广场上发送一条说明。 这是一个好问题。 这就是计算这个的分析方法。 另一种方法是使用动态规划。 因此，在这种情况下， 它是一种迭代算法，而不是一次性算法。 因此，这种情况下的想法是，您将值函数
(27:08) 在任何地方都初始化为零，实际上您可以将其初始化为任何值，这并不重要。 如果你这样做直到收敛。 那么我们要做的就是我们要做的事情将 接近 我们稍后将看到的事情，即行李员备份。 所以，这种情况下的想法是因为马尔可夫性质， 我们说过状态的价值恰好等于 我们获得的直接奖励加上未来奖励的贴现总和。 在这种情况下， 我们可以简单地使用它来导出迭代方程，其中我们使用状态的先前值 来引导 和计算状态的下一个值，并且我们对所有状态都这样做。 而且计算复杂度要低一些，因为它只有 |S| 平方，因为您要对每个状态执行此操作， 然后对所有可能的下一个状态求和。 当我说我们通常会进行这种完全收敛时， 我们在这种情况下所做的就是定义一个规范。 所以，通常我们会这样做， V_k 减去 V_k-1。 我需要这样做，直到它低于某个 epsilon。
(28:17) 因此，这样做的优点是每次迭代更新都 更便宜，并且当我们开始考虑操作时它们也会带来一些好处。 当我们开始采取行动时，另一件事并不那么容易适用， 但我们也会看到它在哪里相关。 因此，这里有两种不同的方法来尝试计算 马尔可夫奖励过程的值，或者说三种方法，一种是模拟， 第二种是分析。 分析方法要求我们一步步确定一组 有限的状态，第三个方法是动态规划。 我们现在也仅在状态空间有限时定义所有这些， 但稍后我们将讨论状态空间何时无限。 所以，现在我们终于可以进入马尔可夫决策过程了。 马尔可夫决策过程与 马尔可夫奖励过程相同，只是现在我们有行动。 所以，我们仍然有动力学模型，但现在 我们有一个为 每个动作单独指定的动力学模型，并且我们还有一个奖励函数。 正如卡米拉之前所问的，我认为 奖励可以是当前状态、 状态和动作到状态动作和下一个状态的函数，在今天剩下的大部分时间里，
(29:23) 我们将使用它的函数 状态和行动。 因此，代理处于采取行动的状态， 他们立即获得奖励， 然后过渡到下一个状态。 因此，如果您考虑提供一个观察，您会看到类似 s、a、r 的内容，然后转换到状态 s'。 因此，马尔可夫决策过程通常被 描述为一个元组，它只是状态、 动作、奖励、动态、模型和折扣因子的集合。 由于您定义动态模型的方式， 是否会出现这样的情况：如果您采取 旨在让您转移到状态 s 的特定操作， 您将无法完全成功地转移到该状态？ 就像我想我很好奇为什么会有-为什么有概率？ 就像如果你深陷 K 行动中的某个状态， 为什么下一个状态是确定性的？ 问题同样如此，为什么会这样——我认为为什么会有随机过程。 嗯，在很多情况下我们没有完美的环境模型。 也许如果我们有更好的模型，那么事情就会是确定性的。 因此，我们将用随机性来近似这些模型的不确定性。
(30:28) 所以，也许你的机器人有点问题，所以 有时它会卡在地毯上，有时它会继续前进。 我们可以将其写为随机转移矩阵， 有时它停留在同一个位置，有时它会前进到下一个状态。 或者也许你在沙子或类似的东西上。 也许当您尝试开车前往 SFO 时，有时会遇到交通堵塞，有时却不会。 你可以想象在你的状态空间中放入更多的变量来 尝试使其成为确定性的结果，或者你可以只是说， “嘿，有时当我尝试去上班时，你知道， 就像我遇到了这么多的红灯，然后 所以我迟到了，其他时候， 你知道，我没有闯红灯，所以我很好。” 所以，如果我们考虑一下我们的火星漫游者 MDP。 现在，我们定义两个动作 A1 和 A2。 您可以将这些事情视为尝试 向左或向右移动的代理，但通常将 它们视为 此特定示例的确定性操作也可能更容易。 因此，我们可以写下 这两种情况下的转换矩阵，它 准确地向我们展示了前一个动作的下一个状态。 因此，在这种情况下发生的情况是，如果代理尝试
(31:33) 在状态 s_1 中执行 a_1，那么它会保持在该状态。 否则，它通常会转移到下一个状态。 如果它尝试执行操作 a_1 并执行操作 a_2，它将向右移动，除非它到达 s_7，然后它将停留在那里。 因此，就像我们在课程开始时所说的那样， 马尔可夫决策过程策略指定了在每个状态下要采取的操作。 策略本身可以是确定性的，也可以是随机的，这 意味着您可以在 给定您所处状态的下一个操作中进行分配，也可以具有确定性映射。 它说每当我处于这种状态时，我总是， 你知道，做动作 a_1。 现在，这门课的很多内容我们都会考虑 确定性策略，但稍后当我们进入 策略搜索时，我们将更多地讨论随机策略。 因此，如果您有 MDP 和策略， 那么它会立即指定马尔可夫奖励过程。 因为一旦您指定了策略，那么您就可以将其视为 诱导马尔可夫奖励过程，因为您
(32:40) 只会指定您所在州的行动分配， 因此您可以考虑奖励是什么 ， 您在任何状态下在该策略下获得的预期奖励，类似地，您可以 根据您采取这些不同操作的权重对您的转换模型进行平均，从而定义马尔可夫奖励过程的转换模型。 因此，考虑 马尔可夫决策过程和马尔可夫奖励过程之间的这些联系是有用的，因为它意味着如果 你有一个固定的策略，你可以使用我们 刚刚为马尔可夫奖励过程描述的所有技术，主要是模拟、 分析、 分析解决方案或动态 规划，以计算策略的价值。 因此，如果我们回到迭代算法，那么它与以前完全相同， 与马尔可夫奖励过程完全相同，只是 现在我们通过策略对奖励进行索引。 因此，为了了解特定策略的价值是什么，我们
(33:45) 通过始终选择策略将采取的操作来实例化奖励函数。 因此，在这种情况下，我这样做是为了简化 确定性策略，然后 类似地 根据我们在该状态下采取的操作来索引要查找的转换模型。 这也称为特定保单的行李员备份。 因此，它允许我们很好地说明该政策下状态的价值是什么， 它只是我 在当前状态下遵循该政策所获得的即时奖励加上 我遵循该政策所获得的预期奖励折扣总和。 然后无论我最终处于什么状态，我都会继续遵循这一政策。 这就是 V^pi_k-1 指定的内容。 如果我们通过 继续遵循我们刚刚过渡到的任何状态的政策而获得预期的折扣奖励金额，将会发生什么。 因此，如果我们进入火星漫游者的 马尔可夫-马尔可夫链
(34:51) 或马-现在的马尔可夫决策过程， 那么让我们看看现在有这两个动作的情况。 奖励函数仍然是，如果您处于状态 1，您对任何操作都将 获得加 1，而在任何状态下，状态 s_7 的任何操作您都会获得加 10。 其他一切都为零。 因此，假设您的策略始终是执行操作 a_1 并且您的折扣因子为零。 那么，在这种情况下， 该策略的价值是什么， 这只是为了提醒您它的迭代计算方式是什么样的。 是的，在后面。 嗯，我认为 除了 s_1 和 s_7 之外的所有值都为零，其中 s_1 和 s_7 为 +1 和 +10。 完全正确。 所以这是一个有点棘手的问题， 因为我没有再次向您展示过渡模型是什么。
(35:56) 说的完全正确。 这里的转换模型是什么并不重要， 嗯，因为伽玛等于零。 所以这意味着所有这一切都会消失， 嗯，所以你只会得到立即的回报。 因此，如果您的折扣系数为零，那么您只关心即时奖励。 因此，该策略的立即奖励是 因为所有操作和状态一的奖励始终为+1。 所有动作和所有其他状态的奖励都是零， 除了在状态 s_7 中，无论你采取哪种动作，它总是 10。 所以这只是等于一。 这就是值函数地址。 好的。 那么让我们，嗯，看看另一个。 所以现在我们有了完全相同的过程。 嗯，我已经写下了状态 s_6 的动力学模型的特定选择。 因此，让我们想象一下，当您处于 几乎一直向右的状态 s_6 时，嗯， 您有 50% 的概率在操作 A1 下停留在那里， 或者有 50% 的概率进入状态 s_7。 这就是上面这句话所说的。
(37:00) 然后还有一大堆其他动力学模型，我们 不需要担心进行此计算。 然后状态 s_1 的奖励仍然是 +1， 状态 s_7 的奖励仍然是 +10， 中间所有状态的奖励仍然是 0。 然后让我们想象一下，嗯， 我们仍在尝试评估您始终采取行动 a_1 的政策。 嗯，我们刚刚说过 V_k 等于 1,0,0,0,10， 嗯，现在我们想做的是再做一次备份。 所以我们想从 V_k=1 开始计算 V_k=2。 那么，每个人如何[噪音]花一点时间并计算出[噪音] 在这个特定政策下，对于 s_6 来说，价值是多少。 所以你可以使用这个方程，嗯， 来算出，因为我知道 我以前的价值函数是什么，因为我在那里指定了它，它是 1,0,0,0,10。 嗯，现在我要做一个备份， 我只要求你为一个州做这件事，
(38:04) 如果你愿意的话，你可以为其他州做这件事。 嗯，如果用这个方程来计算 s_6 的新值是多少？ 它只需要输入奖励的价值是多少。 该值是 and- 以及动态和旧值函数的特定数字。 我举这个例子的原因是为了展示在进行 此计算时信息如何流动。 所以你从一开始就开始。 让我先过去一下吧。 因此，当您开始时，您将在 各处将值函数初始化为零。 您所做的第一次备份基本上将 价值函数初始化为到处的即时奖励。 然后，您将继续 进行这些备份，本质上您正在尝试 计算 此政策下每个州未来奖励的预期折扣总和。 因此，如果你考虑一下这一点， 那就是状态 s_7 很好这一事实的信息， 会有点向后流动到其他状态，因为他们说“好吧， 我一直处于状态 s_4 我不” 现在没有任何奖励，但在 这个过程中的几个时间步骤中我可能会得到任何奖励，因为我可能会达到真正伟大的+10状态。”
(39:12) 因此，当我们进行这些策略评估迭代时， 我们开始将有关未来奖励的信息传播回早期状态。 所以我在这里要求你做的只是再迈出一步。 就说状态s_6， 它的新值是多少？ 它之前的值为零。 现在我们要做一个备份，这个新值是什么。 那么，如果您只是呃， 让我们问一个问题，然后我们都可以花一点时间呃。 我只是想知道，呃，是否重复相同的过程来找到值函数。 我想如果你不一定知道 s 的值函数， 你可以逆向地跟踪它。 问题是如果你不知道 价值函数是什么，你能吗？ 我想我并不完全确定。 这是一种计算值的方法， 请等待您的问题，因为这是一种计算值函数的方法。 所以我们在这里所做的就是我们已经说过的， 我们已经将值函数初始化为零。 这不是真正的值函数， 只是一种初始化。 这个过程允许我们做的是不断 更新每个状态的值，直到它们停止变化。 然后这给了我们预期的折扣奖励总额。
(40:17) 现在你可能会问，好吧，他们——他们能保证停止改变吗？ 我们稍后会讨论这部分。 我们将了解到这样一个事实：整个过程肯定 是一个收缩过程，因此不会永远持续下去。 因此价值函数之间的距离将会缩小。 这就是折扣因素的好处之一。 因此，如果人们没有任何更直接的问题， 我建议我们都花一点时间，然后与 您的邻居比较进行此计算时得到的数字。 只是为了快速检查贝尔曼方程是否有意义。 [噪音] 好吧。 所以，嗯， 无论你走到哪里，嗯， 希望我们有机会 与你旁边的其他人进行比较，检查是否有任何理解。 嗯，在我们继续之前，我只想，嗯， 回答之前提出的一个问题，即 分析解决方案是否总是可以 反转。 让我们回到那个话题。 所以在这种情况下，嗯， 因为 p 是一个随机矩阵， 它的特征值总是小于或等于 1。 如果你的折扣因子小于 1， 那么 I（单位矩阵减去 gamma 乘以 P）总是可逆的。 这就是这个问题的答案。
(41:21) 所以只要 gamma 小于 1，这个矩阵就总是可逆的。 好的。 那么让我们回到这个，嗯， 我们将需要任何方式来实现我们想要的其他一些重要属性。 那么在这种情况下那是什么？ 因此，其直接奖励是 零加上伽玛乘以 [NOISE] 0.5 我们保持在 该状态的概率乘以 s_6 的前一个 V 加上我们进入 s_7 的 V 的 0.5 概率。 这将等于 0 加 0.5 乘以 0 加 0.5 乘以 10。 这只是一个例子，嗯， 如何计算一个 Bellman 备份。 这又回到了我原来的问题，即你似乎正在使用 不带上标 pi 的 V_k 来评估它。 哦，抱歉，这应该是的。 这应该是 pi。 这只是一个错字。 那是正确的。 问题只是那是否应该是 pi 在那里。 是的
(42:26) ，确实如此，感谢您的捕捉。 好了，现在我们可以开始讨论马尔可夫决策过程控制了。 现在只是要注意一下。 所以我带领我们完成了，或者我们只是 以迭代的方式进行了政策评估，你也可以通过 分析来完成，或者你可以通过模拟来完成。 但作为一个特别好的类比，我们现在要开始考虑控制。 那么我所说的控制是什么意思呢？ 这里的控制将是这样一个事实：最终 我们不仅仅关心评估策略， 通常我们希望我们的代理实际上学习策略。 因此，在这种情况下，我们不会讨论学习策略， 我们只会讨论计算最优策略。 所以重要的是存在唯一的最优价值函数。 Soum， MDP 和无限水平有限状态 MDP 的最优策略是确定性的。 因此，这就是为什么 我们只关注确定性政策就足够了，在无限视野中 使用有限状态 MDP，这就是一个很好的理由 。
(43:30) 好的。 那么我们如何计算呢？ 在我们这样做之前，首先让我们考虑一下可能有多少个策略。 所以有七个离散状态。 在本例中，它是机器人所在的位置。 有两个动作。 我不会称它们为“左”和“右”， 我只是将它们称为“a_1”和“a_2”。 因为左和右意味着你一定会实现这一点。 我们也可以将这些视为通常的随机场景。 所以我们就称它们为a_1和a_2吧。 那么问题是有多少种确定性策略， MDP 的最优策略是否总是唯一的？ 所以，我们只需花 一分钟或者说一两分钟，就可以随意与邻居谈论 针对这种特殊情况有多少确定性政策 ，然后如果是这样， 一旦你回答了这个问题 如果你 有 |S| 的话，一般来说就可以考虑一下 状态和 |A| 动作， 这就是这些集合的基数。 有多少种可能的确定性政策？ 嗯，然后第二个问题是这些是否总是唯一的。 [噪音] 我可以 猜测一下这种情况下有多少确定性策略吗？
(44:37) [噪音]。 这是从状态到动作的映射，所以它将是 2 到 7。 完全正确。 那就是它是一个映射。 呃，如果我们回想一下政策的定义， 映射将是从状态到行动的映射。 因此，在这种情况下，这意味着 每个州都有两种选择，并且有七个州。 更一般地说，策略的 [NOISE] 数量是 |A| 到|S|。 所以我们可以很大， 它的指数和状态空间但它是有限的。 所以它是有界的。 嗯， 有人想猜测最优策略是否总是唯一的？ 我告诉过你价值函数是唯一的。 政策独特吗？ 是的。 我认为可能存在并非如此的情况。 完全正确，嗯。 它并不总是独一无二的。 价值函数是唯一的，但在某些情况下可能会出现平局。 因此，可能存在两个
(45:41) 具有相同价值的操作或两个策略。 所以不行。 取决于过程。 你的意思是像独特的最优价值函数？ 是啊。 所以问题是我能否解释一下存在唯一的最优值函数的含义。 我的意思是状态的最优值。 因此，预期回报的贴现总和， 嗯，可能有不止一个最优策略，但 至少存在一个导致该状态最大值的最优策略。 嗯，这只有一个值。 我们 稍后讨论收缩特性时可能会更清楚一些。 嗯，所以对于每个状态来说它只是一个标量值。 它准确地说明了预期贴现收益总和是多少，这 是最优策略下的最大预期贴现收益总和。 是的。 关于我们的[听不清]策略—— 当我们第一次定义策略时，我认为我正在描述 整个哈希表，
(46:46) 每个状态有一个操作，而不是说所有可能的组合。 令人 有点惊讶的是，由于行动，每张地图上的数字都是 2 到 7，而不仅仅是 州的数量。 为了让我更好地澄清，你知道，这是 什么——有多少政策，以及是否 可能——可能看起来 是线性的，但实际上是指数级的。 嗯，我们在这里定义决策策略的方式， 嗯，确定性决策策略是从状态到操作的映射。 因此，这意味着我们可以为每个状态选择一个动作，这 就是为什么这最终会呈指数级增长的原因。 嗯，所以，在这种情况下，让我们想象一下， 我们只有六个或两个状态，而不是七个状态。 现在我们有 s_1 和 s_2。 [噪音] 所以，你可以有动作 a_1-a_1， 你可以有动作 a_1-a_2， 你可以有动作 a_2-a_1 或动作 a_2-a_2。 你必须这么做，而且所有这些都是不同的政策。 所以，这就是为什么空间最终呈指数增长的原因。 当然。 当你有 A 的 S 次方时。我假设 A 指的是 每个州的法律行动，假设
(47:54) 你可以根据不同的州采取不同的行动。 问题是你是否能够 对状态的行动空间有不同的限制，绝对的。 因此，在这种情况下，今天为了简单起见， 我们将假设所有操作都适用于所有状态。 嗯，实际上这往往不是真的。 嗯，在许多现实世界的情况下， 嗯，有些行动可能是特定于国家的。 啊，总的来说，医疗干预的空间很大。 嗯，呃对于他们中的许多人来说， 他们可能根本不合理地考虑， 嗯，因为某些州是适用的。 嗯，所以，一般来说， 每个状态可以有不同的操作子空间 ，然后您可以对操作进行乘积，即与 每个状态相关的操作集的基数。 但就目前而言，我认为这很简单，只需将其视为 一个统一的动作空间，然后它们就可以应用于任何状态。 好的。 所以，嗯，MDP 的最优策略和 代理永远行动的有限视野问题。 嗯，这是确定性的。 它是静止的，这意味着它不依赖于时间步长。 我们上次就开始讨论这个问题了。
(48:58) 嗯，所以，这意味着如果我处于这种状态 - 如果我处于状态 s_7，则 存在处于 状态 s_7 的最佳策略，无论我在时间步 1、 时间步 37、时间步遇到 - 步骤242静止。 嗯，对此的直觉之一是，如果你要永远行动， 无论你在什么时候，总会有无数个未来的时间步骤。 所以，如果你现在总是从状态 s_7 执行操作 a_1， 那么如果你在 50 个时间步内再次遇到它，你仍然有 无限的时间从那里开始，所以你仍然会 采取相同的操作，如果 是最好的选择。 正如我们刚刚讨论的那样，最优策略不一定是唯一的， 因为你可能有多个具有相同价值函数的策略。 那么，我们如何计算呢？ 一种选择是策略搜索呃，几周后我们将在 讨论 函数逼近和拥有非常大的状态空间时详细讨论这一点。 嗯，但即使在表格情况下， 呃我们也可以考虑搜索。 因此，我们刚刚讨论的确定性策略的数量是 A 到 S，
(50:04) 嗯，策略迭代是一种通常比枚举更好的技术。 那么，在这种情况下，枚举是什么意思呢？ 我的意思是政策的数量是有限的。 您可以分别评估它们中的每一个，然后选择最大值。 因此，如果您有大量计算， 您可能只想这样做，如果您真的 关心挂钟并且拥有很多很多处理器，这可能会更好。 你可以彻底地做到这一点。 您可以尝试所有策略，通过 分析或迭代方式或 您想要使用的任何方案来评估所有策略，然后对所有策略取最大值。 但是，如果您没有无限计算能力，那么 如果您必须 连续执行此操作来执行策略迭代，那么通常计算效率会更高，因此我们将讨论这是什么。 因此，在策略迭代中，我们所做的基本上就是 跟踪对最佳策略可能是什么的猜测。 我们评估它的价值，然后尝试改进它。 如果我们不能再改进了， 那么我们可以——那么我们就可以停下来了。 所以，我们的想法是从随机初始化开始。 现在你可以认为下标正在索引我们所处的策略。
(51:12) 因此，最初我们从一些随机策略开始， 然后 π_i 总是会索引 我们当前对最佳策略可能是什么的猜测。 因此，我们所做的就是随机初始化我们的策略，虽然它没有改变， 但我们会讨论它是否可以在一秒钟内改变或返回到相同的策略， 我们会执行价值函数策略。 我们使用我们刚才讨论的相同技术来评估策略， 因为它是一个固定策略，这 意味着我们现在基本上处于马尔可夫奖励过程中。 然后我们进行政策改进。 因此，与我们之前所做的相比，真正的新事物 是政策的改进。 因此，为了定义如何改进政策， 我们将定义一些新的东西，即国家行动价值。 因此，在我们刚刚讨论状态值之前， 状态值用 V 表示。我们谈论的 是 V^pi(s) ，它表示如果您从状态 s 开始并且 遵循策略 pi ，则预期贴现总和是多少 奖励。 国家行动价值说得好， 我将遵循这个政策 pi 但不是立即。
(52:19) 我将首先采取行动 a， 这可能与我的策略告诉我 要做的不同，然后在下一个时间步骤中我将遵循策略 pi。 所以，它只是说我将从采取我选择的行动中获得立即奖励， 然后我将过渡到一个新的状态。 同样，这取决于我当前的状态和我刚刚 采取的行动，从那时起我将采取政策 pi。 因此，这定义了 Q 函数， 而策略改进的作用是，它表示你已经有了一个策略， 你刚刚进行了策略评估，并且得到了它的值。 因此，策略评估只是让您计算 该策略的价值[噪音]，现在我想看看是否可以改进它。 现在，请记住，现在我们知道 动态模型并且知道奖励模型。 所以，我们可以做的是，我们可以通过 Q 计算来做到这一点，我们说好吧，我已经 通过策略得到了先前的值函数，现在
(53:23) 我计算 Q^pi ，这表明如果我采取不同的操作， 它可能是 同样，我们对所有 A 和所有 S 执行此操作。因此， 对于所有 A 和所有 S，我们计算此值，然后我们 将计算一个新策略，这是最大化此 Q 的改进步骤。 所以，我们只是 进行此计算，然后我们取最大值。 现在，根据定义，它必须大于或等于 Q^πi(s, pi_i(a))， 对吧，因为 a 等于 pi_i(a)， 抱歉 pi_i(s)。 因此， 要么你的 arg max 将与 你之前的策略 π_i 相同，要么它会有所不同，并且唯一一次你会以 不同的方式选择它，就好像该替代操作的 Q 函数更好一样。
(54:30) 因此，根据定义，Q^π_i(s,a) 的 A 上的 max Q^π 必须大于或等于 Q^π_i(s, π_i(s))。 问题在后面。 这会容易受影响吗？ 这是否就像找到一个局部最大目标然后就陷入 困境并且[听不清]无法采取行动。 好的。 因此，这将使我们能够进行一些局部单调改进， 但是我们是否会容易陷入增益卡住的境地。 嗯，事实上，对于任何 玩过强化学习和策略梯度之类的人来说， 这正是当我们开始很好地使用基于梯度的方法时可能发生的问题之一， 在这种情况下，这种情况不会发生。 因此，我们保证会收敛到全局最优值，我们稍后会了解原因。 好的。 好的。 这就是它的工作原理。 您执行此策略评估，然后计算 Q 函数，然后 计算采用 Q 函数的 arg max 的新策略。 这就是政策改进的运作方式。
(55:37) 下一个关键问题是 Iris 提出的， 我们为什么要这样做，这是个好主意吗？ 所以，当我们看这个的时候， 让我们再仔细看看这些东西。 我们将得到的是，我们将得到， 嗯，这种有趣的 政策改进步骤，它涉及一些不同的事情。 所以，我只是想强调它的微妙之处。 所以，这里发生的是我们计算这个 Q 函数，然后我们就得到了这个。 我们得到 Q^π_i(s,a) 的 A 上的最大值必须大于等于 R(s, π(a))。 我们之前使用的先前策略。 [噪音]。 所以，我所做的是我说过，好吧， Q 上的最大操作必须
(56:42) 至少与按照定义遵循旧策略一样好， 因为否则你总是可以选择与以前相同的策略， 否则 你会选择一个更好的行动。 这里的奖励函数 正是对旧策略价值的定义。 所以，这意味着您的 Q 函数的最大值 必须至少与您的旧值一样好。 所以，这是令人鼓舞的。 但这是奇怪的部分。 因此，当我们这样做时， 如果我们改为采用 arg max，我们将得到新的策略。 那么，这是在做什么呢？ 它是说， 我正在计算这个新的 Q 函数。 这个Q函数代表什么？ 它代表，如果我采取行动，然后从那时起我就遵循我的旧政策。 然后我会选择使每个州的数量最大化的任何行动。 好的。 因此，我将为每个州执行此过程。 但是，那么这将定义一项新政策，对吗？
(57:48) 就像我认为这可能是相同的，也可能是与 您之前的政策不同的政策。 这是奇怪的事情。 所以，这就是说，如果您遵循 arg max A，然后从那时起遵循您的旧政策， 您将保证比以前做得更好。 但奇怪的是，从那时起我们就不再遵循旧政策了。 我们将永远遵循这项新政策。 因此，请记住，我们正在做的是彻底改变 我们的策略，然后我们将评估所有时间步长的新策略，而 不仅仅是第一个时间步长，然后从那时起遵循旧策略。 所以，至少应该有点不清楚这是一件好事[笑声]。 应该是，好吧，所以你是 说，如果我采取 这一不同的行动，然后遵循我的旧政策， 那么我知道我的价值会比以前更好。 但你真正想要的是这项新政策总体上更好。 因此，很酷的事情是，您可以通过进行 此政策改进来证明它比旧政策单调更好。 所以，这只是说说而已，我们是说，
(58:54) 你知道，如果我们对一项行动采取新政策， 然后永远遵循 pi_i，那么我们保证 至少和以前一样好 我们的价值函数， 但我们新提出的政策就是始终遵循这个新政策。 好的。 那么，为什么我们 在政策价值上这么说，却得到了政策价值的单调提升呢？ 那么，首先我所说的单调改进是什么意思？ 嗯，我的意思是， 如果 所有州的新政策都大于等于旧政策，则该值是单调的。 因此，它必须具有相同的价值或更好。 我的主张是， 如果旧政策不是最优的，那么在所有存在严格不平等的州，新政策都大于或等于旧政策。 那么，为什么这会起作用呢？ 因此，它的作用有以下几个原因。 让我们继续简单地浏览一下证明。 好的。 所以，我们在这里所说的是， 嗯，V^pi_i(s)，
(1:00:01) 这是我们政策的旧价值。 所以，这就像我们以前的政策价值观。 必须小于或等于 Q^pi_i(s, a) 的 max a。 这只是根据定义。 呃，我就这样写吧。 等于 R(s, pi_i+1(s))。 因为请记住，我们 定义 pi_i+1(s) 的方式正好 等于匹配最大化 Q^pi_i 的策略。 好的。 所以，这将是根据定义。 所以，我已经摆脱了那里的最大值。 好的。 所以，这将小于或等于 R 相同的东西在
(1:01:12) 开始时乘以我们 Q^pi_i 的 a 的 max 。 再次根据定义，因为我们已经说过，我们 首先知道 s 素数的饼图 i 也将小于或等于 Q^pi_i(s', a') 的 a 上的 max。 好的。 所以，我们刚刚进行了替换。 然后我们可以使用r奖励重新扩展这部分。 所以，这将是 a' R(s',a') 加上点-点-点的最大值， 基本上是从该行进行替换。 所以，我正在嵌套它。 我正在重新扩展 Q^pi 的定义。 如果你一直这样做， 本质上我们只是继续推进，就好像我们 在未来的所有时间步长上继续采用 pi_i+1 一样。
(1:02:19) 这里要注意的关键是它大于或等于。 因此，如果将其完全嵌套在其中，您将得到值 pi_i+1。 所以，这里有两个关键技巧。 首先要说的是， 请注意 V^pi_i 始终较低 - 是 Q^pi 上 max a 的下界。 然后使用 pi_i+1 的定义重新表达这一点。 然后通过 Q^pi 重新确定 V 的上限，并继续重新扩展它。 因此，您可以执行此操作，然后允许您 重新定义 - 当您使用 pi_i+1 将其替换为所有操作时， 那么您现在已经定义了 pi_i+1 的值。 因此，这让我们知道新的 pi_i+1 值根据定义至少与之前的值函数一样好。 所以，我就把它放在那里[听不清]。 好的。 所以，接下来可能出现的问题是，我们
(1:03:22) 知道我们会得到这种单调的改进， 嗯，所以问题是，如果政策不改变，它还能再改变吗？ 策略迭代是否有最大迭代次数？ 那么，迭代是什么意思呢？ 这里的迭代次数是 i。 这是我们可以通过多少政策？ 那么，我们为什么不花一分钟时间考虑一下这个问题，也许可以和 你周围从未见过的人谈谈， 看看他们对这两个问题的看法。 那么策略是单调改进的， 是否有我们之前读过的最大迭代次数？ [噪音] 只是为了 今天的时间 - 只是为了 今天的时间，因为我希望我们也尝试完成价值迭代， 嗯，为什么不 - 有人想给我，嗯， 一个 猜测政策是否可以——如果政策停止改变， 它是否可以再次改变？ 所以，我的意思是，如果 pi 的政策，
(1:04:26) 那么这里的问题是， 如果 i+1 的 pi 等于所有州的 pi i， 它会再次改变吗？ 有人想分享一下这是否属实的猜测。 一旦停止改变，就永远不会再改变。 所以不行。 第二个问题是， 策略迭代是否有最大次数？ 是的。 不存在——你的迭代次数不能超过策略的数量。 这是正确的。 那里——我们知道最多有一个政策。 由于这种单调的改进， 你永远无法重复一项政策 。 因此，存在最大迭代次数。 好的？ 伟大的。 这只是 - 嗯， 我现在将跳过这一点，以便我们可以进行一些值迭代， 但这只是逐步展示 一旦您的策略停止变化， 本质上您的 Q^pi 将如何 是相同的。 所以你不能——呃， 没有政策可以改进，是的，可以改变。 在它融合之后，你将永远留在那里。 好的，策略迭代 以一种方式计算策略中的最优值。
(1:05:31) 策略迭代的想法是，你总是有一个策略， 嗯，也就是说，你知道它在无限范围内的价值。 然后你逐渐尝试改进它。 价值迭代是一种替代方法。 值迭代本身表明， 如果您要执行有限数量的步骤，我们将考虑计算最佳值。 一 开始只是一步，然后是两步，然后是三步等等。 嗯，你只是不断迭代得越来越长。 所以这是不同的，对吧？ 因为政策表明你 总是有一个政策并且你知道它的价值是什么。 只是可能不太好。 价值迭代表示您始终知道策略中的最优值是什么， 但前提是您要针对 k 个时间步采取行动。 所以他们只是——他们正在计算不同的东西， 嗯，他们最终都会收敛到同一件事。 因此，当我们开始谈论价值迭代时， 想想贝尔曼是很有用的。 嗯，贝尔曼方程和 贝尔曼备份算子是 马尔可夫决策过程和强化学习中经常讨论的话题。 因此，我们之前看到的这个约束条件， 即策略的价值是
(1:06:37) 其直接奖励加上其未来奖励的贴现总和， 嗯，被称为贝尔曼方程。 马尔可夫过程的约束，呃， 马尔可夫决策过程说它满足这一点。 或者， 就像我们之前看到的那样，我们可以将 其视为，嗯， 作为备份运算符， 这意味着我们可以将其应用于 旧的价值函数并将其转换为新的价值函数。 所以就像我们在一些政策评估中所做的那样 ， 我们也可以做这些运营商。 在这种情况下，与我们之前在评估中看到的不同之处 在于我们在那里取最大值。 我们将这个最大值取为 已获得的最佳即时积分加上未来奖励的折扣总和。 因此，有时我们会使用 BV 表示法来表示 Bellman 操作员， 这意味着您将旧的 V 插入此处并执行此操作。 那么价值迭代是如何进行的呢？ 该算法可总结如下。 首先，您可以将所有状态的价值函数初始化为零。 然后你循环直到你收敛，嗯，
(1:07:44) 或者如果你正在做一个有限的地平线， 我们今天可能没有时间到达，但是， 嗯，我-那么你就会去那个地平线。 基本上，对于每个州， 你都会有贝尔曼后备操作员。 所以你会说，我在 k 加上一个时间步的状态的价值是，如果我能够选择 最佳的立即行动加上 使用我从上一时间步获得的旧价值函数的未来奖励的折扣总和。 Vk 表示， 考虑到我必须再执行 k 个时间步，我对该状态 s 的最优值是什么是最优的。 这就是为什么将其初始化为零是一件好事，因为在这种情况下， 或者如果您希望结果 是最佳的，就像您有那么多时间步长一样，那么这当然是合理的事情。 如果你没有更多的时间步可以采取行动，你的价值就为零。 您所做的第一个备份基本上会告诉您， 如果您只能采取一个操作，那么您应该立即采取的最佳操作是什么。 然后你开始后退， 嗯，继续说好， 如果我必须行动两个时间步骤怎么办？ 如果我必须执行三个时间步骤怎么办？
(1:08:46) 在每种情况下，您可以采取的最佳决策顺序是什么？ 嗯，再次就 Bellman 操作而言，如果我们回想一下策略迭代正在做什么， 您可以通过修复策略来实例化此 Bellman 操作符。 因此，如果您看到顶部带有“嗯，啊， pi”的 B，并说，好吧，您 不是在执行最大操作，而是 指定您要执行的操作是什么。 因此，您可以将策略评估视为基本上只是 计算重复应用 此 Bellman 备份的固定点，直到 V 停止收敛并停止变化。 所以，嗯，就策略迭代而言， 这与我们 在这些贝尔曼运算符中看到的并执行此 argmax 之前看到的非常相似。 想看看我们是否可以稍微了解一下收缩运算符。 这就是价值迭代的作用。 这是非常相似的策略迭代和评估。 嗯，让我谈谈收缩方面。 因此，对于任何运算符，嗯，
(1:09:51) 让 O 为运算符，x 表示 x 的范数。 所以 x 可以是像值函数一样的向量，然后我们可以将其视为 L2 范数、L1 范数或 L 无穷范数。 所以，如果你想 - 如果一个运算符是 一个收缩，这意味着如果你将它应用于两个不同的事物， 你可以将它们视为价值函数，嗯， 那么它们之间的距离在之后缩小， 嗯，或者至少是没有 与之前的距离相比，应用运算符后更大。 所以只是为了，嗯-实际上， 我会，我会保存示例供以后使用。 如果你想看这个例子，请随时在课后来找我 ，嗯，或者我可以在广场上做。 但这是收缩的正式定义。 在这种情况下，我们将其视为两个向量之间的距离，嗯，在 应用此运算符后不会变大并且会缩小。 所以，价值迭代是否收敛的关键问题 是因为贝尔曼备份是一个收缩算子。 只要 gamma 小于 1，它就是收缩算子。 这意味着，如果你这样做——假设有两个不同的 Beller，
(1:10:56) 两个不同的价值函数，然后你对它们都进行了 Bellman 备份。 那么他们之间的距离就会缩小。 那么我们如何证明这一点呢？ 嗯，我们证明了这一点——为了节省时间，我会向你展示证明。 再说一遍，我很高兴能够完成它， 嗯，我-或者我们可以在办公时间等完成它。 让我简单地展示一下。 因此，为了证明贝尔曼备份是收缩算子， 我们考虑存在两个不同的值函数 k 和 j。 它们不必——这——不必与值迭代有任何关系。 这只是两个不同的价值函数。 你知道，一个可能是 1、3、7、2，另一个可能是 5、6、9、8。 好的。 所以我们只有两个不同的值函数向量，然后在 应用贝尔曼备份算子后重新表达它们是什么。 因此，最大 a 是 立即奖励加上未来奖励的折扣总和， 其中我们插入了两个不同的价值函数。 然后我们说的是， 如果你为这两个分别选择最大值 a，那么 它们之间的距离 比你尝试
(1:12:00) 通过输入最大值 a 来最大化那里的差异要低。 然后 您可以取消奖励。 这就是第三行发生的情况。 接下来我们可以做的就是我们可以限制并说 这两个值函数之间的差异是 diff- is, um，以这两个值之间的距离的最大值为界。 因此，您可以选择这些价值函数最不同的地方。 然后你可以将其从总和中移出。 现在您正在对总和为 1 的概率分布求和。 这给了你这个。 因此，这意味着 只要小于 1，贝尔曼备份就必须是收缩运算符。 应用 Bellman 运算符后， 两个值函数之间的距离不能 比之前更大。 所以，我认为一个很好的练习是，嗯， 假设它是一个收缩算子， 嗯，这意味着它必须收敛到一个固定点。 必须有一个独特的解决方案。 因此，如果您重复应用贝尔曼运算符， 您将到达一个固定点，该固定点是单个
(1:13:04) 矢量值有趣的值。 如果您只关心收敛后的结果，那么考虑一下初始化和值是否会影响任何事情也是很好的。 好的。 所以，嗯，我想我们可以就此打住。 课程基本结束了。 幻灯片中还有更多内容可以讨论，嗯， 有限视野情况，嗯， 如有任何问题，请随时在 Piazza 上与我们联系。 谢谢。 [噪音]
