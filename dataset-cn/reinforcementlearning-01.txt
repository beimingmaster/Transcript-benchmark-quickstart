大家好，我是艾玛·布伦斯基尔，计算机科学系的助理教授，欢迎来到 CS234，这门强化学习课程面向入门级硕士或博士生。今天我们将首先简要概述强化学习，然后进行课程后勤安排。如果您有任何后勤方面的问题，可以在课程网站上找到相关信息。接着我们会讨论顺序决策中的不确定性，这是我们即将涉及的技术内容之一。在课程的后半部分，我会留时间让大家提出任何未涉及但你们有疑问的问题，包括候补名单或与航班相关的特定事项。如果有需要的话，请随时找我。

有谁在学习机器学习的课程吗？ 好的。 有谁在使用人工智能吗？ 好的。 所以，大多数人都有一些了解。 好的。 很好。 所以，也许每个人都对强化学习有所了解。 根据你的背景有所不同。 我们将从头开始介绍一些内容，就像你对强化学习一无所知一样，但随后我们将迅速深入其他主题，这超出了其他斯坦福相关课程的范围。因此，强化学习关注的是这个核心问题：智能代理如何学习做出一系列良好的决策？这句话概括了强化学习的本质。我们了解这门课程将包括哪些内容吗？实际上，它涵盖了许多关键思想。首先，我们关注的是决策的顺序。与许多机器学习课程不同，我们将探讨代理、智能代理或通用智能代理，它们…

可能是也可能不是人类或生物。另外，我们还需要关注它不仅如何做出一个决定，而且如何做出一系列决策。我们关注的是善良。换句话说，我们感兴趣的是，我们如何学会做出好的决策，所谓的“好”是指一些最优性的概念。我们会对正在做出的决策有一些效用衡量标准。强化学习的最后一个关键方面是学习，然而，代理事先并不知道其决策会如何影响世界，或者哪些决策可能会必然与良好的结果相关联，相反，它是通过经验来获取这些信息。所以，当我们考虑这个问题时, 我们实际上一直在不断地做同样的事情。我们从婴儿时期就开始尝试弄清楚如何在世界中获取高回报，神经科学和心理学领域也正在进行许多令人兴奋的工作，试图从同样的基本问题出发思考：人类智能代理。所以我认为，如果我们想解决人工智能问题，我们需要深入思考这些问题。

要取得重要进展，我们必须在开发强化学习智能体方面取得重大进展。那么，这方面的灵感源自何处呢？耶尔·尼夫（Yael Niv）就是一个很好的例子，她是普林斯顿大学一位杰出的心理学家和神经科学研究员。她给我们提供了一个原始生物的例子，在这个例子中，生物是如何逐步进化的。当它还是幼崽时，它拥有原始大脑和一个眼睛，会四处移动并附着在岩石上。成年后，它会将大脑吸收并静坐不动。这或许表明智力的意义，或者至少在某种程度上拥有大脑的意义有助于指导决策，因此一旦决策与智能体的生活完成，也许就不再需要大脑了。因此，我认为这是一个生物学的例子，但也是一个有益的提示，让我们思考智能体为何需要变得聪明，以及在某种程度上它是否必须做出决策。

当然，现在强化学习已经发生了范式转变。大约在2015年的Neurex会议上，这是主要的机器学习会议之一，David Silver参加了一个研讨会，并展示了使用强化学习直接控制Atari游戏取得的令人难以置信的结果。无论您是否喜欢电子游戏，这都是非常重要的。视频游戏是一个很有趣的例子，它展示了人类玩家可能需要一段时间才能完成的复杂任务。这些任务在开始时我们并不知道如何做，至少需要一点经验。一个令人难以置信的例子是Breakout，代理直接从像素输入中学会玩游戏。从代理的角度看，它们只看到一些彩色像素输入，并且必须学习做出正确的决策，以便学会玩得更好，甚至比人类玩得更好。这真的是令人难以置信的，但这是可能的。当我刚开始研究强化学习时，许多工作实际上集中在非常人为的玩具问题上，有很多基础。

在过去的五年里，我们见证了强化学习技术的巨大进步，涉及到解决各种问题规模。不再局限于电子游戏，如机器人技术。在加州伯克利大学，我的一些同事在机器人技术领域做出了引人注目的工作，运用强化学习让机器人完成抓握、叠衣服等任务。这些都是强化学习在不同领域应用的例子，可能你们之前研究过。强化学习已经被应用于许多领域，这既是机遇也是挑战。在斯坦福大学，我指导人类影响实验室的人工智能项目，我们关注的一个关键问题是如何利用强化学习技术。

人工智能可以帮助发挥人类潜力。一种方法是利用教育游戏等工具来快速有效地教授人们学习分数等内容。另一个重要领域是医疗保健，例如，研究人员如麦吉尔大学的Joel Pineau正在研究如何利用人工智能来个性化治疗癫痫发作。人们也在积极探讨如何利用强化学习在医疗领域中与诸如电子病历系统交互，指导患者治疗等方面发挥作用。近年来，人们对如何利用强化学习和其他应用程序进行优化技术以解决复杂优化问题充满激情。这种情况在视觉、自然语言处理等领域得到体现。因此，考虑到强化学习的关键方面，我们需要思考如何利用这种技术来优化各种难以解决的问题。

这些概念可以概括为以下四个方面，这些方面是用来区分它们与人工智能和机器学习的其他方面的。因此，从我的论述中可知，强化学习涉及在面对不确定性时做出正确决策，涉及到优化、延迟后果、探索和泛化。因此，自然而然地涉及到优化，因为我们关注的是做出良好的决策。我们可以做出一些不同类型的决策，目的是希望做出良好的选择。第二个方面是延迟后果。这意味着我们现在所做的决定可能难以判断是否是正确的，直到很长一段时间后才能得知。比如，你可能周日吃了巧克力，一小时后后悔吃掉两块冰淇淋；或者，当你玩《蒙特祖玛的复仇》这类电子游戏时，你必须拿起一把钥匙，但直到很久以后才会意识到这对过关很有帮助；再比如，你为期三周不辞辛劳地学习，然后在期中考试中取得好成绩。因此，这种做法的挑战之一是，

由于无法立即获得结果的反馈，解决所谓的信用分配问题变得困难，即如何确定您的决策导致了未来结果。这与大多数机器学习问题大为不同。因此，面对这一问题时的一个挑战是我们如何进行探索。智能体基本上尝试着通过强化学习的经验来探究世界是如何运作的，因此我们可以认为智能体实际上就像是世界上尝试事物的科学家，就像学习骑自行车的孩子一样。通过摔倒来理解物理学和自行车平衡的原理。这里真正的挑战之一是数据的评审，也就是说你只能了解你尝试过的事情。因此，来到斯坦福大学对你们所有人来说显然是最佳选择。然而，你们最终并没有搞清楚如果选择其他大学会怎样。

麻省理工学院可能是一个不错的选择，但你无法亲身体验不同选择带来的生活，因为我们只能在特定时间做特定选择。一个你可能感兴趣的问题是政策，即我们要采取什么样的政策措施。政策制定其实就是将经验映射到决策上。你可能会问为什么我们需要学习这个过程。如果我们考虑类似Deep Mind的技术，比如在Atari游戏中玩游戏，它是通过学习像素信息来进行行动决策的。换句话说，它是从图像空间中学习下一步该做什么。如果我们试图用一系列if-then语句来编写这个程序，那将是非常繁琐的。因此，我们需要某种形式的泛化能力，这也是为什么直接从数据中学习以及对任务进行高级表示可能更为有效的原因。这样，即使遇到以前未曾见过的特定像素配置，我们的AI智能体仍然可以做出正确判断。

因为强化学习的构成要素与其他人工智能和机器学习类型不同，是在线强化学习的关键特点。同时，在人工智能领域中，另一个常见概念是规划，在围棋等游戏中会涉及到规划问题。规划涉及优化、泛化和延迟后果等内容。在规划中，你可能需要提前采取行动，但其结果是否明智通常需要经过多步考量才能确定，这与探索是不同的。规划的理念是为你建立一个关于世界运作方式的模型。在强化学习中，则需要计算在给定世界模型的情况下，应该做出什么样的决策，因此探索并非必需。相比之下，监督机器学习则通常涉及优化和概括，但不包括探索和延迟后果。因此，它通常不需要进行探索，因为

在监督学习中，通常会有一个数据集可供使用。因此，你的 AI 智能体并非在获取有关世界的经验或数据，而是在获得经验，并必须利用这些经验来判断图像是否为人脸。它通常会做出一个决定，比如判断一张图像是否为一张人脸，而不必像无监督学习那样时不时地考虑这些决定是否正确，无监督学习通常涉及优化和泛化，但通常不涉及探索或延迟后果，因为你通常没有关于世界的标签。

在监督学习中，你通常会获得有关世界的准确标签，比如图像的真实样貌、是否存在人脸或图像中是否包含人脸。而在无监督学习中，你通常不会获得有关世界的标签，而在强化学习中，通常会得到介于你所获得的标签之间的某种东西，即贴标签的效用。例如，你可能认为这是一张人脸，然后再决定“嗯，是这样的”。

我们会开始信任AI智能体的某些方面，因为有些东西看起来很像脸。但它们并没有获得真正的世界标签，或者它可能决定去斯坦福大学，之后就没有更多信息了。你可能会说，这是一次很棒的经验，但不确定是否是“正确的经验”。这很重要，有些相似但也有一些不同。因此，这涉及到优化、泛化，经常会导致延迟后果。但我们的想法是，我们要从他人的经验中学习。因此，我们的AI智能体不会从世界中获取经验然后自己做决定，它可能会观察另一个智能体（可能是人类），做出决定并观察结果，然后利用这个经验来计算如何行动。这样做会有很多好处，但也有些不同，因为它不必直接去解决探索问题。关于模仿学习，我想多花点时间讨论，因为这变得越来越重要。

据我了解，安德鲁·吴是第一位真正推广模仿学习的人，他曾是这里的教授。通过他的一些直升机材料，他与彼得·阿贝尔一起研究了专家飞行，这是在伯克利的。他们尝试通过模仿专家驾驶玩具直升机来快速学习，这是模仿学习的一个成功应用案例。虽然这种方法可能非常有效，但也可能面临一些挑战。因为当你观察一个轨迹，比如想象一架直升机飞行绕圈时，你的 AI 智能体学到的东西可能与专家的行为并不完全一样，这可能导致你偏离原本的轨迹，冒险进入未知的领域。因此，有很多工作将模仿学习和强化学习结合在一起，这种方法非常有潜力。当我们考虑尝试强化学习时，可以建立在许多不同类型的技术基础上。

接下来我们需要考虑强化学习所特有的挑战，这包括四个关键挑战。因此，强化学习智能体需要不断探索环境，利用这些探索来指导未来的决策。在这一课程中，我们将更深入地探讨这一点。一个重要的问题是奖励是从哪里来的，用于指导智能体决策的信息在何处，谁提供奖励，以及如果提供奖励可能出现的问题。我们将进一步讨论这些问题。

虽然我们不会深入讨论多代理强化学习系统，但这也是一个非常重要的案例，尤其涉及到博弈论方面。这里只是简要概述了强化学习的一些方面，说明了它与其他课程的不同之处。

现在让我们简要介绍一下课程后勤事项，然后开始更深入的内容。在课程后勤介绍之后，我会停下来回答任何问题。就先决条件而言，我们希望每个人在这里都能参与其中。

您是否曾在斯坦福大学或其他相似机构上过人工智能或机器学习课程？如果您对自己是否具备适合该课程的背景感到不确定，请随时在Piazza上与我们联系，我们将会回复您。一般而言，我们期望您至少具备基本的Python熟练程度，并且熟悉概率、统计和多变量微积分。您应该对梯度下降、损失函数的导数等概念很熟悉。大多数人之前可能听说过MDP，但这并非必要。虽然这是一个很长的清单[笑声]，但我会逐一浏览，因为我认为这很重要。那么，班级的目标是什么，学习的目标是什么呢？我们希望您在完成本课程后能够做到以下几点，并且我们的职责是帮助您理解如何实现这些目标。首先是能够准确定义强化学习的关键特征，以便将其与其他类型的人工智能和机器学习方法区分开来。

嗯，区分问题框架非常关键。这是我在课堂上做的一些事情，用来搞清楚强化学习如何与其他类型的问题区分开来。因此，与此相关的是，对于大多数人来说，你们可能最终不会成为学者，而是会进入工业界。所以，当你们面对老板提出的特定问题，或者当你向你的督导者提出问题时，让他们考虑是否应该将其视为强化学习问题，以及应该采用哪些方法。我认为在本课程结束时很重要的一点是，要了解是否遇到了现实世界的问题，比如网络广告、患者治疗或机器人问题，你是否有一种感觉可以将其归为强化学习问题，并且如何将其用框架表述，以及哪些相关算法是有用的。在课堂上，我们还会介绍一些强化学习算法，你们将有机会在代码中实现这些算法，包括深度强化学习的问题。

另一个重要的方面是，当您尝试决定使用什么工具来解决特定问题时，比如机器人问题或医疗保健问题，了解哪种算法可能是有益的以及其原因是很重要的。因此，除了基于经验表现之类的因素，我认为了解我们通常如何评估算法也非常关键。我们可以利用诸如遗憾样本复杂性等理论工具以及计算复杂性等因素来确定哪种算法适合特定任务。强化学习中一个非常重要的方面是探索与利用。当AI智能体需要明白他们要做出什么决策以及通过这些决策来了解环境时，这个问题就变得很关键。因此，在课程结束时，您还应该能够比较不同类型的探索与利用技术，以及这些技术的优缺点。有人对这些学习目标有任何疑问吗？好的。因此，我们的课程会有三项主要作业。

好的，考虑到期中考试，课程结束时会有测验和期末项目。这次测验有些不同寻常。我想简要讨论一下。测验既有个人部分也有小组部分。我们这样设计是希望以一种低风险的方式让学生们练习课程后半部分的内容。在某种程度上，这是一种有趣的吸引方式，鼓励大家深入思考并与同龄人学习。去年我们也这样做过，刚开始有些人对此有些紧张，但最终大家都非常享受。测验是以多项选择题形式进行的。首先，每个人会独自完成测验，然后等所有人都提交答案后，我们会按照预先分配的小组再次进行测验。我们的目标是，每个人都必须独立决定正确答案是什么，然后再与小组讨论确定正确答案。

你是否标出了正确的答案？首先，我们会评分答案的正确性。你的分数不会低于个人成绩。因此，在小组中进行评分只会对你有利。对于斯坦福SCP学生来说，他们不会分组评分。他们只需提供答案理由。再次强调，这是一种相对轻量级的评估方法，目的是确保你能清晰地表达为什么相信某个答案是对的，并在小组讨论后，他们会利用这些信息，找出正确答案。最后的项目与你在其他课程中完成的项目非常类似。这是一个开放的项目，提供了机会来推理、思考和加强学习更深入的知识。我们将提供一个默认项目，该项目会在未来几周内的第一个里程碑之前宣布。如果选择做默认项目，你的评分将基于你的作业演示和书面材料，因为我们相信…

我们之间是相互最好的资源。对于大多数课堂交流，我们会使用 Piazza 进行，除非涉及私人或敏感话题，这时请直接联系课程工作人员。几乎所有讲座、作业和项目相关问题都应该在 Piazza 上讨论。关于迟到政策，我们允许六次迟到，具体细节请查看网页。有关协作的详细信息也请查阅网页。在继续之前，请问有关课程后勤方面是否还有问题？好的，让我们开始吧。我们不会在这部分介绍不确定性下的顺序决策，因为有些人之前已经接触过相关内容。我们将重点介绍主要内容，比大家之前了解得更深入，包括某些理论，尽管不是今天的主题，而是在之前的讲座中。然后我们会继续讨论一些对大家是新的内容。因此，讨论的主题是在不确定性情况下做出顺序决策。

我们在设计这些系统时考虑的基本问题是一个交互式闭环过程，其中有一个智能代理采取行动来影响世界状态，并根据观察和奖励进行反馈。关键目标是让智能体试图最大化未来预期的总奖励。在这个过程中，预期的方面非常重要，因为有时世界本身是随机的，所以代理要尽力让预期最大化，尽管这并不总是正确的标准。大部分强化学习都集中在这一点上，但现在人们开始对荣誉分布、强化学习和其他方面产生了兴趣。其中一个关键挑战是需要平衡及时奖励和长期奖励之间的关系，并可能需要采取策略性行动来获得高回报，这表明可能需要做出一些牺牲。

最初给予高奖励才能获得更好长期回报的情况在很多场景下都能见到。比如，在网络广告领域，你可能拥有一个代理运行着的网站，需要选择向用户展示哪个广告，用户会反馈他们在网站上的停留时间等数据。你可以得到一些点击广告的信息，从而确定哪种广告能够获得最多点击。举个洗碗机卸载机器人的例子，代理在这种情况下可能需要进行关节运动。代理可以通过厨房摄像头图像获取信息，比如若发现柜台上没有盘子，则可能会得到奖励。在这个例子中，奖励可能存在延迟，因为通常柜台上都会有盘子，除非它成功地将它们全部移开并掉到地板上，这可能不是系统设计者预期的结果。因此，在这种情况下，代理可能需要做出一系列决策。

在某些情景下，决策可能不会立刻获得奖励。举例来说，控制血压可能需要规定锻炼或药物，我们观察血压情况来确定奖励。如果血压正常，奖励可能增加；如果药物有副作用，奖励可能少；否则奖励可能为零。

另一个案例是人工导师，面对学生可以提供加法或减法活动，学生回答对或回答错。假设学生一开始不会做这些运算。

这个场景中涉及到一个幼儿园老师教学生数学的问题，学生对数学一无所知。教学代理的奖励机制是：学生答对加一分，答错减一分。现在让我们思考一下，如果是一个试图学习的智能体在这种情况下，为了最大化预期奖励，该怎么做？这会带来哪些问题给智能体，以及这种做法是否正确。

在这里让我澄清一下。我们假设大多数学生对加法比减法更容易掌握，因此，即使学生不熟悉具体概念，学习加法比学习减法更容易。那么在这种情况下可能发生什么呢？我们是否愿意举手告诉其他人，或周围的人认为在这种情况下，特工会发生什么呢？

特工将向他们提供非常简单的加法问题，这是完全正确的。这正是实际发生的情况。Bev Wolf发表了一篇大约2,000字的精彩论文，这是最早的论文之一，但我知道他们在其中应用了强化学习来建立智能辅导系统，奖励是让特工向学生提出问题以确保其回答正确。因为，你知道，学生如果回答正确，就表示他们已经掌握了知识。然而，这里的问题是奖励机制，特工学会提出非常简单的问题。

学生最初可能不知道如何解决这些问题，但后来他们很快学会了解决方法，之后就失去了动力去努力解决问题。 这就是一个奖励黑客的典型例子，你的AI代理会按照你设定的奖励函数执行任务，但在强化学习中，我们往往不太深入思考奖励函数的设计。然而，在现实世界测试时，这一点非常关键。通常，设计者可以选择奖励函数，代理没有内在的动力，它会根据您指定的奖励方式学习不同的行为。所以，学生在这种情况下就像强化学习代理，我们需要思考如何引导他们解决问题，而不是简单忽视这一点。因此，你提出的问题很中肯，人们可能确实像强化学习代理一样行动。

也许他们会开始说，“嘿，我需要回答更难的问题，或者进行更多互动。” 对于这门课的大部分内容，我们可能会忽略一个事实，即我们与之交互的世界本身也可能是一个强化学习代理。实际上，这一点非常关键，有时我们会考虑到对抗性的情况，就像博弈论一样。我认为最令人兴奋的事情之一是当我们以合作的方式来思考。那么，有谁听说过机器教学这个分支学科吗？或许还没有人听说过，这是一个非常有趣的新领域，可能已经存在了5-10年，有些甚至更久。其中一个想法是，如果你有两个智能代理互相交互，并且知道彼此都在试图帮助对方，会发生什么呢？对于那些对机器学习不太熟悉的人，有一个非常好的经典例子。想象一下，你正在尝试训练一个分类器来判断沿着某条线的事情是积极的还是消极的。因此，一般来说，在这种情况下，如果您...

在线学习中获取正负标签的必要性会需要一定数量的样本点。在主动学习环境下，通过策略性地请求人们标记特定点来减少样本数量到大约对数级别，这是一种常见做法。在机器学习中，一项很酷的事情是，如果我知道你要教我如何划定一条线，我通常只需要一个或最多两个点就能完成。因为如果我是要教你，我不会随机地给事物打标签，我可能只会标记一个加号和一个减号，就可以告诉你正确的线在哪里。所以，当一个智能体意识到另一个智能体试图教授知识时，它的学习效率实际上会比我们通常认为的要高。因此，我认为机器学习在教学方面有巨大潜力，可以非常有效。虽然我们会忽略课程中的很多内容，但如果您想在项目中探索这些内容，欢迎您的参与。这与强化学习有很多联系。如果我们从整体上审视这个过程，

在考虑某种顺序决策过程时，我们会涉及到一个AI智能体。通常情况下，我们都要考虑一个谨慎的计时器。这样，智能体会做出决策，影响世界，观察世界，并得到一些新的观察和奖励。智能体接收这些信息，再次做出决策。因此，在这种情况下，当我们提到历史时，我们指的是智能体之前采取的一系列行动，以及它接收到的观察和奖励。其次，则是定义状态空间这一关键点。通常情况下，当刚开始讨论时，人们认为这些是不可改变的。但在实际应用中，这是您必须定义的东西，即如何表示世界。在我们当前讨论的框架中，我们假设状态是历史的函数。这也可能会涉及到其他方面——智能体可能希望获取其它感官信息以做出决策。但这将受限于迄今为止收到的观察、采取的行动以及观察到的奖励。

现在，也存在一些现实世界的状态。 这就是真实世界。 代理不一定能够获取到所有现实世界的信息，它们可能只能获取到其中的一部分。 例如，就拿人类而言，我有一双视野大约为180度的眼睛。 但我无法看到自己头后面的情况，尽管我的头后面仍属于世界的状态之一。 因此，世界的状态即是现实世界，而代理有着自己的状态空间，用于做出决策。 因此，我们通常会假设代理具有某种历史功能。 在本课程中我们将经常涉及的一个假设，也许你们之前已经了解过，那就是马尔可夫假设。 马尔可夫假设简单来说就是，我们假设代理所使用的状态是具有历史完备性的统计数据，为了预测未来，你只需要知道当前环境的状态。 这基本上表明，如果当前存在正确的全局统计数据，那么未来与过去无关。

嗯......让我看看，我想你在谈论国家和历史的区别吧？让我用一个例子再解释一下，就像你说的有点困惑。国家，嗯，比如说想象一台机器人。假设你有一台机器人在一个长走廊上移动。假设有两个长走廊。你的机器人从这个点开始。它尝试向右移动，然后向下，向下，再向下。比方说，它的传感器可以检测前方或者侧面是否有墙。所以，机器人的感知空间简单来说就是观察四周是否都是墙壁。可能背后有点盲区，但是基本上，这个智能体要做的就是……

通过激光测距仪或类似的设备，AI智能体具有某种本地观测。因此，它可以感知周围是否存在墙壁，周围是否有一个方形的结构，除此之外则一无所有。因此，在这种情况下，智能体的观察历史可能是墙壁初始时的样子，然后变成其他样子，再变成另一种样子，依此类推。历史记录会包含所有这些变化。但它的本地状态就仅仅是当前的观测结果。这一点在这里变得重要，因为很多位置看起来都是一样的。因此，通过跟踪完整的历史记录，智能体可以确定自己的位置。但如果仅仅跟踪本地位置状态，就可能发生很多部分别名的情况。举个例子，就像在高血压控制的情况下，你可以将状态想象为当前的血压水平，而行动则是是否服药。那么，当前血压水平的意义就像你所了解的那样，比如每秒钟的血压数值。你认为这种系统是马尔可夫系统吗？

我看到有些人摇了摇头。几乎确定不是这样的。几乎可以肯定还有其他因素会影响到您，比如您是否在锻炼，刚刚用餐，外面是否很热。那又怎么样呢？如果你刚刚得到一架飞机。所有这些其他因素可能会影响您下次的血压是高还是低，特别是对某些药物的反应。类似地，就像在网上购物时，你可以想象当前的状态就是你正在查看的产品是什么？比如，当我打开亚马逊网站时，我正在查看一些电脑，这些产品现在就显示在我的页面上，而网站的反应就是向我推荐其他产品。您认为这个系统是马尔可夫系统吗？系统不是马尔可夫系统吗？您是指一般系统吗？但如果假设它是马尔可夫系统但并不适用呢？问题在于系统是否总体上符合马尔可夫性质，即使某些假设不适用或不成立 - 这只是更多的细节。我会考虑一下。我的意思是，重新设定系统的这一特定选择是符合马尔可夫性质的。因此，在现实世界发生变化的同时，代理还可以使用世界模型。

我在这里要说的是，这些特定的世界模型不是马尔可夫模型。虽然可能存在其他类型的世界模型。如果我们选择将当前的血压作为我们的状态，那么它可能并非真正的马尔可夫状态。但这并不意味着我们不能使用算法来处理它。只是要意识到可能会违反一些假设。是这样吗？我在想，如果将足够的历史信息纳入状态中，是否可以使其成为马尔可夫的一部分呢？好的，这是一个好问题。那么，为什么马尔可夫模型如此受欢迎呢？你能知道——总是能得到马尔可夫模型吗？一般来说是可以的。如果将所有历史信息包括在内，那么系统就总是可以被视作马尔可夫的。在实践中，通常你可以只使用最近的观察结果或者最后四个观察结果作为相当充分的统计数据。这在很大程度上取决于具体领域。当然，在某些领域，就像导航世界中那样，如何建模非常关键。可以选择将整个历史作为状态，或者考虑部分观察性的情况。

根据您所说的内容，以及我所了解到的情况，最近的观察可能已经足够。目前的一个挑战是可能不想使用整个历史记录，因为信息量太大。您需要随着时间的推移来跟踪信息。因此，拥有足够的统计数据会更有帮助。一些方面正在通过长短时记忆网络（LSTM）和类似的技术发生改变。因此，之前我们对随着状态空间增大而扩展的假设现在随着深度学习发生了一些变化。然而，历史上看来，拥有较小的状态空间确实有其好处。计算复杂性、所需数据以及最终性能取决于状态空间大小等因素将产生重要影响。为了直观解释这一点，如果你将自己一生中发生的一切都写出来，那将会提供非常丰富的信息。每种情况只有一个数据点，不会有重复。

这个领域真的很难学习，因为所有的状态都是不同的。通常情况下，要学会如何做某事，我们要么需要某种形式的总结，要么需要一种形式的分类或聚合，这样我们才能比较经验，从之前类似的经验中学到应该怎么做。因此，如果我们把您的观察结果看作是您的状态，那么当代理获得最新的观察结果时，我们将其视为状态。然后，代理就是在对世界进行建模，这就是马尔可夫决策过程。因此，它在考虑采取行动、获得观察和奖励时，会设定状态，即世界的状态——用来作为观察的环境状态。如果代理将世界视为部分可观察的，那么它会认为代理的状态是不同的，它可能会使用历史记录或关于世界状态的信念等信息来汇总先前采取的动作序列和接收到的信息，然后用这些来做决策。例如，在像扑克这样的游戏中。

当您玩游戏时，尽管您只能看到自己的牌，但其他玩家的牌会影响游戏进程。您不能确定其他玩家的具体牌是什么，但您可以观察到哪些牌被丢弃，这是可以部分观察到的信息。基于这些观察，您可以形成对其他玩家牌的信念状态，并据此做出决策。类似地，在医疗保健领域，虽然人体生理过程复杂多变，但您可以监测一些指标，比如血压或体温，然后据此做出决策。在决策过程中，其中一种类型是"强盗"。这是一种简化的标记决策过程，因为您的操作不会影响下一个观察结果。这种方法在某些情境下是合理的，比如想象您有一系列访问您网站的客户，您向每个客户展示不同的广告，然后客户会选择点击或不点击。

然后您将允许另一个客户登录到您的网站。在这种情况下，向第一个客户展示的广告通常不会影响接下来登录的第二个客户。有时客户可能会在 Facebook 上分享说他们喜欢某个广告，让别人去看。但大多数情况下，无论您向客户展示什么广告，都不会影响下一个登录您网站的人。因此，您的决定只会影响第一个客户，而第二个客户是完全独立的。在过去50年里，强盗确实非常重要。人们会考虑如何分配参加临床试验的人员。您可以将这些概念应用于网站和其他许多领域。MDP和POMDP告诉我们，您的行动会影响世界的状态，通常也会影响您下一个观察结果和奖励。您必须考虑您的行动在闭环系统中如何改变世界状态。因此，您向客户推荐的产品可能会影响客户下一步的看法，这其实是您所期望的。

在这些情况下，我们要考虑的是行为对实际影响世界状况的影响。另一个重要的问题是世界是如何变化的？一个想法是世界会经历确定性变化。因此，在特定状态下采取行动后，会进入不同的状态，但所进入的状态是确定的，只有一个可能性。这在许多机器人和控制领域中通常是一个常见的假设。请记得，提到的那位是托马斯·洛萨诺-佩雷斯（Tomás Lozano-Pérez）。

考虑这样一个具体例子：当部署漫游车或机器人到很遥远的行星上时，由于通信成本高昂，让这些机器人更加自主将变得非常重要。假设我们有一个简单的火星探测器，它有七个状态系统：刚着陆时，有特定位置，可以选择向左移动或向右移动。通过指令“向左尝试”或“向右尝试”，它尝试执行动作，但结果可能成功也可能失败。

想象一下，不同的科学信息分布在这七个状态中：S1可能有一些有用的科学信息，而S7可能有大量信息，例如水的存在。其他状态则无信息。以口服药物常见成分为例，模型通常是其中一个常见组件，代表着智能体在执行任务时的一种能力。

在制定决策时，我们需要一个模型来表示如果从某个特定状态开始采取某个行动，可能会达到的下一个状态的概率分布，同时还需要一个奖励模型来预测在特定状态下采取行动的预期奖励。例如，假设我们有一个AI智能体，它认为所有地方都没有奖励，同时认为它的电机控制非常糟糕。因此，它可能估计每次尝试移动时有50%的概率停留在原地，有50%的概率实际移动。然而，这个模型可能是错误的。

实际上，如果记得之前提到的信息，真正的奖励是在状态S1获得加一，在状态S7获得10，其他状态均为零。而上面提到的奖励模型是所有地方都是零。因此，这种全零奖励模型是一个智能体可能会拥有的完全合理的奖励模型之一。

这个错误只是偶然发生的。 在许多情况下，模型可能会出现错误，但通常仍可以被智能体用于有意义的方式。因此，口述政策总是治标不治本的下一个重要组成部分。政策或者决策策略是我们进行决策的方式。在考虑马尔可夫决策过程时，我们将其视为从状态到动作的映射。确定性政策意味着每个状态有一个确定的动作。而随机意味着可能会有动作的概率分布。例如，每次驾车去机场时，都会抛硬币决定是选择小路还是高速公路。假设在每个状态下，我们都试图执行正确的动作，这是确定性政策还是随机性政策？确定性政策很好。稍后我们将详细探讨为什么确定性策略有用以及随机策略何时有用。现在，价值函数是指特定政策下未来奖励的预期折现总和。

所以，这其实是在等待。换句话说，我认为我现在和未来会收到多少奖励取决于我对即时奖励和长期奖励的重视程度。折扣因子 gamma 通常取值介于 0 和 1 之间。因此，值函数能帮助我们评估不同状态的优劣。以火星漫游者为例，假设我们的折扣因子设为零。我们的决策是朝着正确的方向前进。在这种情况下，我们可以说这就是我们的值函数。它表明处于状态 S1 的价值为1，其他状态价值为零，而处于状态 S7 的价值为10。当然，这可能是也可能不是准确的值函数，这取决于真实的动态模型，但这就是代理根据该策略拥有的价值函数。简单来说，它告诉我们，如果您遵循这一政策从某状态开始，您将获得的预期奖励的加权总和是多少，该加权总和通过折扣每个奖励值以考虑到达该奖励所需的时间步数。所以，在我们思考的时候，是的。

因此，如果我们要在这个例子中引入折扣系数，根据奖励的进展情况来增加或减少其价值是否合理呢？是的。问题在于这里的 Gamma 是否非零。如果 Gamma 为 0，这意味着我们只关注即时奖励。无论状态如何转移，如果我理解正确，只要开始看到奖励，答案就是是的。所以下一步我们将看到更多相关内容，但如果折扣因子不为零，那么基本上意味着你不仅关心当前获得的奖励，而且是在关注未来可能获得的奖励。因此，就强化学习代理的常见类型而言，其中一些是基于模型的，这意味着它们维护了对世界运作方式的直接模型，例如转移模型和奖励模型。它们可能具有策略或价值函数，也可能没有。它们始终必须计算出策略，弄清楚应该采取什么行动，但它们可能不会明确表示在任何状态下会采取什么行动。

嗯，“无模型方法”指的是具有明确价值函数和策略函数，但不使用模型。回到之前的幻灯片，当评估价值函数时，我感到困惑 - 也就是设置了"yes"。那么，为什么S_6的值为10呢？因为如果您在S_6处采取正确动作，就会到达S_7。你说得对，我们怎样才能判断奖励何时会发生呢。下次我们会详细讨论这个问题。实际上，人们对奖励的看法各不相同。有些认为是基于当前状态的奖励，有些认为是基于环境状态和采取的动作的奖励，还有一种是r-SAS prime的定义，意味着在状态转变前你无法得知奖励。在这里使用的特定定义是假设奖励发生在该状态之后。这些定义基本上是同构的，但我们需要小心区分使用哪一种。在课堂上我们经常使用s,a，表示当您处于某个状态时，

当我们考虑强化学习代理时，它们涉及到模型、价值函数和策略，这几个方面有很多交叉点。有一些经典的RL算法或代理可以归为三种不同类别，即具有模型、明确策略或明确价值函数的。其它一些算法则在一定程度上结合了这三种要素。例如，Actor-Critic方法结合了价值函数和策略。在实践中，这些算法通常是非常有前景的，因为它们综合了多种优点和缺点。对于对学习理论感兴趣的人来说，这些算法提供了更多的研究内容。

最近有一些非常酷的研究专注于从微软研究院得出的基于模型和无模型强化学习在形式上的基本区别。这些研究显示基于模型的方法和无模型方法可能存在根本性差异，这在深度学习领域一直不太清楚。因此，如有问题可以随时向我提问。

在如何做出正确决策的框架中，面临的挑战之一是规划问题。即使掌握了世界运作的模型，也需要利用它来决定应该采取什么行动，以期获取高回报。在这种情况下，如果只是给定了模型，却没有与真实世界的互动，就无法进行规划。有人可能会说，这是你的Transformer模型，这是奖励模型，你可以在计算机上或通过计算得出最佳行动，然后返回真实世界执行，这个过程不需要额外的经验。但在强化学习中，

在处理其他附加问题时，我们不仅需要考虑根据目前信息做出最佳决策，还需要思考采取何种措施以获取所需信息，以便未来做出正确的抉择。比如，当你去了一个全新的餐厅或者搬到一个新城镇时，在选择最佳菜肴时就像面对这个问题一样。想象一下，第一次去餐厅，有五种不同的菜肴供选择。你可以用一段时间尝试每种菜式，逐一品尝，最终找出哪一种最符合你的口味。这样，通过尝试所有选择，你最终会找到最好的那道菜。在这种情况下，AI智能体必须清楚地考虑应该采取什么行动，以获取所需信息，以便在未来作出正确的决策。对于规划而言，这是一个挑战，就像掌握纸牌游戏规则或围棋、国际象棋等游戏规则一样，需要认真思考和努力实践。

在进行行动时，下一个状态的概率分布是可以预测的，这可以帮助计算潜在的得分。因此，在使用树搜索或动态规划等方法时，我们会更多地讨论这些内容，特别是关于动态规划。通过动态规划，可以根据给定的世界模型来做出正确的决策。然而，强化学习本身有点像没有规则书的孤独旅程。在这个过程中，我们只是在探索和观察发生的事情，努力寻求更大的奖励。我们可能会利用经验来显式地计算模型，然后在该模型上进行规划，也可能直接计算策略或价值函数。现在，我想再次强调探索和利用的重要性。就像火星探测器一样，它需要了解在其尝试的行动中，世界是如何运作的。因此，在状态S2中，如果尝试向左移动，就可以观察结果，并基于这一观察做出正确的下一步行动。这一点显而易见，但也可能会导致困境，因为它必须能够根据先前的经验做出决策。

在寻找平衡点，考虑过去的不幸经历可能使我们更倾向于尝试一些预计会有好结果的新事物。 举例来说，观影可以是观看自己喜爱的电影（类比剥削），而探索则是尝试看一部全新的电影，结果可能好也可能糟。 在广告方面，剥削是展示目前点击率最高的广告，而探索则是展示不同的广告内容。 驾驶时，剥削是根据以往经验选择最快的路线，而探索则是尝试不同的道路选择。

这段文字讨论了有限视野环境下的决策策略和探索行为。在有限的地平线环境中，决策策略会随着时间和状态的变化而变化，而在无限范围的环境中，最优策略和标记设置是固定的。在有限视野的情况下，探索行为的意义不大，因为决策是基于有限的信息和时间步长，不会对最终结果产生影响。

在未来，由于你不再做出任何决定，因此将始终根据现有信息做出最优决策。在有限的视野下，你的决策必须基于从信息中获得的价值，以改变你的决策和剩余的视野。这种情况在实际案例中经常发生。例如，如果地平线是有限的，但你不知道具体位置，这将变得更加复杂。我记得这些内容来自博弈论，通常情况下非常复杂。这种情况被称为不定视野问题，其中视野是有限的，但你却难以预测结果。一种对其建模的方法是将其视为具有终止状态的无限地平线问题。因此，一些状态实质上是终结状态，一旦到达这些状态，过程就终止了。在游戏中经常发生这种情况，你无法准确预测游戏何时结束，但它确实是有限的。这种处理方式可以纳入形式主义的方法，但确实具有一定的复杂性。在这些情况下，我们可能更倾向于采取形式化的处理方式。

在对无限范围内的建模中，我们可以观察到不同终止状态的概率。如果你想探索概念中的子问题，特别是在驾驶领域，建议利用你已有的知识，并尝试探索那些你还不熟悉的领域，而不是完全采用全新的方法。对于探索、利用和汽车驾驶这种复合情况，或许最好的做法并不是完全随机地尝试所有事情，而是需要一些证据来支持这些尝试。在某些情况下，最好尽早做出各种探索，或者至少以相同的程度进行探索，然后根据这些信息做出决策，以备将来使用，但具体情况取决于决策的流程。在期末考试后，我们将花很多时间思考探索与利用的问题，这在强化学习中是一个非常关键的部分，特别是在高风险领域。高风险领域指的是什么呢？

我理解你说的是决策对影响人们的重要性。 无论是涉及客户、患者还是学生，我们的决策都会直接影响到真实人们的生活。因此，我们希望能够尽快学习并做出正确的决策。你还有其他问题吗？

当你面临一种之前未曾经历过的情况时，你会选择随机行动来尝试摆脱这种状态吗？或者你会利用以往的经验来做出决策？这是一个很好的问题。同样地，当你要面对一种全新的状态时，你会怎么做呢？你有办法做出比随机选择更好的决定吗？或者你能够利用以往的经验来应对新情况吗？

泛化的一个重要方面是利用我们通过深度学习或其他方式学到的状态特征来共享信息。因此，即使某个状态可能是你从未亲身经历过的，你也可以利用之前学到的知识来指导可能的行动选择。然而，如果你的信息分享方向错误，可能就会导致做出错误的决策。因此，如果你过度泛化，可能会受限于以往的经验，而忽略了在新环境下可能采取更好行动的机会。你对此有什么疑问吗？

在接下来的几堂讲座中，我们将讨论两个非常基本的问题，即评估和控制。评估涉及确定给定政策或机器人行为在世界中表现得有多好。比如，当您的经理给出一个广告展示策略时，您需要评估它的效果。如果您没有足够的信息，可能需要收集数据来评估政策的有效性。控制则涉及优化，即寻找一个最佳策略。这通常需要对子组件进行评估，以确定什么是“最好”的政策。

评估政策的好坏程度是一个重要问题。强化学习在这方面有个很酷的特点，就是我们可以通过对政策进行评价来了解其效果。这意味着我们可以利用从其他政策中积累的数据来评估不同政策可能采取的反事实措施。这种方法非常有帮助，因为我们不必试验所有可能的政策，节省了很多时间和精力。

举个例子，假如我们拿火星探测器的政策评估来说，如果有人告诉你在所有情况下应该采取的行动，那么应该遵循正确的政策。这就涉及到了我所关心的折扣系数问题，请帮我计算或评估该政策的实际价值。在实践中，有些情况可能会出现无法确定政策应该是什么的情况。他们可能只是希望你提供具有最高预期奖励折扣金额的保单，这时一个关键问题是：这些预期的奖励折扣金额来自哪里呢？因此，他们可能关心的是特定的初始状态。

他们可能会说：“我希望你找到最佳策略，假设我从 S4 开始”。他们可能会说：“我希望你从所有起始状态计算最佳策略，或者某种平均值”。所以，在课程的其余部分中，我们得到了——是的。我只是想知道是否可以同时学习最优策略和奖励函数？通过举例来说，如果我对奖励函数中包含的内容或某个动作有一定的信念，会出现一种状态，但后来事实证明这是错误的，那么我们必须重新开始训练以找到最佳策略，或者我可以利用到目前为止所学到的知识。此外，假设数据组织中有一些模糊的奖励信念？这是一个假设性的问题。好的。假设我有一个策略要开始评估，但我不知道奖励函数是什么，也不知道最佳策略是什么，结果表明这个策略并不理想。我是否需要重新启动，或者我可以利用之前的经验告知下一个尝试的策略是什么？也许会有一整套不同的策略？一般来说，您可以利用以前的经验。

您想了解我们下一个政策的相关信息。需要注意的是，您的行动应该具有一定程度的随机性。如果您只采取您已知的某个状态下的行动，您就无法了解其他可能的行动。因此，您需要假设您的策略具有某种概括性或随机性，以便这些信息可以用于评估其他策略。这是一个关键问题。这涉及到反事实推理，以及我们如何利用旧数据来指导未来的决策，尤其是在旧政策可能不是最优策略时。因此，总的来说，我们会深入讨论马尔可夫决策过程和规划，以及如何评估我们的策略。我们需要了解世界是如何运作的，这意味着我们需要一个转移模型和奖励模型。当我们没有这些模型时，我们将探讨无模型策略评估和控制。接着，我们会详细研究深度强化学习和其应用。

函数逼近在强化学习中是一个广泛研究的领域，目前正在迅速增长。我计划制作一个图表，展示目前该领域已发表的论文数量，这个数据真是令人难以置信。接下来，我们将讨论有关策略搜索的许多问题。在实践中，尤其是在机器人技术领域，这是目前最具影响力的方法之一。我们将花很多时间来探索这些高级主题。因此，今天我们稍微谈了一下强化学习，并且比较了它与人工智能和机器学习的其他方面的区别。我们还开始讨论了在不确定性情况下的顺序决策，这是我们物流课程的一部分。在下次课上，我们会尽量提前两天发布讲座幻灯片，这样你就有时间打印出来，如果你想在课堂上使用的话。下周三见。
